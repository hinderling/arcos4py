{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"arcos4py \u00b6 Arcos4py is a python package to detect collective Spatio-temporal phenomena. The package is currently in the development phase. Additional features, such as more plotting functionality will come with future updates. This also means that functionality might change in the feature. Documentation: https://bgraedel.github.io/arcos4py GitHub: https://github.com/bgraedel/arcos4py PyPI: https://pypi.org/project/arcos4py/ Free software: MIT Features \u00b6 Automated Recognition of Collective Signalling for python (arcos4py) is a python port of the R package ARCOS (https://github.com/dmattek/ARCOS ) to identify collective spatial events in time-series data. The software identifies collective protein activation in 2- and 3D cell cultures and can track events over time. Such collective waves have been recently identified in various biological systems and have been demonstrated to play a crucial role in the maintenance of epithelial homeostasis (Gagliardi et al., 2020, Takeuchi et al., 2020, Aikin et al., 2020), in the acinar morphogenesis (Ender et al., 2020), osteoblast regeneration (De Simone et al., 2021), and the coordination of collective cell migration (Aoki et al., 2017, Hino et al., 2020). Despite its focus on cell signaling, the framework can also be applied to other spatiotemporally correlated phenomena. Todo's \u00b6 Add additional plotting functions such as noodle plots for collective-id tracks Add additional tests for binarization and de-biasing modules. Add example processing to documentation with images of collective events. Data Format \u00b6 The time series should be arranged in a long table format where each row defines the object's location, time, and optionally the measurement value. ARCOS defines an ARCOS object on which several class methods can be used to prepare the data and calculate collective events. Optionally the objects used in the ARCOS class can be used individually by importing them from arcos.tools Installation \u00b6 Arcos4py can be installed from PyPI with: pip install arcos4py Napari Plugin \u00b6 Arcos4py is also available as a Napari Plugin arcos-gui . arcos-gui can simplify parameter finding and visualization. Credits \u00b6 Maciej Dobrzynski created the original ARCOS algorithm. This package was created with Cookiecutter and the waynerv/cookiecutter-pypackage project template.","title":"Home"},{"location":"#arcos4py","text":"Arcos4py is a python package to detect collective Spatio-temporal phenomena. The package is currently in the development phase. Additional features, such as more plotting functionality will come with future updates. This also means that functionality might change in the feature. Documentation: https://bgraedel.github.io/arcos4py GitHub: https://github.com/bgraedel/arcos4py PyPI: https://pypi.org/project/arcos4py/ Free software: MIT","title":"arcos4py"},{"location":"#features","text":"Automated Recognition of Collective Signalling for python (arcos4py) is a python port of the R package ARCOS (https://github.com/dmattek/ARCOS ) to identify collective spatial events in time-series data. The software identifies collective protein activation in 2- and 3D cell cultures and can track events over time. Such collective waves have been recently identified in various biological systems and have been demonstrated to play a crucial role in the maintenance of epithelial homeostasis (Gagliardi et al., 2020, Takeuchi et al., 2020, Aikin et al., 2020), in the acinar morphogenesis (Ender et al., 2020), osteoblast regeneration (De Simone et al., 2021), and the coordination of collective cell migration (Aoki et al., 2017, Hino et al., 2020). Despite its focus on cell signaling, the framework can also be applied to other spatiotemporally correlated phenomena.","title":"Features"},{"location":"#todos","text":"Add additional plotting functions such as noodle plots for collective-id tracks Add additional tests for binarization and de-biasing modules. Add example processing to documentation with images of collective events.","title":"Todo's"},{"location":"#data-format","text":"The time series should be arranged in a long table format where each row defines the object's location, time, and optionally the measurement value. ARCOS defines an ARCOS object on which several class methods can be used to prepare the data and calculate collective events. Optionally the objects used in the ARCOS class can be used individually by importing them from arcos.tools","title":"Data Format"},{"location":"#installation","text":"Arcos4py can be installed from PyPI with: pip install arcos4py","title":"Installation"},{"location":"#napari-plugin","text":"Arcos4py is also available as a Napari Plugin arcos-gui . arcos-gui can simplify parameter finding and visualization.","title":"Napari Plugin"},{"location":"#credits","text":"Maciej Dobrzynski created the original ARCOS algorithm. This package was created with Cookiecutter and the waynerv/cookiecutter-pypackage project template.","title":"Credits"},{"location":"api/","text":"Arcos4py top level module. This package is a python port of an R package to detect and analyse collective phenomena. arcos4py \u00b6 Main Module of arcos4py. This module contains the ARCOS class, which implements most functionallity of arcos4py to prepare data and to detect and track collective events. Examples: >>> from arcos4py import ARCOS >>> ts = ARCOS ( data ,[ \"x\" ], 'time' , 'id' , 'meas' , 'clTrackID' ) >>> ts . interpolate_measurements () >>> ts . clip_meas ( clip_low = 0.001 , clip_high = 0.999 ) >>> ts . bin_measurements ( smoothK int = 3 , biasK = 51 , peakThr = 0.2 , binThr = 0.1 , polyDeg = 1 , biasMet = \"runmed\" ,) >>> events_df = ts . trackCollev ( eps = 1 , minClsz = 1 , nPrev = 1 ) ARCOS \u00b6 Detects and tracks collective events in a tracked time-series dataset. Requires binarized measurement column, that can be generated with the bin_measurements method. Tracking makes use of the dbscan algorithm, which is applied to every frame and subsequently connects collective events between frames located within eps distance of each other. Attributes: Name Type Description data DataFrame Data of tracked time-series in \"long format\". Can be used to acess modified dataframe at any point. posCols list List containing position column names strings inside data e.g. At least one dimension is required. frame_column str Indicating the frame column in input_data. id_column str Indicating the track id/id column in input_data. measurement_column str Indicating the measurement column in input_data. clid_column str Indicating the column name containing the collective event ids. Source code in arcos4py/arcos4py.py class ARCOS : \"\"\"Detects and tracks collective events in a tracked time-series dataset. Requires binarized measurement column, that can be generated with the bin_measurements method. Tracking makes use of the dbscan algorithm, which is applied to every frame and subsequently connects collective events between frames located within eps distance of each other. Attributes: data (DataFrame): Data of tracked time-series in \"long format\". Can be used to acess modified dataframe at any point. posCols (list): List containing position column names strings inside data e.g. At least one dimension is required. frame_column (str): Indicating the frame column in input_data. id_column (str): Indicating the track id/id column in input_data. measurement_column (str): Indicating the measurement column in input_data. clid_column (str): Indicating the column name containing the collective event ids. \"\"\" def __init__ ( self , data : pd . DataFrame , posCols : list = [ \"x\" ], frame_column : str = 'time' , id_column : str = 'id' , measurement_column : str = 'meas' , clid_column : str = 'clTrackID' , ) -> None : \"\"\"Constructs class with provided arguments. Arguments: data (DataFrame): Input Data of tracked time-series in \"long format\" containing position columns, a measurement and an object ID column. posCols (list): List ontaining position column names strings inside data e.g. At least one dimension is required. frame_column (str): Indicating the frame column in input_data. id_column (str): Indicating the track id/id column in input_data. measurement_column (str): Indicating the measurement column in input_data. clid_column (str): Indicating the column name containing the collective event ids. \"\"\" self . data = data self . posCols = posCols self . frame_column = frame_column self . id_column = id_column self . measurement_column = measurement_column self . clid_column = clid_column self . data_binarized : pd . DataFrame = None self . tracked_events : pd . DataFrame = None self . bin_col : Union [ str , None ] = None # to check if no measurement was provided assign None self . data = self . data . sort_values ( by = [ self . frame_column , self . id_column ]) self . _check_col () if self . measurement_column is not None : self . resc_col = f \" { self . measurement_column } .resc\" self . bin_col = f \" { self . measurement_column } .bin\" def __repr__ ( self ) -> pd . DataFrame : \"\"\"Set __repr___ to return self.data.\"\"\" return repr ( self . data ) def _check_col ( self ): \"\"\"Checks that self.cols contains all required columns.\"\"\" columns = self . data . columns input_columns = [ self . frame_column , self . id_column , self . id_column , self . measurement_column ] input_columns = [ col for col in input_columns if col is not None ] if not all ( item in columns for item in input_columns ): raise ValueError ( f \"Columns { input_columns } do not match with column in dataframe.\" ) def interpolate_measurements ( self ) -> pd . DataFrame : \"\"\"Interpolates NaN's in place in measurement column. Returns: Dataframe with interpolated measurement column. \"\"\" meas_interp = interpolation ( self . data ) . interpolate () self . data = meas_interp return self . data def clip_meas ( self , clip_low : float = 0.001 , clip_high : float = 0.999 ) -> pd . DataFrame : \"\"\"Clip measurement column to upper and lower quantiles defined in clip_low and clip_high. Arguments: clip_low (float): Lower clipping boundary (quantile). clip_high (float): Upper clipping boundary (quantile). Returns: Dataframe with in place clipped measurement column. \"\"\" meas_column = self . data [ self . measurement_column ] . to_numpy () meas_clipped = clipMeas ( meas_column ) . clip ( clip_low , clip_high ) self . data [ self . measurement_column ] = meas_clipped return self . data def bin_measurements ( self , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , binThr : float = 0.1 , polyDeg : int = 1 , biasMet : str = \"runmed\" , ) -> pd . DataFrame : r \"\"\"Smooth, de-trend, and binarise the input data. First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold Arguments: smoothK (int): Size of the short-term median smoothing filter. biasK (int): Size of the long-term de-trending median filter peakThr (float): Threshold for rescaling of the de-trended signal. binThr (float): Threshold for binary classification. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): De-trending method, one of ['runmed', 'lm', 'none']. Returns: DataFrame with detrended/smoothed and binarized measurement column. \"\"\" self . data = binData ( smoothK , biasK , peakThr , binThr , polyDeg , biasMet , ) . run ( self . data , colMeas = self . measurement_column , colGroup = self . id_column , colFrame = self . frame_column ) return self . data def trackCollev ( self , eps : float = 1 , minClsz : int = 1 , nPrev : int = 1 ) -> pd . DataFrame : \"\"\"Requires binarized measurement column. Makes use of the dbscan algorithm, applies this to every timeframe and subsequently connects collective events between frames located within eps distance of each other. Arguments: eps (float): The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. Value is also used to connect collective events across multiple frames. minClsz (str): The minimum size for a cluster to be identified as a collective event nPrev (int): Number of previous frames the tracking algorithm looks back to connect collective events Returns: DataFrame with detected collective events across time. \"\"\" self . data = detectCollev ( self . data , eps = eps , minClSz = minClsz , nPrev = nPrev , posCols = self . posCols , frame_column = self . frame_column , id_column = self . id_column , bin_meas_column = self . bin_col , clid_column = self . clid_column , ) . run () return self . data __init__ ( self , data , posCols = [ 'x' ], frame_column = 'time' , id_column = 'id' , measurement_column = 'meas' , clid_column = 'clTrackID' ) special \u00b6 Constructs class with provided arguments. Parameters: Name Type Description Default data DataFrame Input Data of tracked time-series in \"long format\" containing position columns, a measurement and an object ID column. required posCols list List ontaining position column names strings inside data e.g. At least one dimension is required. ['x'] frame_column str Indicating the frame column in input_data. 'time' id_column str Indicating the track id/id column in input_data. 'id' measurement_column str Indicating the measurement column in input_data. 'meas' clid_column str Indicating the column name containing the collective event ids. 'clTrackID' Source code in arcos4py/arcos4py.py def __init__ ( self , data : pd . DataFrame , posCols : list = [ \"x\" ], frame_column : str = 'time' , id_column : str = 'id' , measurement_column : str = 'meas' , clid_column : str = 'clTrackID' , ) -> None : \"\"\"Constructs class with provided arguments. Arguments: data (DataFrame): Input Data of tracked time-series in \"long format\" containing position columns, a measurement and an object ID column. posCols (list): List ontaining position column names strings inside data e.g. At least one dimension is required. frame_column (str): Indicating the frame column in input_data. id_column (str): Indicating the track id/id column in input_data. measurement_column (str): Indicating the measurement column in input_data. clid_column (str): Indicating the column name containing the collective event ids. \"\"\" self . data = data self . posCols = posCols self . frame_column = frame_column self . id_column = id_column self . measurement_column = measurement_column self . clid_column = clid_column self . data_binarized : pd . DataFrame = None self . tracked_events : pd . DataFrame = None self . bin_col : Union [ str , None ] = None # to check if no measurement was provided assign None self . data = self . data . sort_values ( by = [ self . frame_column , self . id_column ]) self . _check_col () if self . measurement_column is not None : self . resc_col = f \" { self . measurement_column } .resc\" self . bin_col = f \" { self . measurement_column } .bin\" __repr__ ( self ) special \u00b6 Set __repr___ to return self.data. Source code in arcos4py/arcos4py.py def __repr__ ( self ) -> pd . DataFrame : \"\"\"Set __repr___ to return self.data.\"\"\" return repr ( self . data ) bin_measurements ( self , smoothK = 3 , biasK = 51 , peakThr = 0.2 , binThr = 0.1 , polyDeg = 1 , biasMet = 'runmed' ) \u00b6 Smooth, de-trend, and binarise the input data. First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold Parameters: Name Type Description Default smoothK int Size of the short-term median smoothing filter. 3 biasK int Size of the long-term de-trending median filter 51 peakThr float Threshold for rescaling of the de-trended signal. 0.2 binThr float Threshold for binary classification. 0.1 polyDeg int Sets the degree of the polynomial for lm fitting. 1 biasMet str De-trending method, one of ['runmed', 'lm', 'none']. 'runmed' Returns: Type Description DataFrame DataFrame with detrended/smoothed and binarized measurement column. Source code in arcos4py/arcos4py.py def bin_measurements ( self , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , binThr : float = 0.1 , polyDeg : int = 1 , biasMet : str = \"runmed\" , ) -> pd . DataFrame : r \"\"\"Smooth, de-trend, and binarise the input data. First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold Arguments: smoothK (int): Size of the short-term median smoothing filter. biasK (int): Size of the long-term de-trending median filter peakThr (float): Threshold for rescaling of the de-trended signal. binThr (float): Threshold for binary classification. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): De-trending method, one of ['runmed', 'lm', 'none']. Returns: DataFrame with detrended/smoothed and binarized measurement column. \"\"\" self . data = binData ( smoothK , biasK , peakThr , binThr , polyDeg , biasMet , ) . run ( self . data , colMeas = self . measurement_column , colGroup = self . id_column , colFrame = self . frame_column ) return self . data clip_meas ( self , clip_low = 0.001 , clip_high = 0.999 ) \u00b6 Clip measurement column to upper and lower quantiles defined in clip_low and clip_high. Parameters: Name Type Description Default clip_low float Lower clipping boundary (quantile). 0.001 clip_high float Upper clipping boundary (quantile). 0.999 Returns: Type Description DataFrame Dataframe with in place clipped measurement column. Source code in arcos4py/arcos4py.py def clip_meas ( self , clip_low : float = 0.001 , clip_high : float = 0.999 ) -> pd . DataFrame : \"\"\"Clip measurement column to upper and lower quantiles defined in clip_low and clip_high. Arguments: clip_low (float): Lower clipping boundary (quantile). clip_high (float): Upper clipping boundary (quantile). Returns: Dataframe with in place clipped measurement column. \"\"\" meas_column = self . data [ self . measurement_column ] . to_numpy () meas_clipped = clipMeas ( meas_column ) . clip ( clip_low , clip_high ) self . data [ self . measurement_column ] = meas_clipped return self . data interpolate_measurements ( self ) \u00b6 Interpolates NaN's in place in measurement column. Returns: Type Description DataFrame Dataframe with interpolated measurement column. Source code in arcos4py/arcos4py.py def interpolate_measurements ( self ) -> pd . DataFrame : \"\"\"Interpolates NaN's in place in measurement column. Returns: Dataframe with interpolated measurement column. \"\"\" meas_interp = interpolation ( self . data ) . interpolate () self . data = meas_interp return self . data trackCollev ( self , eps = 1 , minClsz = 1 , nPrev = 1 ) \u00b6 Requires binarized measurement column. Makes use of the dbscan algorithm, applies this to every timeframe and subsequently connects collective events between frames located within eps distance of each other. Parameters: Name Type Description Default eps float The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. Value is also used to connect collective events across multiple frames. 1 minClsz str The minimum size for a cluster to be identified as a collective event 1 nPrev int Number of previous frames the tracking algorithm looks back to connect collective events 1 Returns: Type Description DataFrame DataFrame with detected collective events across time. Source code in arcos4py/arcos4py.py def trackCollev ( self , eps : float = 1 , minClsz : int = 1 , nPrev : int = 1 ) -> pd . DataFrame : \"\"\"Requires binarized measurement column. Makes use of the dbscan algorithm, applies this to every timeframe and subsequently connects collective events between frames located within eps distance of each other. Arguments: eps (float): The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. Value is also used to connect collective events across multiple frames. minClsz (str): The minimum size for a cluster to be identified as a collective event nPrev (int): Number of previous frames the tracking algorithm looks back to connect collective events Returns: DataFrame with detected collective events across time. \"\"\" self . data = detectCollev ( self . data , eps = eps , minClSz = minClsz , nPrev = nPrev , posCols = self . posCols , frame_column = self . frame_column , id_column = self . id_column , bin_meas_column = self . bin_col , clid_column = self . clid_column , ) . run () return self . data plotting special \u00b6 Tools for plotting collective events. plotting \u00b6 Module to plot different metrics generated by arcos4py functions. Examples: >>> from arcos4py.plotting import plotOriginalDetrended >>> plot = arcosPlots ( data , 'time' , 'meas' , 'detrended' , 'id' ) >>> plot . plot_detrended () NoodlePlot \u00b6 Create Noodle Plot of cell tracks, colored by collective event id. Attributes: Name Type Description df pd.DataFrame DataFrame containing collective events from arcos. colev str Name of the collective event column in df. trackid str Name of the track column in df. frame str Name of the frame column in df. posx str Name of the X coordinate column in df. posy str Name of the Y coordinate column in df. posz str Name of the Z coordinate column in df, or None if no z column. Source code in arcos4py/plotting/plotting.py class NoodlePlot : \"\"\"Create Noodle Plot of cell tracks, colored by collective event id. Attributes: df (pd.DataFrame): DataFrame containing collective events from arcos. colev (str): Name of the collective event column in df. trackid (str): Name of the track column in df. frame (str): Name of the frame column in df. posx (str): Name of the X coordinate column in df. posy (str): Name of the Y coordinate column in df. posz (str): Name of the Z coordinate column in df, or None if no z column. \"\"\" def __init__ ( self , df : pd . DataFrame , colev : str , trackid : str , frame : str , posx : str , posy : str , posz : Union [ str , None ] = None , ): \"\"\"Constructs class with given parameters. Arguments: df (pd.DataFrame): DataFrame containing collective events from arcos. colev (str): Name of the collective event column in df. trackid (str): Name of the track column in df. frame (str): Name of the frame column in df. posx (str): Name of the X coordinate column in df. posy (str): Name of the Y coordinate column in df. posz (str | None): Name of the Z coordinate column in df, or \"None\" (str) if no z column. \"\"\" self . df = df self . colev = colev self . trackid = trackid self . frame = frame self . posx = posx self . posy = posy self . posz = posz def _prepare_data_noodleplot ( self , df : pd . DataFrame , color_cylce : list [ str ], colev : str , trackid : str , frame : str , posx : str , posy : str , posz : Union [ str , None ] = None , ): \"\"\"From arcos collective event data,\\ generates a list of numpy arrays, one for each event. Arguments: df (pd.DataFrame): DataFrame containing collective events from arcos. color_cylce (list[str]): list of colors used to color trackid's for individual collective events. colev (str): Name of the collective event column in df. trackid (str): Name of the track column in df. frame: (str): Name of the frame column in df. posx (str): Name of the X coordinate column in df. posy (str): Name of the Y coordinate column in df. posz (str): Name of the Z coordinate column in df, or None if no z column. Returns: list[np.ndarray], np.ndarray: List of collective events data, and colors for each collective event. \"\"\" # values need to be sorted to group with numpy df . sort_values ([ colev , trackid ], inplace = True ) if posz : array = df [[ colev , trackid , frame , posx , posy , posz ]] . to_numpy () else : array = df [[ colev , trackid , frame , posx , posy ]] . to_numpy () # generate goroups for each unique value grouped_array = np . split ( array , np . unique ( array [:, 0 ], axis = 0 , return_index = True )[ 1 ][ 1 :]) # make collids sequential seq_colids = np . concatenate ( [ np . repeat ( i , value . shape [ 0 ]) for i , value in enumerate ( grouped_array )], axis = 0 , ) array_seq_colids = np . column_stack (( array , seq_colids )) # split sequential collids array by trackid and collid grouped_array = np . split ( array_seq_colids , np . unique ( array_seq_colids [:, : 2 ], axis = 0 , return_index = True )[ 1 ][ 1 :], ) # generate colors for each collective event, wrap arround the color cycle colors = np . take ( np . array ( color_cylce ), [ i + 1 for i in np . unique ( seq_colids )], mode = \"wrap\" ) return grouped_array , colors def _create_noodle_plot ( self , grouped_data : np . ndarray , colors : np . ndarray ): \"\"\"Plots the noodle plot.\"\"\" fig , ax = plt . subplots () ax . set_xlabel ( \"Time Point\" ) ax . set_ylabel ( \"Position\" ) for dat in grouped_data : ax . plot ( dat [:, 2 ], dat [:, self . projection_index ], c = colors [ int ( dat [ 0 , - 1 ])], ) return fig , ax def plot ( self , projection_axis : str , color_cylce : list [ str ] = TAB20 ): \"\"\"Create Noodle Plot of cell tracks, colored by collective event id. Arguments: projection_axis (str): Specify with witch coordinate the noodle plot should be drawn. Has to be one of the posx, posy or posz arguments passed in during the class instantiation process. color_cylce (list[str]): List of hex color values or string names (i.e. ['red', 'yellow']) used to color collecitve events. Cycles through list. Returns: fig, axes: Matplotlib figure and axes are returned for the noodle plot. \"\"\" if projection_axis not in [ self . posx , self . posy , self . posz ]: raise ValueError ( f \"projection_axis has to be one of { [ self . posx , self . posy , self . posz ] } \" ) if projection_axis == self . posx : self . projection_index = 3 elif projection_axis == self . posy : self . projection_index = 4 elif projection_axis == self . posz : self . projection_index = 5 grpd_data , colors = self . _prepare_data_noodleplot ( self . df , color_cylce , self . colev , self . trackid , self . frame , self . posx , self . posy , self . posz ) fig , axes = self . _create_noodle_plot ( grpd_data , colors ) return fig , axes __init__ ( self , df , colev , trackid , frame , posx , posy , posz = None ) special \u00b6 Constructs class with given parameters. Parameters: Name Type Description Default df pd.DataFrame DataFrame containing collective events from arcos. required colev str Name of the collective event column in df. required trackid str Name of the track column in df. required frame str Name of the frame column in df. required posx str Name of the X coordinate column in df. required posy str Name of the Y coordinate column in df. required posz str | None Name of the Z coordinate column in df, or \"None\" (str) if no z column. None Source code in arcos4py/plotting/plotting.py def __init__ ( self , df : pd . DataFrame , colev : str , trackid : str , frame : str , posx : str , posy : str , posz : Union [ str , None ] = None , ): \"\"\"Constructs class with given parameters. Arguments: df (pd.DataFrame): DataFrame containing collective events from arcos. colev (str): Name of the collective event column in df. trackid (str): Name of the track column in df. frame (str): Name of the frame column in df. posx (str): Name of the X coordinate column in df. posy (str): Name of the Y coordinate column in df. posz (str | None): Name of the Z coordinate column in df, or \"None\" (str) if no z column. \"\"\" self . df = df self . colev = colev self . trackid = trackid self . frame = frame self . posx = posx self . posy = posy self . posz = posz plot ( self , projection_axis , color_cylce = [ '#1f77b4' , '#aec7e8' , '#ff7f0e' , '#ffbb78' , '#2ca02c' , '#98df8a' , '#d62728' , '#ff9896' , '#9467bd' , '#c5b0d5' , '#8c564b' , '#c49c94' , '#e377c2' , '#f7b6d2' , '#7f7f7f' , '#c7c7c7' , '#bcbd22' , '#dbdb8d' , '#17becf' , '#9edae5' ]) \u00b6 Create Noodle Plot of cell tracks, colored by collective event id. Parameters: Name Type Description Default projection_axis str Specify with witch coordinate the noodle plot should be drawn. Has to be one of the posx, posy or posz arguments passed in during the class instantiation process. required color_cylce list[str] List of hex color values or string names (i.e. ['red', 'yellow']) used to color collecitve events. Cycles through list. ['#1f77b4', '#aec7e8', '#ff7f0e', '#ffbb78', '#2ca02c', '#98df8a', '#d62728', '#ff9896', '#9467bd', '#c5b0d5', '#8c564b', '#c49c94', '#e377c2', '#f7b6d2', '#7f7f7f', '#c7c7c7', '#bcbd22', '#dbdb8d', '#17becf', '#9edae5'] Returns: Type Description fig, axes Matplotlib figure and axes are returned for the noodle plot. Source code in arcos4py/plotting/plotting.py def plot ( self , projection_axis : str , color_cylce : list [ str ] = TAB20 ): \"\"\"Create Noodle Plot of cell tracks, colored by collective event id. Arguments: projection_axis (str): Specify with witch coordinate the noodle plot should be drawn. Has to be one of the posx, posy or posz arguments passed in during the class instantiation process. color_cylce (list[str]): List of hex color values or string names (i.e. ['red', 'yellow']) used to color collecitve events. Cycles through list. Returns: fig, axes: Matplotlib figure and axes are returned for the noodle plot. \"\"\" if projection_axis not in [ self . posx , self . posy , self . posz ]: raise ValueError ( f \"projection_axis has to be one of { [ self . posx , self . posy , self . posz ] } \" ) if projection_axis == self . posx : self . projection_index = 3 elif projection_axis == self . posy : self . projection_index = 4 elif projection_axis == self . posz : self . projection_index = 5 grpd_data , colors = self . _prepare_data_noodleplot ( self . df , color_cylce , self . colev , self . trackid , self . frame , self . posx , self . posy , self . posz ) fig , axes = self . _create_noodle_plot ( grpd_data , colors ) return fig , axes dataPlots \u00b6 Plot different metrics of input data. Attributes: Name Type Description data Dataframe containing ARCOS data. frame str name of frame column in data. measurement str name of measurement column in data. id str name of track id column. Source code in arcos4py/plotting/plotting.py class dataPlots : \"\"\"Plot different metrics of input data. Attributes: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. id (str): name of track id column. \"\"\" def __init__ ( self , data : pd . DataFrame , frame : str , measurement : str , id : str ): \"\"\"Plot different metrics such as histogram, position-t and density. Arguments: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. id (str): name of track id column. \"\"\" self . data = data self . id = id self . frame = frame self . measurement = measurement def position_t_plot ( self , posCol : set [ str ] = { 'x' }, n : int = 20 ): \"\"\"Plots X and Y over T to visualize tracklength. Arguments: posCol (set): containing names of position columns in data. n (int): number of samples to plot. Returns: fig, axes: Matplotlib fig and axes of density plot. \"\"\" sample = pd . Series ( self . data [ self . id ] . unique ()) . sample ( n ) pd_from_r_df = self . data . loc [ self . data [ self . id ] . isin ( sample )] fig , axes = plt . subplots ( 1 , len ( posCol ), figsize = ( 6 , 3 )) for label , df in pd_from_r_df . groupby ( self . id ): for index , value in enumerate ( posCol ): if len ( posCol ) > 1 : df . plot ( x = self . frame , y = value , ax = axes [ index ], legend = None ) else : df . plot ( x = self . frame , y = value , ax = axes , legend = None ) if len ( posCol ) > 1 : for index , value in enumerate ( posCol ): axes [ index ] . set_title ( value ) else : axes . set_title ( value ) return fig , axes def density_plot ( self , * args , ** kwargs ): \"\"\"Density plot of measurement. Uses Seaborn distplot to plot measurement density. Arguments: measurement_col (str): name of measurement column. *args (Any): arguments passed on to seaborn histplot function. **kwargs (Any): keyword arguments passed on to seaborn histplot function. Returns: FacetGrid: Seaborn FacetGrid of density density plot. \"\"\" plot = sns . displot ( self . data [ self . measurement ], kind = \"kde\" , palette = \"pastel\" , label = self . measurement , * args , ** kwargs ) # Plot formatting plt . legend ( prop = { 'size' : 10 }) plt . title ( 'Density Plot of Measurement' ) plt . xlabel ( 'Measurement' ) plt . ylabel ( 'Density' ) return plot def histogram ( self , bins : str = 'auto' , * args , ** kwargs ): \"\"\"Histogram of tracklenght. Uses seaborn histplot function to plot tracklenght histogram. Arguments: bins (str): number or width of bins in histogram *args (Any): arguments passed on to seaborn histplot function. **kwargs (Any): keyword arguments passed on to seaborn histplot function. Returns: AxesSubplot: Matplotlib AxesSubplot of histogram. \"\"\" # Draw histogram track_length = self . data . groupby ( self . id ) . size () axes = sns . histplot ( track_length , label = \"Track Length\" , bins = bins , * args , ** kwargs ) # Plot formatting plt . title ( 'Track length Histogram' ) axes . set_xlabel ( 'Track Length' ) axes . set_ylabel ( 'Count' ) return axes __init__ ( self , data , frame , measurement , id ) special \u00b6 Plot different metrics such as histogram, position-t and density. Parameters: Name Type Description Default data Dataframe containing ARCOS data. required frame str name of frame column in data. required measurement str name of measurement column in data. required id str name of track id column. required Source code in arcos4py/plotting/plotting.py def __init__ ( self , data : pd . DataFrame , frame : str , measurement : str , id : str ): \"\"\"Plot different metrics such as histogram, position-t and density. Arguments: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. id (str): name of track id column. \"\"\" self . data = data self . id = id self . frame = frame self . measurement = measurement density_plot ( self , * args , ** kwargs ) \u00b6 Density plot of measurement. Uses Seaborn distplot to plot measurement density. Parameters: Name Type Description Default measurement_col str name of measurement column. required *args Any arguments passed on to seaborn histplot function. () **kwargs Any keyword arguments passed on to seaborn histplot function. {} Returns: Type Description FacetGrid Seaborn FacetGrid of density density plot. Source code in arcos4py/plotting/plotting.py def density_plot ( self , * args , ** kwargs ): \"\"\"Density plot of measurement. Uses Seaborn distplot to plot measurement density. Arguments: measurement_col (str): name of measurement column. *args (Any): arguments passed on to seaborn histplot function. **kwargs (Any): keyword arguments passed on to seaborn histplot function. Returns: FacetGrid: Seaborn FacetGrid of density density plot. \"\"\" plot = sns . displot ( self . data [ self . measurement ], kind = \"kde\" , palette = \"pastel\" , label = self . measurement , * args , ** kwargs ) # Plot formatting plt . legend ( prop = { 'size' : 10 }) plt . title ( 'Density Plot of Measurement' ) plt . xlabel ( 'Measurement' ) plt . ylabel ( 'Density' ) return plot histogram ( self , bins = 'auto' , * args , ** kwargs ) \u00b6 Histogram of tracklenght. Uses seaborn histplot function to plot tracklenght histogram. Parameters: Name Type Description Default bins str number or width of bins in histogram 'auto' *args Any arguments passed on to seaborn histplot function. () **kwargs Any keyword arguments passed on to seaborn histplot function. {} Returns: Type Description AxesSubplot Matplotlib AxesSubplot of histogram. Source code in arcos4py/plotting/plotting.py def histogram ( self , bins : str = 'auto' , * args , ** kwargs ): \"\"\"Histogram of tracklenght. Uses seaborn histplot function to plot tracklenght histogram. Arguments: bins (str): number or width of bins in histogram *args (Any): arguments passed on to seaborn histplot function. **kwargs (Any): keyword arguments passed on to seaborn histplot function. Returns: AxesSubplot: Matplotlib AxesSubplot of histogram. \"\"\" # Draw histogram track_length = self . data . groupby ( self . id ) . size () axes = sns . histplot ( track_length , label = \"Track Length\" , bins = bins , * args , ** kwargs ) # Plot formatting plt . title ( 'Track length Histogram' ) axes . set_xlabel ( 'Track Length' ) axes . set_ylabel ( 'Count' ) return axes position_t_plot ( self , posCol = { 'x' }, n = 20 ) \u00b6 Plots X and Y over T to visualize tracklength. Parameters: Name Type Description Default posCol set containing names of position columns in data. {'x'} n int number of samples to plot. 20 Returns: Type Description fig, axes Matplotlib fig and axes of density plot. Source code in arcos4py/plotting/plotting.py def position_t_plot ( self , posCol : set [ str ] = { 'x' }, n : int = 20 ): \"\"\"Plots X and Y over T to visualize tracklength. Arguments: posCol (set): containing names of position columns in data. n (int): number of samples to plot. Returns: fig, axes: Matplotlib fig and axes of density plot. \"\"\" sample = pd . Series ( self . data [ self . id ] . unique ()) . sample ( n ) pd_from_r_df = self . data . loc [ self . data [ self . id ] . isin ( sample )] fig , axes = plt . subplots ( 1 , len ( posCol ), figsize = ( 6 , 3 )) for label , df in pd_from_r_df . groupby ( self . id ): for index , value in enumerate ( posCol ): if len ( posCol ) > 1 : df . plot ( x = self . frame , y = value , ax = axes [ index ], legend = None ) else : df . plot ( x = self . frame , y = value , ax = axes , legend = None ) if len ( posCol ) > 1 : for index , value in enumerate ( posCol ): axes [ index ] . set_title ( value ) else : axes . set_title ( value ) return fig , axes plotOriginalDetrended \u00b6 Plot different detrended vs original data. Attributes: Name Type Description data Dataframe containing ARCOS data. frame str name of frame column in data. measurement str name of measurement column in data. detrended str name of detrended column with detrended data. id str name of track id column. Source code in arcos4py/plotting/plotting.py class plotOriginalDetrended : \"\"\"Plot different detrended vs original data. Attributes: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. detrended (str): name of detrended column with detrended data. id (str): name of track id column. \"\"\" def __init__ ( self , data : pd . DataFrame , frame : str , measurement : str , detrended : str , id : str ): \"\"\"Plot detrended vs original data. Arguments: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. detrended (str): name of detrended column with detrended data. id (str): name of track id column. \"\"\" self . data = data self . measurement = measurement self . detrended = detrended self . id = id self . frame = frame def plot_detrended ( self , n_samples : int = 25 , subplots : tuple = ( 5 , 5 ), plotsize : tuple = ( 20 , 10 ) ) -> matplotlib . axes . Axes : \"\"\"Method to plot detrended vs original data. Arguments: n_samples (int): Number of tracks to plot. subplots (tuple): Number of subplots, should be approx. one per sample. plotsize (tuple): Size of generated plot. Returns: Fig, Axes: Matplotlib figure and axes2d of detrended vs original data. \"\"\" vals = np . random . choice ( self . data [ self . id ] . unique (), n_samples , replace = False ) self . data = self . data . set_index ( self . id ) . loc [ vals ] . reset_index () grouped = self . data . groupby ( self . id ) ncols = subplots [ 0 ] nrows = subplots [ 1 ] fig , axes2d = plt . subplots ( nrows = nrows , ncols = ncols , figsize = plotsize , sharey = True ) for ( key , ax ) in zip ( grouped . groups . keys (), axes2d . flatten ()): grouped . get_group ( key ) . plot ( x = self . frame , y = [ self . measurement , self . detrended ], ax = ax ) ax . get_legend () . remove () handles , labels = ax . get_legend_handles_labels () fig . legend ( handles , labels , loc = \"lower right\" ) return fig , axes2d __init__ ( self , data , frame , measurement , detrended , id ) special \u00b6 Plot detrended vs original data. Parameters: Name Type Description Default data Dataframe containing ARCOS data. required frame str name of frame column in data. required measurement str name of measurement column in data. required detrended str name of detrended column with detrended data. required id str name of track id column. required Source code in arcos4py/plotting/plotting.py def __init__ ( self , data : pd . DataFrame , frame : str , measurement : str , detrended : str , id : str ): \"\"\"Plot detrended vs original data. Arguments: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. detrended (str): name of detrended column with detrended data. id (str): name of track id column. \"\"\" self . data = data self . measurement = measurement self . detrended = detrended self . id = id self . frame = frame plot_detrended ( self , n_samples = 25 , subplots = ( 5 , 5 ), plotsize = ( 20 , 10 )) \u00b6 Method to plot detrended vs original data. Parameters: Name Type Description Default n_samples int Number of tracks to plot. 25 subplots tuple Number of subplots, should be approx. one per sample. (5, 5) plotsize tuple Size of generated plot. (20, 10) Returns: Type Description Fig, Axes Matplotlib figure and axes2d of detrended vs original data. Source code in arcos4py/plotting/plotting.py def plot_detrended ( self , n_samples : int = 25 , subplots : tuple = ( 5 , 5 ), plotsize : tuple = ( 20 , 10 ) ) -> matplotlib . axes . Axes : \"\"\"Method to plot detrended vs original data. Arguments: n_samples (int): Number of tracks to plot. subplots (tuple): Number of subplots, should be approx. one per sample. plotsize (tuple): Size of generated plot. Returns: Fig, Axes: Matplotlib figure and axes2d of detrended vs original data. \"\"\" vals = np . random . choice ( self . data [ self . id ] . unique (), n_samples , replace = False ) self . data = self . data . set_index ( self . id ) . loc [ vals ] . reset_index () grouped = self . data . groupby ( self . id ) ncols = subplots [ 0 ] nrows = subplots [ 1 ] fig , axes2d = plt . subplots ( nrows = nrows , ncols = ncols , figsize = plotsize , sharey = True ) for ( key , ax ) in zip ( grouped . groups . keys (), axes2d . flatten ()): grouped . get_group ( key ) . plot ( x = self . frame , y = [ self . measurement , self . detrended ], ax = ax ) ax . get_legend () . remove () handles , labels = ax . get_legend_handles_labels () fig . legend ( handles , labels , loc = \"lower right\" ) return fig , axes2d statsPlots \u00b6 Plot data generated by the stats module. Attributes: Name Type Description data DataFrame containing ARCOS stats data. Source code in arcos4py/plotting/plotting.py class statsPlots : \"\"\"Plot data generated by the stats module. Attributes: data (DataFrame): containing ARCOS stats data. \"\"\" def __init__ ( self , data : pd . DataFrame ): \"\"\"Plot detrended vs original data. Arguments: data (DataFrame): containing ARCOS stats data. \"\"\" self . data = data def plot_events_duration ( self , total_size : str , duration : str , point_size : int = 40 , * args , ** kwargs ): \"\"\"Scatterplot of collective event duration. Arguments: total_size (str): name of total size column. duration (str):, name of column with collective event duration. point_size (int): scatterplot point size. *args (Any): Arguments passed on to seaborn scatterplot function. **kwargs (Any): Keyword arguments passed on to seaborn scatterplot function. Returns: Axes: Matplotlib Axes object of scatterplot \"\"\" plot = sns . scatterplot ( x = self . data [ total_size ], y = self . data [ duration ], s = point_size , * args , ** kwargs ) return plot __init__ ( self , data ) special \u00b6 Plot detrended vs original data. Parameters: Name Type Description Default data DataFrame containing ARCOS stats data. required Source code in arcos4py/plotting/plotting.py def __init__ ( self , data : pd . DataFrame ): \"\"\"Plot detrended vs original data. Arguments: data (DataFrame): containing ARCOS stats data. \"\"\" self . data = data plot_events_duration ( self , total_size , duration , point_size = 40 , * args , ** kwargs ) \u00b6 Scatterplot of collective event duration. Parameters: Name Type Description Default total_size str name of total size column. required duration str , name of column with collective event duration. required point_size int scatterplot point size. 40 *args Any Arguments passed on to seaborn scatterplot function. () **kwargs Any Keyword arguments passed on to seaborn scatterplot function. {} Returns: Type Description Axes Matplotlib Axes object of scatterplot Source code in arcos4py/plotting/plotting.py def plot_events_duration ( self , total_size : str , duration : str , point_size : int = 40 , * args , ** kwargs ): \"\"\"Scatterplot of collective event duration. Arguments: total_size (str): name of total size column. duration (str):, name of column with collective event duration. point_size (int): scatterplot point size. *args (Any): Arguments passed on to seaborn scatterplot function. **kwargs (Any): Keyword arguments passed on to seaborn scatterplot function. Returns: Axes: Matplotlib Axes object of scatterplot \"\"\" plot = sns . scatterplot ( x = self . data [ total_size ], y = self . data [ duration ], s = point_size , * args , ** kwargs ) return plot tools special \u00b6 Tools for detecting collective events. binarize_detrend \u00b6 Module containing binarization and detrending classes. Examples: >>> from arcos4py.tools import binData >>> binarizer = binData ( biasMet = \"lm\" , polyDeg = 1 ) >>> data_rescaled = binarizer . run ( data , colMeas = \"ERK_KTR\" , colGroup = \"trackID\" ) binData ( detrender ) \u00b6 Smooth, de-trend, and binarise the input data. First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold. Attributes: Name Type Description smoothK int Size of the short-term median smoothing filter. biasK int Size of the long-term de-trending median filter. peakThr float Threshold for rescaling of the de-trended signal. binThr float Threshold for binarizing the de-trended signal. polyDeg int Sets the degree of the polynomial for lm fitting. biasMet str De-trending method, one of ['runmed', 'lm', 'none']. Source code in arcos4py/tools/binarize_detrend.py class binData ( detrender ): \"\"\"Smooth, de-trend, and binarise the input data. First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold. Attributes: smoothK (int): Size of the short-term median smoothing filter. biasK (int): Size of the long-term de-trending median filter. peakThr (float): Threshold for rescaling of the de-trended signal. binThr (float): Threshold for binarizing the de-trended signal. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): De-trending method, one of ['runmed', 'lm', 'none']. \"\"\" def __init__ ( self , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , binThr : float = 0.1 , polyDeg : int = 1 , biasMet : str = \"runmed\" , ) -> None : \"\"\"Smooth, de-trend, and binarise the input data. Arguments: smoothK (int): Size of the short-term median smoothing filter. biasK (int): Size of the long-term de-trending median filter. peakThr (float): Threshold for rescaling of the de-trended signal. binThr (float): Threshold for binarizing the de-trended signal. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): De-trending method, one of ['runmed', 'lm', 'none']. \"\"\" super () . __init__ ( smoothK , biasK , peakThr , polyDeg , biasMet ) self . binThr = binThr def _rescale_data ( self , x : np . ndarray , group_index : int , meas_index : int , feat_range : tuple = ( 0 , 1 )) -> np . ndarray : grouped_array = np . split ( x [ meas_index , :], np . unique ( x [ group_index , :], axis = 0 , return_index = True )[ 1 ][ 1 :]) out = [ minmax_scale ( i , feature_range = feat_range ) for i in grouped_array ] rescaled = [ item for sublist in out for item in sublist ] # rescaled = minmax_scale(x[:,1], feature_range=feat_range) x [:, meas_index ] = rescaled return x def _bin_data ( self , x : np . ndarray ) -> np . ndarray : bin = ( x > self . binThr ) . astype ( np . int_ ) return bin def run ( self , x : pd . DataFrame , colGroup : str , colMeas : str , colFrame : str ) -> pd . DataFrame : \"\"\"Runs binarization and detrending. If the bias Method is 'none', first it rescales the data to between [0,1], then local smoothing is applied to the measurement by groups, followed by binarization. If biasMeth is one of ['lm', 'runmed'], first the data is detrended locally with a median filter and then detrended globally, for 'lm' with a linear model and for 'runmed' with a median filter. Followed by binarization of the data. Arguments: x (DataFrame): The time-series data for smoothing, detrending and binarization. colGroup (str): Object id column in x. Detrending and rescaling is performed on a per-object basis. colMeas (str): Measurement column in x on which detrending and rescaling is performed. colFrame (str): Frame column in Time-series data. Used for sorting. Returns: DataFrame: Dataframe containing binarized data, rescaled data and the original columns. \"\"\" col_resc = f \" { colMeas } .resc\" col_bin = f \" { colMeas } .bin\" cols = [ colGroup , colMeas ] x . sort_values ([ colGroup , colFrame ], inplace = True ) data_np = x [ cols ] . to_numpy () if self . biasMet == \"none\" : rescaled_data = self . _rescale_data ( data_np , group_index = 0 , meas_index = 1 ) detrended_data = self . detrend ( rescaled_data , 0 , 1 ) binarized_data = self . _bin_data ( detrended_data ) else : detrended_data = self . detrend ( data_np , group_index = 0 , meas_index = 1 ) binarized_data = self . _bin_data ( detrended_data ) x [ col_resc ] = detrended_data x [ col_bin ] = binarized_data return x __init__ ( self , smoothK = 3 , biasK = 51 , peakThr = 0.2 , binThr = 0.1 , polyDeg = 1 , biasMet = 'runmed' ) special \u00b6 Smooth, de-trend, and binarise the input data. Parameters: Name Type Description Default smoothK int Size of the short-term median smoothing filter. 3 biasK int Size of the long-term de-trending median filter. 51 peakThr float Threshold for rescaling of the de-trended signal. 0.2 binThr float Threshold for binarizing the de-trended signal. 0.1 polyDeg int Sets the degree of the polynomial for lm fitting. 1 biasMet str De-trending method, one of ['runmed', 'lm', 'none']. 'runmed' Source code in arcos4py/tools/binarize_detrend.py def __init__ ( self , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , binThr : float = 0.1 , polyDeg : int = 1 , biasMet : str = \"runmed\" , ) -> None : \"\"\"Smooth, de-trend, and binarise the input data. Arguments: smoothK (int): Size of the short-term median smoothing filter. biasK (int): Size of the long-term de-trending median filter. peakThr (float): Threshold for rescaling of the de-trended signal. binThr (float): Threshold for binarizing the de-trended signal. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): De-trending method, one of ['runmed', 'lm', 'none']. \"\"\" super () . __init__ ( smoothK , biasK , peakThr , polyDeg , biasMet ) self . binThr = binThr run ( self , x , colGroup , colMeas , colFrame ) \u00b6 Runs binarization and detrending. If the bias Method is 'none', first it rescales the data to between [0,1], then local smoothing is applied to the measurement by groups, followed by binarization. If biasMeth is one of ['lm', 'runmed'], first the data is detrended locally with a median filter and then detrended globally, for 'lm' with a linear model and for 'runmed' with a median filter. Followed by binarization of the data. Parameters: Name Type Description Default x DataFrame The time-series data for smoothing, detrending and binarization. required colGroup str Object id column in x. Detrending and rescaling is performed on a per-object basis. required colMeas str Measurement column in x on which detrending and rescaling is performed. required colFrame str Frame column in Time-series data. Used for sorting. required Returns: Type Description DataFrame Dataframe containing binarized data, rescaled data and the original columns. Source code in arcos4py/tools/binarize_detrend.py def run ( self , x : pd . DataFrame , colGroup : str , colMeas : str , colFrame : str ) -> pd . DataFrame : \"\"\"Runs binarization and detrending. If the bias Method is 'none', first it rescales the data to between [0,1], then local smoothing is applied to the measurement by groups, followed by binarization. If biasMeth is one of ['lm', 'runmed'], first the data is detrended locally with a median filter and then detrended globally, for 'lm' with a linear model and for 'runmed' with a median filter. Followed by binarization of the data. Arguments: x (DataFrame): The time-series data for smoothing, detrending and binarization. colGroup (str): Object id column in x. Detrending and rescaling is performed on a per-object basis. colMeas (str): Measurement column in x on which detrending and rescaling is performed. colFrame (str): Frame column in Time-series data. Used for sorting. Returns: DataFrame: Dataframe containing binarized data, rescaled data and the original columns. \"\"\" col_resc = f \" { colMeas } .resc\" col_bin = f \" { colMeas } .bin\" cols = [ colGroup , colMeas ] x . sort_values ([ colGroup , colFrame ], inplace = True ) data_np = x [ cols ] . to_numpy () if self . biasMet == \"none\" : rescaled_data = self . _rescale_data ( data_np , group_index = 0 , meas_index = 1 ) detrended_data = self . detrend ( rescaled_data , 0 , 1 ) binarized_data = self . _bin_data ( detrended_data ) else : detrended_data = self . detrend ( data_np , group_index = 0 , meas_index = 1 ) binarized_data = self . _bin_data ( detrended_data ) x [ col_resc ] = detrended_data x [ col_bin ] = binarized_data return x detrender \u00b6 Smooth and de-trend input data. First, a short-term median filter with size smoothK is applied to remove fast noise from the time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. Attributes: Name Type Description smoothK int Representing the size of the short-term median smoothing filter. biasK int Representing the size of the long-term de-trending median filter. peakThr float Threshold for rescaling of the de-trended signal. polyDeg int Sets the degree of the polynomial for lm fitting. biasMet str Indicating de-trending method, one of ['runmed', 'lm', 'none']. Source code in arcos4py/tools/binarize_detrend.py class detrender : \"\"\"Smooth and de-trend input data. First, a short-term median filter with size smoothK is applied to remove fast noise from the time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. Attributes: smoothK (int): Representing the size of the short-term median smoothing filter. biasK (int): Representing the size of the long-term de-trending median filter. peakThr (float): Threshold for rescaling of the de-trended signal. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): Indicating de-trending method, one of ['runmed', 'lm', 'none']. \"\"\" def __init__ ( self , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , polyDeg : int = 1 , biasMet : str = \"runmed\" , ) -> None : \"\"\"Smooth and de-trend input data. Arguments: smoothK (int): Representing the size of the short-term median smoothing filter. biasK (int): Representing the size of the long-term de-trending median filter. peakThr (float): Threshold for rescaling of the de-trended signal. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): Indicating de-trending method, one of ['runmed', 'lm', 'none']. \"\"\" # check if biasmethod contains one of these three types biasMet_types = [ \"runmed\" , \"lm\" , \"none\" ] if biasMet not in biasMet_types : raise ValueError ( f \"Invalid bias method. Expected one of: { biasMet_types } \" ) self . smoothK = smoothK self . biasK = biasK self . peakThr = peakThr self . polyDeg = polyDeg self . biasMet = biasMet def _detrend_runnmed ( self , x , filter_size , endrule_mode ): local_smoothing = median_filter ( input = x , size = filter_size , mode = endrule_mode ) return local_smoothing def _detrend_lm ( self , x , polynomial_degree ): x = np . linspace ( 1 , x . size , x . size ) . astype ( int ) . reshape (( - 1 , 1 )) transformer = PolynomialFeatures ( degree = polynomial_degree , include_bias = False ) data_ = transformer . fit_transform ( x ) model = LinearRegression () . fit ( X = data_ , y = x ) predicted_value = model . predict ( data_ ) return predicted_value def _run_detrend ( self , x : np . ndarray ) -> np . ndarray : if x . size : local_smoothed = self . _detrend_runnmed ( x , self . smoothK , \"constant\" ) if self . biasMet != \"none\" : if self . biasMet == \"runmed\" : global_smoothed = self . _detrend_runnmed ( x = local_smoothed , filter_size = self . biasK , endrule_mode = \"constant\" , ) elif self . biasMet == \"lm\" : global_smoothed = self . _detrend_lm ( local_smoothed , self . polyDeg ) local_smoothed = np . subtract ( local_smoothed , global_smoothed ) local_smoothed = np . clip ( local_smoothed , 0 , None ) if ( local_smoothed . max () - local_smoothed . min ()) > self . peakThr : local_smoothed = np . divide ( local_smoothed , local_smoothed . max ()) local_smoothed = np . nan_to_num ( local_smoothed ) else : local_smoothed = None return local_smoothed def detrend ( self , x : np . ndarray , group_index : int , meas_index : int ) -> np . ndarray : \"\"\"Run detrinding on input data. The method applies detrending to each group defined in group_col and outputs it into the resc_column. Arguments: x (np.ndarray): Time series data for smoothing. group_index (int): Index of measurement column in x. meas_index (int): Index of id column in x. Returns (np.ndarray): Dataframe containing rescaled column. \"\"\" grouped_array = np . split ( x [:, meas_index ], np . unique ( x [:, group_index ], axis = 0 , return_index = True )[ 1 ][ 1 :]) out = [ self . _run_detrend ( x ) for x in grouped_array ] out_list = [ item for sublist in out for item in sublist ] return np . array ( out_list ) __init__ ( self , smoothK = 3 , biasK = 51 , peakThr = 0.2 , polyDeg = 1 , biasMet = 'runmed' ) special \u00b6 Smooth and de-trend input data. Parameters: Name Type Description Default smoothK int Representing the size of the short-term median smoothing filter. 3 biasK int Representing the size of the long-term de-trending median filter. 51 peakThr float Threshold for rescaling of the de-trended signal. 0.2 polyDeg int Sets the degree of the polynomial for lm fitting. 1 biasMet str Indicating de-trending method, one of ['runmed', 'lm', 'none']. 'runmed' Source code in arcos4py/tools/binarize_detrend.py def __init__ ( self , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , polyDeg : int = 1 , biasMet : str = \"runmed\" , ) -> None : \"\"\"Smooth and de-trend input data. Arguments: smoothK (int): Representing the size of the short-term median smoothing filter. biasK (int): Representing the size of the long-term de-trending median filter. peakThr (float): Threshold for rescaling of the de-trended signal. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): Indicating de-trending method, one of ['runmed', 'lm', 'none']. \"\"\" # check if biasmethod contains one of these three types biasMet_types = [ \"runmed\" , \"lm\" , \"none\" ] if biasMet not in biasMet_types : raise ValueError ( f \"Invalid bias method. Expected one of: { biasMet_types } \" ) self . smoothK = smoothK self . biasK = biasK self . peakThr = peakThr self . polyDeg = polyDeg self . biasMet = biasMet detrend ( self , x , group_index , meas_index ) \u00b6 Run detrinding on input data. The method applies detrending to each group defined in group_col and outputs it into the resc_column. Parameters: Name Type Description Default x np.ndarray Time series data for smoothing. required group_index int Index of measurement column in x. required meas_index int Index of id column in x. required Returns (np.ndarray): Dataframe containing rescaled column. Source code in arcos4py/tools/binarize_detrend.py def detrend ( self , x : np . ndarray , group_index : int , meas_index : int ) -> np . ndarray : \"\"\"Run detrinding on input data. The method applies detrending to each group defined in group_col and outputs it into the resc_column. Arguments: x (np.ndarray): Time series data for smoothing. group_index (int): Index of measurement column in x. meas_index (int): Index of id column in x. Returns (np.ndarray): Dataframe containing rescaled column. \"\"\" grouped_array = np . split ( x [:, meas_index ], np . unique ( x [:, group_index ], axis = 0 , return_index = True )[ 1 ][ 1 :]) out = [ self . _run_detrend ( x ) for x in grouped_array ] out_list = [ item for sublist in out for item in sublist ] return np . array ( out_list ) cleandata \u00b6 Module containing clipping and interpolation classes. Examples: >>> # Interpolation >>> from arcos4py.tools import interpolation >>> a = interpolation ( data ) >>> data_interp = a . interpolate () >>> # clipping >>> from arcos4py.tools import clipMeas >>> a = clipMeas ( data ) >>> data_clipped = a . clip ( 0.001 , 0.999 ) clipMeas \u00b6 Clip input array. Source code in arcos4py/tools/cleandata.py class clipMeas : \"\"\"Clip input array.\"\"\" def __init__ ( self , data : np . ndarray ) -> None : \"\"\"Clips array to quantilles. Arguments: data (ndarray): To be clipped. \"\"\" self . data = data def _calculate_percentile ( self , data : np . ndarray , clip_low : float , clip_high : float ): \"\"\"Calculate upper and lower quantille. Arguments: data (ndarray): To calculate upper and lower quantile on. clip_low (float): Lower clipping boundary (quantile). clip_high (float): Upper clipping boundry (quantille). Returns: np.ndarray: Array with lower quantile and array with upper quantile. \"\"\" quantille_low = np . quantile ( data , clip_low , keepdims = True ) quantille_high = np . quantile ( data , clip_high , keepdims = True ) return quantille_low , quantille_high def clip ( self , clip_low : float = 0.001 , clip_high : float = 0.999 ) -> np . ndarray : \"\"\"Clip input array to upper and lower quantiles defined in clip_low and clip_high. Arguments: clip_low (float): Lower clipping boundary (quantile). clip_high (float): Upper clipping boundry (quantille). Returns: np.ndarray: A clipped array of the input data. \"\"\" low , high = self . _calculate_percentile ( self . data , clip_low , clip_high ) out = self . data . clip ( low , high ) return out __init__ ( self , data ) special \u00b6 Clips array to quantilles. Parameters: Name Type Description Default data ndarray To be clipped. required Source code in arcos4py/tools/cleandata.py def __init__ ( self , data : np . ndarray ) -> None : \"\"\"Clips array to quantilles. Arguments: data (ndarray): To be clipped. \"\"\" self . data = data clip ( self , clip_low = 0.001 , clip_high = 0.999 ) \u00b6 Clip input array to upper and lower quantiles defined in clip_low and clip_high. Parameters: Name Type Description Default clip_low float Lower clipping boundary (quantile). 0.001 clip_high float Upper clipping boundry (quantille). 0.999 Returns: Type Description np.ndarray A clipped array of the input data. Source code in arcos4py/tools/cleandata.py def clip ( self , clip_low : float = 0.001 , clip_high : float = 0.999 ) -> np . ndarray : \"\"\"Clip input array to upper and lower quantiles defined in clip_low and clip_high. Arguments: clip_low (float): Lower clipping boundary (quantile). clip_high (float): Upper clipping boundry (quantille). Returns: np.ndarray: A clipped array of the input data. \"\"\" low , high = self . _calculate_percentile ( self . data , clip_low , clip_high ) out = self . data . clip ( low , high ) return out interpolation \u00b6 Interpolate nan values in a numpy array. Attributes: Name Type Description data DataFrame Where NaN should be replaced with interpolated values. Source code in arcos4py/tools/cleandata.py class interpolation : \"\"\"Interpolate nan values in a numpy array. Attributes: data (DataFrame): Where NaN should be replaced with interpolated values. \"\"\" def __init__ ( self , data : pd . DataFrame ): \"\"\"Interpolate nan values in a pandas dataframe. Uses pandas.interpolate with liner interpolation. Arguments: data (DataFrame): Where NaN should be replaced with interpolated values. \"\"\" self . data = data def interpolate ( self ) -> pd . DataFrame : \"\"\"Interpolate nan and missing values. Returns: DataFrame: Interpolated input data. \"\"\" self . data = self . data . interpolate ( axis = 0 ) return self . data __init__ ( self , data ) special \u00b6 Interpolate nan values in a pandas dataframe. Uses pandas.interpolate with liner interpolation. Parameters: Name Type Description Default data DataFrame Where NaN should be replaced with interpolated values. required Source code in arcos4py/tools/cleandata.py def __init__ ( self , data : pd . DataFrame ): \"\"\"Interpolate nan values in a pandas dataframe. Uses pandas.interpolate with liner interpolation. Arguments: data (DataFrame): Where NaN should be replaced with interpolated values. \"\"\" self . data = data interpolate ( self ) \u00b6 Interpolate nan and missing values. Returns: Type Description DataFrame Interpolated input data. Source code in arcos4py/tools/cleandata.py def interpolate ( self ) -> pd . DataFrame : \"\"\"Interpolate nan and missing values. Returns: DataFrame: Interpolated input data. \"\"\" self . data = self . data . interpolate ( axis = 0 ) return self . data detect_events \u00b6 Module to track and detect collective events. Examples: >>> from arcos4py.tools import detectCollev >>> ts = detectCollev ( data ) >>> events_df = ts . run () detectCollev \u00b6 Identifies and tracks collective signalling events. Requires binarized measurement column. Makes use of the dbscan algorithm, applies this to every timeframe and subsequently connects collective events between frames located within eps distance of each other. Attributes: Name Type Description input_data DataFrame Input data to be processed. Must contain a binarized measurement column. eps float The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. Value is also used to connect collective events across multiple frames. minClSz int Minimum size for a cluster to be identified as a collective event. nPrev int Number of previous frames the tracking algorithm looks back to connect collective events. posCols list List of position columns contained in the data. Must at least contain one frame_column str Indicating the frame column in input_data. id_column str Indicating the track id/id column in input_data. bin_meas_column str Indicating the bin_meas_column in input_data or None. clid_column str Indicating the column name containing the ids of collective events. Source code in arcos4py/tools/detect_events.py class detectCollev : \"\"\"Identifies and tracks collective signalling events. Requires binarized measurement column. Makes use of the dbscan algorithm, applies this to every timeframe and subsequently connects collective events between frames located within eps distance of each other. Attributes: input_data (DataFrame): Input data to be processed. Must contain a binarized measurement column. eps (float): The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. Value is also used to connect collective events across multiple frames. minClSz (int): Minimum size for a cluster to be identified as a collective event. nPrev (int): Number of previous frames the tracking algorithm looks back to connect collective events. posCols (list): List of position columns contained in the data. Must at least contain one frame_column (str): Indicating the frame column in input_data. id_column (str): Indicating the track id/id column in input_data. bin_meas_column (str): Indicating the bin_meas_column in input_data or None. clid_column (str): Indicating the column name containing the ids of collective events. \"\"\" def __init__ ( self , input_data : pd . DataFrame , eps : float = 1 , minClSz : int = 1 , nPrev : int = 1 , posCols : list = [ \"x\" ], frame_column : str = 'time' , id_column : Union [ str , None ] = None , bin_meas_column : Union [ str , None ] = 'meas' , clid_column : str = 'clTrackID' , ) -> None : \"\"\"Constructs class with input parameters. Arguments: input_data (DataFrame): Input data to be processed. Must contain a binarized measurement column. eps (float): The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. Value is also used to connect collective events across multiple frames. minClSz (int): Minimum size for a cluster to be identified as a collective event. nPrev (int): Number of previous frames the tracking algorithm looks back to connect collective events. posCols (list): List of position columns contained in the data. Must at least contain one frame_column (str): Indicating the frame column in input_data. id_column (str | None): Indicating the track id/id column in input_data, optional. bin_meas_column (str): Indicating the bin_meas_column in input_data or None. clid_column (str): Indicating the column name containing the ids of collective events. \"\"\" # assign some variables passed in as arguments to the object self . input_data = input_data self . eps = eps self . minClSz = minClSz self . nPrev = nPrev self . frame_column = frame_column self . id_column = id_column self . bin_meas_column = bin_meas_column self . clid_column = clid_column self . posCols = posCols self . columns_input = self . input_data . columns self . clidFrame = f ' { clid_column } .frame' self . pos_cols_inputdata = [ col for col in self . posCols if col in self . columns_input ] # run input checks self . _run_input_checks () def _check_input_data ( self ): \"\"\"Checks if input contains data\\ raises error if not.\"\"\" if self . input_data is None : raise noDataError ( \"Input is None\" ) elif self . input_data . empty : raise noDataError ( \"Input is empty\" ) def _check_pos_columns ( self ): \"\"\"Checks if Input contains correct columns\\ raises Exception if not.\"\"\" if not all ( item in self . columns_input for item in self . posCols ): raise columnError ( \"Input data does not have the indicated position columns!\" ) def _check_frame_column ( self ): if self . frame_column not in self . columns_input : raise columnError ( \"Input data does not have the indicated frame column!\" ) def _check_eps ( self ): \"\"\"Checks if eps is greater than 0.\"\"\" if self . eps <= 0 : raise epsError ( \"eps has to be greater than 0\" ) def _check_minClSz ( self ): \"\"\"Checks if minClSiz is greater than 0.\"\"\" if self . minClSz <= 0 : raise minClSzError ( \"Parameter minClSiz has to be greater than 0!\" ) def _check_nPrev ( self ): \"\"\"Checks if nPrev is greater than 0.\"\"\" if self . nPrev <= 0 and isinstance ( self . nPrev , int ): raise nPrevError ( \"Parameter nPrev has to be an integer greater than 0 and an integer!\" ) def _run_input_checks ( self ): \"\"\"Run input checks.\"\"\" self . _check_input_data () self . _check_pos_columns () self . _check_eps () self . _check_minClSz () self . _check_nPrev () self . _check_frame_column () def _select_necessary_columns ( self , data : pd . DataFrame , frame_col : str , id_col : Union [ str , None ], pos_col : list , bin_col : Union [ str , None ] ) -> pd . DataFrame : \"\"\"Select necessary input colums from input data into dataframe. Arguments: data (DataFrame): Containing necessary columns. frame_col (str): Frame column in data. id_col (str): Id column in data. pos_col (list): string representation of position columns in data. bin_col (str): Name of binary column. Returns: DataFrame: Filtered columns necessary for calculation. \"\"\" columns = [ frame_col , id_col , bin_col ] columns = [ col for col in columns if col ] columns . extend ( pos_col ) neccessary_data = data [ columns ] . copy ( deep = True ) return neccessary_data def _filter_active ( self , data : pd . DataFrame , bin_meas_col : Union [ str , None ]) -> pd . DataFrame : \"\"\"Selects rows with binary value of greater than 0. Arguments: data (DataFrame): Dataframe containing necessary columns. bin_meas_col (str|None): Either name of the binary column or None if no such column exists. Returns: DataFrame: Filtered pandas DataFrame. \"\"\" if bin_meas_col is not None : data = data [ data [ bin_meas_col ] > 0 ] return data def _dbscan ( self , x : np . ndarray ) -> list : \"\"\"Dbscan method to run and merge the cluster id labels to the original dataframe. Arguments: x (np.ndarray): With unique frame and position columns. collid_col (str): Column to be created containing cluster-id labels. Returns: list[np.ndarray]: list with added collective id column detected by DBSCAN. \"\"\" db_array = DBSCAN ( eps = self . eps , min_samples = self . minClSz , algorithm = \"kd_tree\" ) . fit ( x [:, 1 :]) cluster_labels = db_array . labels_ cluster_list = [ id + 1 if id > - 1 else np . nan for id in cluster_labels ] return cluster_list def _run_dbscan ( self , data : pd . DataFrame , frame : str , clid_frame : str , id_column : Union [ str , None ]) -> pd . DataFrame : \"\"\"Apply dbscan method to every group i.e. frame. Arguments: data (DataFrame): Must contain position columns and frame columns. frame (str): Name of frame column in data. clid_frame (str): column to be created containing the output cluster ids from dbscan. id_column (str | None): track_id column Returns: DataFrame: Dataframe with added collective id column detected by DBSCAN for every frame. \"\"\" if self . id_column : data = data . sort_values ([ frame , id_column ]) . reset_index ( drop = True ) else : data = data . sort_values ([ frame ]) . reset_index ( drop = True ) subset = [ frame ] + self . pos_cols_inputdata data_np = data [ subset ] . to_numpy ( dtype = np . float64 ) grouped_array = np . split ( data_np , np . unique ( data_np [:, 0 ], axis = 0 , return_index = True )[ 1 ][ 1 :]) # map dbscan to grouped_array out = [ self . _dbscan ( i ) for i in grouped_array ] out_list = [ item for sublist in out for item in sublist ] data [ clid_frame ] = out_list data = data . dropna () return data def _make_db_id_unique ( self , db_data : pd . DataFrame , frame : str , clid_frame , clid ) -> pd . DataFrame : \"\"\"Make db_scan cluster id labels unique by adding the\\ cummulative sum of previous group to next group. Arguments: db_data (DataFrame): Returned by _run_dbscan function with non-unique cluster ids. frame (str): Frame column. clid_frame (str): Column name of cluster-id per frame. clid (str): Column name of unique cluster ids to be returned. Returns: DataFrame: Dataframe with unique collective events. \"\"\" db_data_np = db_data [[ frame , clid_frame ]] . to_numpy () grouped_array = np . split ( db_data_np [:, 1 ], np . unique ( db_data_np [:, 0 ], axis = 0 , return_index = True )[ 1 ][ 1 :]) max_array = [ 0 ] + [ np . max ( i ) for i in grouped_array if i . size != 0 ] out = [ np . add ( value , np . cumsum ( max_array )[ i ]) for i , value in enumerate ( grouped_array )] db_gp = np . concatenate ( out ) db_data [ clid ] = db_gp . astype ( np . int64 ) return db_data def _nearest_neighbour ( self , data_a : np . ndarray , data_b : np . ndarray , nbr_nearest_neighbours : int = 1 , ): \"\"\"Calculates nearest neighbour in from data_a\\ to data_b nearest_neighbours in data_b. Arguments: data_a (DataFrame): containing position values. data_b (DataFrame): containing position values. nbr_nearest_neighbours (int): of the number of nearest neighbours to be calculated. Returns: tuple(np.ndarray, np.ndarray): Returns tuple of 2 arrays containing nearest neighbour indices and distances. \"\"\" kdB = KDTree ( data = data_a ) nearest_neighbours = kdB . query ( data_b , k = nbr_nearest_neighbours ) return nearest_neighbours def _link_clusters_between_frames ( self , data : pd . DataFrame , frame : str , colid : str ) -> pd . DataFrame : \"\"\"Tracks clusters detected with DBSCAN along a frame axis,\\ returns tracked collective events as a pandas dataframe. Arguments: data (DataFrame): Output from dbscan. frame (str): Frame column. colid (str): Colid column. Returns: DataFrame: Pandas dataframe with tracked collective ids. \"\"\" essential_cols = [ frame , colid ] + self . posCols data_essential = data [ essential_cols ] data_np = data_essential . to_numpy () data_np_frame = data_np [:, 0 ] # loop over all frames to link detected clusters iteratively for t in np . unique ( data_np_frame , return_index = False )[ 1 :]: prev_frame = data_np [( data_np_frame >= ( t - self . nPrev )) & ( data_np_frame < t )] current_frame = data_np [ data_np_frame == t ] # only continue if objects were detected in previous frame if prev_frame . size : colid_current = current_frame [:, 1 ] # loop over unique cluster in frame for cluster in np . unique ( colid_current , return_index = False ): pos_current = current_frame [:, 2 :][ colid_current == cluster ] pos_previous = prev_frame [:, 2 :] # calculate nearest neighbour between previoius and current frame nn_dist , nn_indices = self . _nearest_neighbour ( pos_previous , pos_current ) prev_cluster_nbr_all = prev_frame [ nn_indices , 1 ] prev_cluster_nbr_eps = prev_cluster_nbr_all [( nn_dist <= self . eps )] # only continue if neighbours # were detected within eps distance if prev_cluster_nbr_eps . size : prev_clusternbr_eps_unique = np . unique ( prev_cluster_nbr_eps , return_index = False ) if prev_clusternbr_eps_unique . size > 0 : # propagate cluster id from previous frame data_np [(( data_np_frame == t ) & ( data_np [:, 1 ] == cluster )), 1 ] = prev_cluster_nbr_all np_out = data_np [:, 1 ] sorter = np_out . argsort ()[:: 1 ] grouped_array = np . split ( np_out [ sorter ], np . unique ( np_out [ sorter ], axis = 0 , return_index = True )[ 1 ][ 1 :]) np_grouped_consecutive = ( np . repeat ( i + 1 , value . size ) for i , value in enumerate ( grouped_array )) out_array = np . array ([ item for sublist in np_grouped_consecutive for item in sublist ]) data [ colid ] = out_array [ sorter . argsort ()] . astype ( 'int64' ) return data def _get_export_columns ( self ): \"\"\"Get columns that will contained in the pandas dataframe returned by the run method.\"\"\" self . pos_cols_inputdata = [ col for col in self . posCols if col in self . columns_input ] if self . id_column : columns = [ self . frame_column , self . id_column ] else : columns = [ self . frame_column ] columns . extend ( self . pos_cols_inputdata ) columns . append ( self . clid_column ) return columns def run ( self ) -> pd . DataFrame : \"\"\"Method to execute the different steps necessary for tracking. 1. Selects columns. 2. filters data on binary column > 1. 3. Applies dbscan algorithm to every frame. 4. Makes cluster ids unique across frames. 5. Tracks collective events i.e. links cluster ids across frames. 6. Creates final DataFrame. Returns (Dataframe): Dataframe with tracked collective events is returned. \"\"\" filtered_cols = self . _select_necessary_columns ( self . input_data , self . frame_column , self . id_column , self . pos_cols_inputdata , self . bin_meas_column , ) active_data = self . _filter_active ( filtered_cols , self . bin_meas_column ) db_data = self . _run_dbscan ( data = active_data , frame = self . frame_column , clid_frame = self . clidFrame , id_column = self . id_column , ) db_data = self . _make_db_id_unique ( db_data , frame = self . frame_column , clid_frame = self . clidFrame , clid = self . clid_column , ) tracked_events = self . _link_clusters_between_frames ( db_data , self . frame_column , self . clid_column ) return_columns = self . _get_export_columns () tracked_events = tracked_events [ return_columns ] if self . clid_column in self . input_data . columns : df_to_merge = self . input_data . drop ( columns = [ self . clid_column ]) else : df_to_merge = self . input_data tracked_events = tracked_events . merge ( df_to_merge , how = \"left\" ) tracked_events = tracked_events return tracked_events __init__ ( self , input_data , eps = 1 , minClSz = 1 , nPrev = 1 , posCols = [ 'x' ], frame_column = 'time' , id_column = None , bin_meas_column = 'meas' , clid_column = 'clTrackID' ) special \u00b6 Constructs class with input parameters. Parameters: Name Type Description Default input_data DataFrame Input data to be processed. Must contain a binarized measurement column. required eps float The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. Value is also used to connect collective events across multiple frames. 1 minClSz int Minimum size for a cluster to be identified as a collective event. 1 nPrev int Number of previous frames the tracking algorithm looks back to connect collective events. 1 posCols list List of position columns contained in the data. Must at least contain one ['x'] frame_column str Indicating the frame column in input_data. 'time' id_column str | None Indicating the track id/id column in input_data, optional. None bin_meas_column str Indicating the bin_meas_column in input_data or None. 'meas' clid_column str Indicating the column name containing the ids of collective events. 'clTrackID' Source code in arcos4py/tools/detect_events.py def __init__ ( self , input_data : pd . DataFrame , eps : float = 1 , minClSz : int = 1 , nPrev : int = 1 , posCols : list = [ \"x\" ], frame_column : str = 'time' , id_column : Union [ str , None ] = None , bin_meas_column : Union [ str , None ] = 'meas' , clid_column : str = 'clTrackID' , ) -> None : \"\"\"Constructs class with input parameters. Arguments: input_data (DataFrame): Input data to be processed. Must contain a binarized measurement column. eps (float): The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. Value is also used to connect collective events across multiple frames. minClSz (int): Minimum size for a cluster to be identified as a collective event. nPrev (int): Number of previous frames the tracking algorithm looks back to connect collective events. posCols (list): List of position columns contained in the data. Must at least contain one frame_column (str): Indicating the frame column in input_data. id_column (str | None): Indicating the track id/id column in input_data, optional. bin_meas_column (str): Indicating the bin_meas_column in input_data or None. clid_column (str): Indicating the column name containing the ids of collective events. \"\"\" # assign some variables passed in as arguments to the object self . input_data = input_data self . eps = eps self . minClSz = minClSz self . nPrev = nPrev self . frame_column = frame_column self . id_column = id_column self . bin_meas_column = bin_meas_column self . clid_column = clid_column self . posCols = posCols self . columns_input = self . input_data . columns self . clidFrame = f ' { clid_column } .frame' self . pos_cols_inputdata = [ col for col in self . posCols if col in self . columns_input ] # run input checks self . _run_input_checks () run ( self ) \u00b6 Method to execute the different steps necessary for tracking. Selects columns. filters data on binary column > 1. Applies dbscan algorithm to every frame. Makes cluster ids unique across frames. Tracks collective events i.e. links cluster ids across frames. Creates final DataFrame. Returns (Dataframe): Dataframe with tracked collective events is returned. Source code in arcos4py/tools/detect_events.py def run ( self ) -> pd . DataFrame : \"\"\"Method to execute the different steps necessary for tracking. 1. Selects columns. 2. filters data on binary column > 1. 3. Applies dbscan algorithm to every frame. 4. Makes cluster ids unique across frames. 5. Tracks collective events i.e. links cluster ids across frames. 6. Creates final DataFrame. Returns (Dataframe): Dataframe with tracked collective events is returned. \"\"\" filtered_cols = self . _select_necessary_columns ( self . input_data , self . frame_column , self . id_column , self . pos_cols_inputdata , self . bin_meas_column , ) active_data = self . _filter_active ( filtered_cols , self . bin_meas_column ) db_data = self . _run_dbscan ( data = active_data , frame = self . frame_column , clid_frame = self . clidFrame , id_column = self . id_column , ) db_data = self . _make_db_id_unique ( db_data , frame = self . frame_column , clid_frame = self . clidFrame , clid = self . clid_column , ) tracked_events = self . _link_clusters_between_frames ( db_data , self . frame_column , self . clid_column ) return_columns = self . _get_export_columns () tracked_events = tracked_events [ return_columns ] if self . clid_column in self . input_data . columns : df_to_merge = self . input_data . drop ( columns = [ self . clid_column ]) else : df_to_merge = self . input_data tracked_events = tracked_events . merge ( df_to_merge , how = \"left\" ) tracked_events = tracked_events return tracked_events filter_events \u00b6 Module to filter collective events. Examples: >>> from arcos4py.tools import filterCollev >>> f = filterCollev ( data , 'time' , 'collid' ) >>> df = f . filter ( coll_duration = 9 , coll_total_size = 10 ) filterCollev \u00b6 Select Collective events that last longer than coll_duration and have a larger total size than coll_total_size. Attributes: Name Type Description data Dataframe With detected collective events. frame_column str Indicating the frame column in data. collid_column str Indicating the collective event id column in data. obj_id_column str Inidicating the object identifier column such as cell track id. Source code in arcos4py/tools/filter_events.py class filterCollev : \"\"\"Select Collective events that last longer than coll_duration\\ and have a larger total size than coll_total_size. Attributes: data (Dataframe): With detected collective events. frame_column (str): Indicating the frame column in data. collid_column (str): Indicating the collective event id column in data. obj_id_column (str): Inidicating the object identifier column such as cell track id. \"\"\" def __init__ ( self , data : pd . DataFrame , frame_column : str = \"time\" , collid_column : str = \"collid\" , obj_id_column : str = \"trackID\" , ): \"\"\"Constructs filterCollev class with Parameters. Arguments: data (Dataframe): With detected collective events. frame_column (str): Indicating the frame column in data. collid_column (str): Indicating the collective event id column in data. obj_id_column (str): Inidicating the object identifier column such as cell track id. \"\"\" self . data = data self . frame_column = frame_column self . collid_column = collid_column self . obj_id_column = obj_id_column def _filter_collev ( self , data : pd . DataFrame , collev_stats : pd . DataFrame , collev_id : str , min_duration : int , min_size : int , ): \"\"\"Uses the dataframe generated by self._get_collev_duration()\\ to filter collective events that last longer than\\ min_duration and are larger than min_size. Arguments: data (DataFrame): Containing unfiltered collective events. collev_stats (DataFrame): Containing stats of collective events. collev_id (str): Indicating the contained collective id column. min_duration (str): minimal duration of a collective event for it to be returned. min_size (int): minimal size for a collective event to be returned. Returns: DataFrame: Dataframe containing filtered collective events. \"\"\" collev_stats = collev_stats [ ( collev_stats [ \"duration\" ] >= min_duration ) & ( collev_stats [ \"total_size\" ] >= min_size ) ] data = data [ data [ collev_id ] . isin ( collev_stats [ collev_id ])] return data def filter ( self , coll_duration : int = 9 , coll_total_size : int = 10 ) -> pd . DataFrame : \"\"\"Filter collective events. Method to filter collective events according to the parameters specified in the object instance. Arguments: coll_duration (int): Minimal duration of collective events to be selected. coll_total_size (int): Minimal total size of collective events to be selected. Returns: Returns pandas dataframe containing filtered collective events \"\"\" if self . data . empty : return self . data stats = calcCollevStats () colev_duration = stats . calculate ( self . data , self . frame_column , self . collid_column , self . obj_id_column ) filtered_df = self . _filter_collev ( self . data , colev_duration , self . collid_column , coll_duration , coll_total_size , ) return filtered_df __init__ ( self , data , frame_column = 'time' , collid_column = 'collid' , obj_id_column = 'trackID' ) special \u00b6 Constructs filterCollev class with Parameters. Parameters: Name Type Description Default data Dataframe With detected collective events. required frame_column str Indicating the frame column in data. 'time' collid_column str Indicating the collective event id column in data. 'collid' obj_id_column str Inidicating the object identifier column such as cell track id. 'trackID' Source code in arcos4py/tools/filter_events.py def __init__ ( self , data : pd . DataFrame , frame_column : str = \"time\" , collid_column : str = \"collid\" , obj_id_column : str = \"trackID\" , ): \"\"\"Constructs filterCollev class with Parameters. Arguments: data (Dataframe): With detected collective events. frame_column (str): Indicating the frame column in data. collid_column (str): Indicating the collective event id column in data. obj_id_column (str): Inidicating the object identifier column such as cell track id. \"\"\" self . data = data self . frame_column = frame_column self . collid_column = collid_column self . obj_id_column = obj_id_column filter ( self , coll_duration = 9 , coll_total_size = 10 ) \u00b6 Filter collective events. Method to filter collective events according to the parameters specified in the object instance. Parameters: Name Type Description Default coll_duration int Minimal duration of collective events to be selected. 9 coll_total_size int Minimal total size of collective events to be selected. 10 Returns: Type Description DataFrame Returns pandas dataframe containing filtered collective events Source code in arcos4py/tools/filter_events.py def filter ( self , coll_duration : int = 9 , coll_total_size : int = 10 ) -> pd . DataFrame : \"\"\"Filter collective events. Method to filter collective events according to the parameters specified in the object instance. Arguments: coll_duration (int): Minimal duration of collective events to be selected. coll_total_size (int): Minimal total size of collective events to be selected. Returns: Returns pandas dataframe containing filtered collective events \"\"\" if self . data . empty : return self . data stats = calcCollevStats () colev_duration = stats . calculate ( self . data , self . frame_column , self . collid_column , self . obj_id_column ) filtered_df = self . _filter_collev ( self . data , colev_duration , self . collid_column , coll_duration , coll_total_size , ) return filtered_df stats \u00b6 Module containing tools to calculate statistics of collective events. Examples: >>> from arcos4py.tools import calcCollevStats >>> test = calcCollevStats () >>> out = test . calculate ( data = data , frame_column = \"frame\" , collid_column = \"collid\" ) calcCollevStats \u00b6 Class to calculate statistics of collective events. Source code in arcos4py/tools/stats.py class calcCollevStats : \"\"\"Class to calculate statistics of collective events.\"\"\" def __init__ ( self ) -> None : \"\"\"Class to calculate statistics of collective events.\"\"\" pass def _calculate_duration_size_group ( self , data : np . ndarray ) -> np . ndarray : \"\"\"Calculates duration and size for the collective event in the dataframe. Arguments: data (np.ndarray): Containing a single collective event. Returns: np.ndarray: Array containing collid, duration, tot_size, min_size, max_size, \"nd_frame, first_frame_centroid and last_frame_centroid of the current collective event. \"\"\" coll_dur = max ( data [:, 0 ]) - min ( data [:, 0 ]) + 1 coll_total_size = np . unique ( data [:, 1 ]) . size ( unique , counts ) = np . unique ( data [:, 0 ], return_counts = True ) frequencies = np . asarray (( unique , counts )) . T coll_min_size = np . min ( frequencies [:, 1 ]) coll_max_size = np . max ( frequencies [:, 1 ]) coll_start_frame = np . min ( data [:, 0 ]) coll_end_frame = np . max ( data [:, 0 ]) if data . shape [ 1 ] > 3 : coll_start_coord = np . mean ( data [( data [:, 0 ] == coll_start_frame )][:, 3 :], axis = 0 ) coll_end_coord = np . mean ( data [( data [:, 0 ] == coll_end_frame )][:, 3 :], axis = 0 ) else : coll_start_coord = np . nan coll_end_coord = np . nan d = np . array ( [ data [ 0 , 2 ], coll_dur , coll_total_size , coll_min_size , coll_max_size , coll_start_frame , coll_end_frame , coll_start_coord , coll_end_coord , ], dtype = object , ) return d def _get_collev_duration ( self , data : pd . DataFrame , frame_column : str , collev_id : str , obj_id_column : str , posCol : Union [ list , None ], ) -> pd . DataFrame : \"\"\"Applies self._calculate_duration_size_group() to every group\\ i.e. every collective event. Arguments: data (DataFrame): Containing unfiltered collective events. collev_id (str): Indicating the contained collective id column. frame_column (str): Indicating the contained frame column. obj_id_column (str): Indicating object id. posCol (list | None): Contains names of position columns. If None coordinates of start and end frame are not calcualted Returns: DataFrame: DataFrame containing \"collid\", \"duration\", \"total_size\", \"min_size\",\"max_size\", \"start_frame\", \"end_frame\", \"first_frame_centroid\" and \"last_frame_centroid\" of all collective events. \"\"\" cols = [ 'collid' , \"duration\" , \"total_size\" , \"min_size\" , \"max_size\" , \"start_frame\" , \"end_frame\" , \"first_frame_centroid\" , \"last_frame_centroid\" , ] subset = [ frame_column , obj_id_column , collev_id ] if posCol : subset . extend ( posCol ) data_np = data [ subset ] . to_numpy ( dtype = np . float64 ) data_np = data_np [ ~ np . isnan ( data_np ) . any ( axis = 1 )] data_np_sorted = data_np [ data_np [:, 2 ] . argsort ()] grouped_array = np . split ( data_np_sorted , np . unique ( data_np_sorted [:, 2 ], axis = 0 , return_index = True )[ 1 ][ 1 :]) # map dbscan to grouped_array out = map ( self . _calculate_duration_size_group , grouped_array ) out_list = [ i for i in out ] df = pd . DataFrame ( out_list , columns = cols ) return df def calculate ( self , data : pd . DataFrame , frame_column : str , collid_column : str , obj_id_column : str , posCol : Union [ list , None ] = None , ) -> pd . DataFrame : \"\"\"Calculate statistics of collective events. Arguments: data (DataFrame): Containing collective events. frame_column (str): Indicating the frame column in data. collid_column (str): Indicating the collective event id column in data. obj_id_column (str): Indicating object id. posCol (list | None): Contains names of position columns. If None coordinates of start and end frame are not calcualted Returns: DataFrame: DataFrame containing \"collid\", \"duration\", \"total_size\", \"min_size\",\"max_size\", \"start_frame\", \"end_frame\", \"first_frame_centroid\" and \"last_frame_centroid\" of all collective events. \"\"\" if data . empty : return data colev_stats = self . _get_collev_duration ( data , frame_column , collid_column , obj_id_column , posCol ) return colev_stats __init__ ( self ) special \u00b6 Class to calculate statistics of collective events. Source code in arcos4py/tools/stats.py def __init__ ( self ) -> None : \"\"\"Class to calculate statistics of collective events.\"\"\" pass calculate ( self , data , frame_column , collid_column , obj_id_column , posCol = None ) \u00b6 Calculate statistics of collective events. Parameters: Name Type Description Default data DataFrame Containing collective events. required frame_column str Indicating the frame column in data. required collid_column str Indicating the collective event id column in data. required obj_id_column str Indicating object id. required posCol list | None Contains names of position columns. If None coordinates of start and end frame are not calcualted None Returns: Type Description DataFrame DataFrame containing \"collid\", \"duration\", \"total_size\", \"min_size\",\"max_size\", \"start_frame\", \"end_frame\", \"first_frame_centroid\" and \"last_frame_centroid\" of all collective events. Source code in arcos4py/tools/stats.py def calculate ( self , data : pd . DataFrame , frame_column : str , collid_column : str , obj_id_column : str , posCol : Union [ list , None ] = None , ) -> pd . DataFrame : \"\"\"Calculate statistics of collective events. Arguments: data (DataFrame): Containing collective events. frame_column (str): Indicating the frame column in data. collid_column (str): Indicating the collective event id column in data. obj_id_column (str): Indicating object id. posCol (list | None): Contains names of position columns. If None coordinates of start and end frame are not calcualted Returns: DataFrame: DataFrame containing \"collid\", \"duration\", \"total_size\", \"min_size\",\"max_size\", \"start_frame\", \"end_frame\", \"first_frame_centroid\" and \"last_frame_centroid\" of all collective events. \"\"\" if data . empty : return data colev_stats = self . _get_collev_duration ( data , frame_column , collid_column , obj_id_column , posCol ) return colev_stats","title":"Modules"},{"location":"api/#arcos4py.arcos4py","text":"Main Module of arcos4py. This module contains the ARCOS class, which implements most functionallity of arcos4py to prepare data and to detect and track collective events. Examples: >>> from arcos4py import ARCOS >>> ts = ARCOS ( data ,[ \"x\" ], 'time' , 'id' , 'meas' , 'clTrackID' ) >>> ts . interpolate_measurements () >>> ts . clip_meas ( clip_low = 0.001 , clip_high = 0.999 ) >>> ts . bin_measurements ( smoothK int = 3 , biasK = 51 , peakThr = 0.2 , binThr = 0.1 , polyDeg = 1 , biasMet = \"runmed\" ,) >>> events_df = ts . trackCollev ( eps = 1 , minClsz = 1 , nPrev = 1 )","title":"arcos4py"},{"location":"api/#arcos4py.arcos4py.ARCOS","text":"Detects and tracks collective events in a tracked time-series dataset. Requires binarized measurement column, that can be generated with the bin_measurements method. Tracking makes use of the dbscan algorithm, which is applied to every frame and subsequently connects collective events between frames located within eps distance of each other. Attributes: Name Type Description data DataFrame Data of tracked time-series in \"long format\". Can be used to acess modified dataframe at any point. posCols list List containing position column names strings inside data e.g. At least one dimension is required. frame_column str Indicating the frame column in input_data. id_column str Indicating the track id/id column in input_data. measurement_column str Indicating the measurement column in input_data. clid_column str Indicating the column name containing the collective event ids. Source code in arcos4py/arcos4py.py class ARCOS : \"\"\"Detects and tracks collective events in a tracked time-series dataset. Requires binarized measurement column, that can be generated with the bin_measurements method. Tracking makes use of the dbscan algorithm, which is applied to every frame and subsequently connects collective events between frames located within eps distance of each other. Attributes: data (DataFrame): Data of tracked time-series in \"long format\". Can be used to acess modified dataframe at any point. posCols (list): List containing position column names strings inside data e.g. At least one dimension is required. frame_column (str): Indicating the frame column in input_data. id_column (str): Indicating the track id/id column in input_data. measurement_column (str): Indicating the measurement column in input_data. clid_column (str): Indicating the column name containing the collective event ids. \"\"\" def __init__ ( self , data : pd . DataFrame , posCols : list = [ \"x\" ], frame_column : str = 'time' , id_column : str = 'id' , measurement_column : str = 'meas' , clid_column : str = 'clTrackID' , ) -> None : \"\"\"Constructs class with provided arguments. Arguments: data (DataFrame): Input Data of tracked time-series in \"long format\" containing position columns, a measurement and an object ID column. posCols (list): List ontaining position column names strings inside data e.g. At least one dimension is required. frame_column (str): Indicating the frame column in input_data. id_column (str): Indicating the track id/id column in input_data. measurement_column (str): Indicating the measurement column in input_data. clid_column (str): Indicating the column name containing the collective event ids. \"\"\" self . data = data self . posCols = posCols self . frame_column = frame_column self . id_column = id_column self . measurement_column = measurement_column self . clid_column = clid_column self . data_binarized : pd . DataFrame = None self . tracked_events : pd . DataFrame = None self . bin_col : Union [ str , None ] = None # to check if no measurement was provided assign None self . data = self . data . sort_values ( by = [ self . frame_column , self . id_column ]) self . _check_col () if self . measurement_column is not None : self . resc_col = f \" { self . measurement_column } .resc\" self . bin_col = f \" { self . measurement_column } .bin\" def __repr__ ( self ) -> pd . DataFrame : \"\"\"Set __repr___ to return self.data.\"\"\" return repr ( self . data ) def _check_col ( self ): \"\"\"Checks that self.cols contains all required columns.\"\"\" columns = self . data . columns input_columns = [ self . frame_column , self . id_column , self . id_column , self . measurement_column ] input_columns = [ col for col in input_columns if col is not None ] if not all ( item in columns for item in input_columns ): raise ValueError ( f \"Columns { input_columns } do not match with column in dataframe.\" ) def interpolate_measurements ( self ) -> pd . DataFrame : \"\"\"Interpolates NaN's in place in measurement column. Returns: Dataframe with interpolated measurement column. \"\"\" meas_interp = interpolation ( self . data ) . interpolate () self . data = meas_interp return self . data def clip_meas ( self , clip_low : float = 0.001 , clip_high : float = 0.999 ) -> pd . DataFrame : \"\"\"Clip measurement column to upper and lower quantiles defined in clip_low and clip_high. Arguments: clip_low (float): Lower clipping boundary (quantile). clip_high (float): Upper clipping boundary (quantile). Returns: Dataframe with in place clipped measurement column. \"\"\" meas_column = self . data [ self . measurement_column ] . to_numpy () meas_clipped = clipMeas ( meas_column ) . clip ( clip_low , clip_high ) self . data [ self . measurement_column ] = meas_clipped return self . data def bin_measurements ( self , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , binThr : float = 0.1 , polyDeg : int = 1 , biasMet : str = \"runmed\" , ) -> pd . DataFrame : r \"\"\"Smooth, de-trend, and binarise the input data. First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold Arguments: smoothK (int): Size of the short-term median smoothing filter. biasK (int): Size of the long-term de-trending median filter peakThr (float): Threshold for rescaling of the de-trended signal. binThr (float): Threshold for binary classification. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): De-trending method, one of ['runmed', 'lm', 'none']. Returns: DataFrame with detrended/smoothed and binarized measurement column. \"\"\" self . data = binData ( smoothK , biasK , peakThr , binThr , polyDeg , biasMet , ) . run ( self . data , colMeas = self . measurement_column , colGroup = self . id_column , colFrame = self . frame_column ) return self . data def trackCollev ( self , eps : float = 1 , minClsz : int = 1 , nPrev : int = 1 ) -> pd . DataFrame : \"\"\"Requires binarized measurement column. Makes use of the dbscan algorithm, applies this to every timeframe and subsequently connects collective events between frames located within eps distance of each other. Arguments: eps (float): The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. Value is also used to connect collective events across multiple frames. minClsz (str): The minimum size for a cluster to be identified as a collective event nPrev (int): Number of previous frames the tracking algorithm looks back to connect collective events Returns: DataFrame with detected collective events across time. \"\"\" self . data = detectCollev ( self . data , eps = eps , minClSz = minClsz , nPrev = nPrev , posCols = self . posCols , frame_column = self . frame_column , id_column = self . id_column , bin_meas_column = self . bin_col , clid_column = self . clid_column , ) . run () return self . data","title":"ARCOS"},{"location":"api/#arcos4py.arcos4py.ARCOS.__init__","text":"Constructs class with provided arguments. Parameters: Name Type Description Default data DataFrame Input Data of tracked time-series in \"long format\" containing position columns, a measurement and an object ID column. required posCols list List ontaining position column names strings inside data e.g. At least one dimension is required. ['x'] frame_column str Indicating the frame column in input_data. 'time' id_column str Indicating the track id/id column in input_data. 'id' measurement_column str Indicating the measurement column in input_data. 'meas' clid_column str Indicating the column name containing the collective event ids. 'clTrackID' Source code in arcos4py/arcos4py.py def __init__ ( self , data : pd . DataFrame , posCols : list = [ \"x\" ], frame_column : str = 'time' , id_column : str = 'id' , measurement_column : str = 'meas' , clid_column : str = 'clTrackID' , ) -> None : \"\"\"Constructs class with provided arguments. Arguments: data (DataFrame): Input Data of tracked time-series in \"long format\" containing position columns, a measurement and an object ID column. posCols (list): List ontaining position column names strings inside data e.g. At least one dimension is required. frame_column (str): Indicating the frame column in input_data. id_column (str): Indicating the track id/id column in input_data. measurement_column (str): Indicating the measurement column in input_data. clid_column (str): Indicating the column name containing the collective event ids. \"\"\" self . data = data self . posCols = posCols self . frame_column = frame_column self . id_column = id_column self . measurement_column = measurement_column self . clid_column = clid_column self . data_binarized : pd . DataFrame = None self . tracked_events : pd . DataFrame = None self . bin_col : Union [ str , None ] = None # to check if no measurement was provided assign None self . data = self . data . sort_values ( by = [ self . frame_column , self . id_column ]) self . _check_col () if self . measurement_column is not None : self . resc_col = f \" { self . measurement_column } .resc\" self . bin_col = f \" { self . measurement_column } .bin\"","title":"__init__()"},{"location":"api/#arcos4py.arcos4py.ARCOS.__repr__","text":"Set __repr___ to return self.data. Source code in arcos4py/arcos4py.py def __repr__ ( self ) -> pd . DataFrame : \"\"\"Set __repr___ to return self.data.\"\"\" return repr ( self . data )","title":"__repr__()"},{"location":"api/#arcos4py.arcos4py.ARCOS.bin_measurements","text":"Smooth, de-trend, and binarise the input data. First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold Parameters: Name Type Description Default smoothK int Size of the short-term median smoothing filter. 3 biasK int Size of the long-term de-trending median filter 51 peakThr float Threshold for rescaling of the de-trended signal. 0.2 binThr float Threshold for binary classification. 0.1 polyDeg int Sets the degree of the polynomial for lm fitting. 1 biasMet str De-trending method, one of ['runmed', 'lm', 'none']. 'runmed' Returns: Type Description DataFrame DataFrame with detrended/smoothed and binarized measurement column. Source code in arcos4py/arcos4py.py def bin_measurements ( self , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , binThr : float = 0.1 , polyDeg : int = 1 , biasMet : str = \"runmed\" , ) -> pd . DataFrame : r \"\"\"Smooth, de-trend, and binarise the input data. First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold Arguments: smoothK (int): Size of the short-term median smoothing filter. biasK (int): Size of the long-term de-trending median filter peakThr (float): Threshold for rescaling of the de-trended signal. binThr (float): Threshold for binary classification. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): De-trending method, one of ['runmed', 'lm', 'none']. Returns: DataFrame with detrended/smoothed and binarized measurement column. \"\"\" self . data = binData ( smoothK , biasK , peakThr , binThr , polyDeg , biasMet , ) . run ( self . data , colMeas = self . measurement_column , colGroup = self . id_column , colFrame = self . frame_column ) return self . data","title":"bin_measurements()"},{"location":"api/#arcos4py.arcos4py.ARCOS.clip_meas","text":"Clip measurement column to upper and lower quantiles defined in clip_low and clip_high. Parameters: Name Type Description Default clip_low float Lower clipping boundary (quantile). 0.001 clip_high float Upper clipping boundary (quantile). 0.999 Returns: Type Description DataFrame Dataframe with in place clipped measurement column. Source code in arcos4py/arcos4py.py def clip_meas ( self , clip_low : float = 0.001 , clip_high : float = 0.999 ) -> pd . DataFrame : \"\"\"Clip measurement column to upper and lower quantiles defined in clip_low and clip_high. Arguments: clip_low (float): Lower clipping boundary (quantile). clip_high (float): Upper clipping boundary (quantile). Returns: Dataframe with in place clipped measurement column. \"\"\" meas_column = self . data [ self . measurement_column ] . to_numpy () meas_clipped = clipMeas ( meas_column ) . clip ( clip_low , clip_high ) self . data [ self . measurement_column ] = meas_clipped return self . data","title":"clip_meas()"},{"location":"api/#arcos4py.arcos4py.ARCOS.interpolate_measurements","text":"Interpolates NaN's in place in measurement column. Returns: Type Description DataFrame Dataframe with interpolated measurement column. Source code in arcos4py/arcos4py.py def interpolate_measurements ( self ) -> pd . DataFrame : \"\"\"Interpolates NaN's in place in measurement column. Returns: Dataframe with interpolated measurement column. \"\"\" meas_interp = interpolation ( self . data ) . interpolate () self . data = meas_interp return self . data","title":"interpolate_measurements()"},{"location":"api/#arcos4py.arcos4py.ARCOS.trackCollev","text":"Requires binarized measurement column. Makes use of the dbscan algorithm, applies this to every timeframe and subsequently connects collective events between frames located within eps distance of each other. Parameters: Name Type Description Default eps float The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. Value is also used to connect collective events across multiple frames. 1 minClsz str The minimum size for a cluster to be identified as a collective event 1 nPrev int Number of previous frames the tracking algorithm looks back to connect collective events 1 Returns: Type Description DataFrame DataFrame with detected collective events across time. Source code in arcos4py/arcos4py.py def trackCollev ( self , eps : float = 1 , minClsz : int = 1 , nPrev : int = 1 ) -> pd . DataFrame : \"\"\"Requires binarized measurement column. Makes use of the dbscan algorithm, applies this to every timeframe and subsequently connects collective events between frames located within eps distance of each other. Arguments: eps (float): The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. Value is also used to connect collective events across multiple frames. minClsz (str): The minimum size for a cluster to be identified as a collective event nPrev (int): Number of previous frames the tracking algorithm looks back to connect collective events Returns: DataFrame with detected collective events across time. \"\"\" self . data = detectCollev ( self . data , eps = eps , minClSz = minClsz , nPrev = nPrev , posCols = self . posCols , frame_column = self . frame_column , id_column = self . id_column , bin_meas_column = self . bin_col , clid_column = self . clid_column , ) . run () return self . data","title":"trackCollev()"},{"location":"api/#arcos4py.plotting","text":"Tools for plotting collective events.","title":"plotting"},{"location":"api/#arcos4py.plotting.plotting","text":"Module to plot different metrics generated by arcos4py functions. Examples: >>> from arcos4py.plotting import plotOriginalDetrended >>> plot = arcosPlots ( data , 'time' , 'meas' , 'detrended' , 'id' ) >>> plot . plot_detrended ()","title":"plotting"},{"location":"api/#arcos4py.plotting.plotting.NoodlePlot","text":"Create Noodle Plot of cell tracks, colored by collective event id. Attributes: Name Type Description df pd.DataFrame DataFrame containing collective events from arcos. colev str Name of the collective event column in df. trackid str Name of the track column in df. frame str Name of the frame column in df. posx str Name of the X coordinate column in df. posy str Name of the Y coordinate column in df. posz str Name of the Z coordinate column in df, or None if no z column. Source code in arcos4py/plotting/plotting.py class NoodlePlot : \"\"\"Create Noodle Plot of cell tracks, colored by collective event id. Attributes: df (pd.DataFrame): DataFrame containing collective events from arcos. colev (str): Name of the collective event column in df. trackid (str): Name of the track column in df. frame (str): Name of the frame column in df. posx (str): Name of the X coordinate column in df. posy (str): Name of the Y coordinate column in df. posz (str): Name of the Z coordinate column in df, or None if no z column. \"\"\" def __init__ ( self , df : pd . DataFrame , colev : str , trackid : str , frame : str , posx : str , posy : str , posz : Union [ str , None ] = None , ): \"\"\"Constructs class with given parameters. Arguments: df (pd.DataFrame): DataFrame containing collective events from arcos. colev (str): Name of the collective event column in df. trackid (str): Name of the track column in df. frame (str): Name of the frame column in df. posx (str): Name of the X coordinate column in df. posy (str): Name of the Y coordinate column in df. posz (str | None): Name of the Z coordinate column in df, or \"None\" (str) if no z column. \"\"\" self . df = df self . colev = colev self . trackid = trackid self . frame = frame self . posx = posx self . posy = posy self . posz = posz def _prepare_data_noodleplot ( self , df : pd . DataFrame , color_cylce : list [ str ], colev : str , trackid : str , frame : str , posx : str , posy : str , posz : Union [ str , None ] = None , ): \"\"\"From arcos collective event data,\\ generates a list of numpy arrays, one for each event. Arguments: df (pd.DataFrame): DataFrame containing collective events from arcos. color_cylce (list[str]): list of colors used to color trackid's for individual collective events. colev (str): Name of the collective event column in df. trackid (str): Name of the track column in df. frame: (str): Name of the frame column in df. posx (str): Name of the X coordinate column in df. posy (str): Name of the Y coordinate column in df. posz (str): Name of the Z coordinate column in df, or None if no z column. Returns: list[np.ndarray], np.ndarray: List of collective events data, and colors for each collective event. \"\"\" # values need to be sorted to group with numpy df . sort_values ([ colev , trackid ], inplace = True ) if posz : array = df [[ colev , trackid , frame , posx , posy , posz ]] . to_numpy () else : array = df [[ colev , trackid , frame , posx , posy ]] . to_numpy () # generate goroups for each unique value grouped_array = np . split ( array , np . unique ( array [:, 0 ], axis = 0 , return_index = True )[ 1 ][ 1 :]) # make collids sequential seq_colids = np . concatenate ( [ np . repeat ( i , value . shape [ 0 ]) for i , value in enumerate ( grouped_array )], axis = 0 , ) array_seq_colids = np . column_stack (( array , seq_colids )) # split sequential collids array by trackid and collid grouped_array = np . split ( array_seq_colids , np . unique ( array_seq_colids [:, : 2 ], axis = 0 , return_index = True )[ 1 ][ 1 :], ) # generate colors for each collective event, wrap arround the color cycle colors = np . take ( np . array ( color_cylce ), [ i + 1 for i in np . unique ( seq_colids )], mode = \"wrap\" ) return grouped_array , colors def _create_noodle_plot ( self , grouped_data : np . ndarray , colors : np . ndarray ): \"\"\"Plots the noodle plot.\"\"\" fig , ax = plt . subplots () ax . set_xlabel ( \"Time Point\" ) ax . set_ylabel ( \"Position\" ) for dat in grouped_data : ax . plot ( dat [:, 2 ], dat [:, self . projection_index ], c = colors [ int ( dat [ 0 , - 1 ])], ) return fig , ax def plot ( self , projection_axis : str , color_cylce : list [ str ] = TAB20 ): \"\"\"Create Noodle Plot of cell tracks, colored by collective event id. Arguments: projection_axis (str): Specify with witch coordinate the noodle plot should be drawn. Has to be one of the posx, posy or posz arguments passed in during the class instantiation process. color_cylce (list[str]): List of hex color values or string names (i.e. ['red', 'yellow']) used to color collecitve events. Cycles through list. Returns: fig, axes: Matplotlib figure and axes are returned for the noodle plot. \"\"\" if projection_axis not in [ self . posx , self . posy , self . posz ]: raise ValueError ( f \"projection_axis has to be one of { [ self . posx , self . posy , self . posz ] } \" ) if projection_axis == self . posx : self . projection_index = 3 elif projection_axis == self . posy : self . projection_index = 4 elif projection_axis == self . posz : self . projection_index = 5 grpd_data , colors = self . _prepare_data_noodleplot ( self . df , color_cylce , self . colev , self . trackid , self . frame , self . posx , self . posy , self . posz ) fig , axes = self . _create_noodle_plot ( grpd_data , colors ) return fig , axes","title":"NoodlePlot"},{"location":"api/#arcos4py.plotting.plotting.NoodlePlot.__init__","text":"Constructs class with given parameters. Parameters: Name Type Description Default df pd.DataFrame DataFrame containing collective events from arcos. required colev str Name of the collective event column in df. required trackid str Name of the track column in df. required frame str Name of the frame column in df. required posx str Name of the X coordinate column in df. required posy str Name of the Y coordinate column in df. required posz str | None Name of the Z coordinate column in df, or \"None\" (str) if no z column. None Source code in arcos4py/plotting/plotting.py def __init__ ( self , df : pd . DataFrame , colev : str , trackid : str , frame : str , posx : str , posy : str , posz : Union [ str , None ] = None , ): \"\"\"Constructs class with given parameters. Arguments: df (pd.DataFrame): DataFrame containing collective events from arcos. colev (str): Name of the collective event column in df. trackid (str): Name of the track column in df. frame (str): Name of the frame column in df. posx (str): Name of the X coordinate column in df. posy (str): Name of the Y coordinate column in df. posz (str | None): Name of the Z coordinate column in df, or \"None\" (str) if no z column. \"\"\" self . df = df self . colev = colev self . trackid = trackid self . frame = frame self . posx = posx self . posy = posy self . posz = posz","title":"__init__()"},{"location":"api/#arcos4py.plotting.plotting.NoodlePlot.plot","text":"Create Noodle Plot of cell tracks, colored by collective event id. Parameters: Name Type Description Default projection_axis str Specify with witch coordinate the noodle plot should be drawn. Has to be one of the posx, posy or posz arguments passed in during the class instantiation process. required color_cylce list[str] List of hex color values or string names (i.e. ['red', 'yellow']) used to color collecitve events. Cycles through list. ['#1f77b4', '#aec7e8', '#ff7f0e', '#ffbb78', '#2ca02c', '#98df8a', '#d62728', '#ff9896', '#9467bd', '#c5b0d5', '#8c564b', '#c49c94', '#e377c2', '#f7b6d2', '#7f7f7f', '#c7c7c7', '#bcbd22', '#dbdb8d', '#17becf', '#9edae5'] Returns: Type Description fig, axes Matplotlib figure and axes are returned for the noodle plot. Source code in arcos4py/plotting/plotting.py def plot ( self , projection_axis : str , color_cylce : list [ str ] = TAB20 ): \"\"\"Create Noodle Plot of cell tracks, colored by collective event id. Arguments: projection_axis (str): Specify with witch coordinate the noodle plot should be drawn. Has to be one of the posx, posy or posz arguments passed in during the class instantiation process. color_cylce (list[str]): List of hex color values or string names (i.e. ['red', 'yellow']) used to color collecitve events. Cycles through list. Returns: fig, axes: Matplotlib figure and axes are returned for the noodle plot. \"\"\" if projection_axis not in [ self . posx , self . posy , self . posz ]: raise ValueError ( f \"projection_axis has to be one of { [ self . posx , self . posy , self . posz ] } \" ) if projection_axis == self . posx : self . projection_index = 3 elif projection_axis == self . posy : self . projection_index = 4 elif projection_axis == self . posz : self . projection_index = 5 grpd_data , colors = self . _prepare_data_noodleplot ( self . df , color_cylce , self . colev , self . trackid , self . frame , self . posx , self . posy , self . posz ) fig , axes = self . _create_noodle_plot ( grpd_data , colors ) return fig , axes","title":"plot()"},{"location":"api/#arcos4py.plotting.plotting.dataPlots","text":"Plot different metrics of input data. Attributes: Name Type Description data Dataframe containing ARCOS data. frame str name of frame column in data. measurement str name of measurement column in data. id str name of track id column. Source code in arcos4py/plotting/plotting.py class dataPlots : \"\"\"Plot different metrics of input data. Attributes: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. id (str): name of track id column. \"\"\" def __init__ ( self , data : pd . DataFrame , frame : str , measurement : str , id : str ): \"\"\"Plot different metrics such as histogram, position-t and density. Arguments: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. id (str): name of track id column. \"\"\" self . data = data self . id = id self . frame = frame self . measurement = measurement def position_t_plot ( self , posCol : set [ str ] = { 'x' }, n : int = 20 ): \"\"\"Plots X and Y over T to visualize tracklength. Arguments: posCol (set): containing names of position columns in data. n (int): number of samples to plot. Returns: fig, axes: Matplotlib fig and axes of density plot. \"\"\" sample = pd . Series ( self . data [ self . id ] . unique ()) . sample ( n ) pd_from_r_df = self . data . loc [ self . data [ self . id ] . isin ( sample )] fig , axes = plt . subplots ( 1 , len ( posCol ), figsize = ( 6 , 3 )) for label , df in pd_from_r_df . groupby ( self . id ): for index , value in enumerate ( posCol ): if len ( posCol ) > 1 : df . plot ( x = self . frame , y = value , ax = axes [ index ], legend = None ) else : df . plot ( x = self . frame , y = value , ax = axes , legend = None ) if len ( posCol ) > 1 : for index , value in enumerate ( posCol ): axes [ index ] . set_title ( value ) else : axes . set_title ( value ) return fig , axes def density_plot ( self , * args , ** kwargs ): \"\"\"Density plot of measurement. Uses Seaborn distplot to plot measurement density. Arguments: measurement_col (str): name of measurement column. *args (Any): arguments passed on to seaborn histplot function. **kwargs (Any): keyword arguments passed on to seaborn histplot function. Returns: FacetGrid: Seaborn FacetGrid of density density plot. \"\"\" plot = sns . displot ( self . data [ self . measurement ], kind = \"kde\" , palette = \"pastel\" , label = self . measurement , * args , ** kwargs ) # Plot formatting plt . legend ( prop = { 'size' : 10 }) plt . title ( 'Density Plot of Measurement' ) plt . xlabel ( 'Measurement' ) plt . ylabel ( 'Density' ) return plot def histogram ( self , bins : str = 'auto' , * args , ** kwargs ): \"\"\"Histogram of tracklenght. Uses seaborn histplot function to plot tracklenght histogram. Arguments: bins (str): number or width of bins in histogram *args (Any): arguments passed on to seaborn histplot function. **kwargs (Any): keyword arguments passed on to seaborn histplot function. Returns: AxesSubplot: Matplotlib AxesSubplot of histogram. \"\"\" # Draw histogram track_length = self . data . groupby ( self . id ) . size () axes = sns . histplot ( track_length , label = \"Track Length\" , bins = bins , * args , ** kwargs ) # Plot formatting plt . title ( 'Track length Histogram' ) axes . set_xlabel ( 'Track Length' ) axes . set_ylabel ( 'Count' ) return axes","title":"dataPlots"},{"location":"api/#arcos4py.plotting.plotting.dataPlots.__init__","text":"Plot different metrics such as histogram, position-t and density. Parameters: Name Type Description Default data Dataframe containing ARCOS data. required frame str name of frame column in data. required measurement str name of measurement column in data. required id str name of track id column. required Source code in arcos4py/plotting/plotting.py def __init__ ( self , data : pd . DataFrame , frame : str , measurement : str , id : str ): \"\"\"Plot different metrics such as histogram, position-t and density. Arguments: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. id (str): name of track id column. \"\"\" self . data = data self . id = id self . frame = frame self . measurement = measurement","title":"__init__()"},{"location":"api/#arcos4py.plotting.plotting.dataPlots.density_plot","text":"Density plot of measurement. Uses Seaborn distplot to plot measurement density. Parameters: Name Type Description Default measurement_col str name of measurement column. required *args Any arguments passed on to seaborn histplot function. () **kwargs Any keyword arguments passed on to seaborn histplot function. {} Returns: Type Description FacetGrid Seaborn FacetGrid of density density plot. Source code in arcos4py/plotting/plotting.py def density_plot ( self , * args , ** kwargs ): \"\"\"Density plot of measurement. Uses Seaborn distplot to plot measurement density. Arguments: measurement_col (str): name of measurement column. *args (Any): arguments passed on to seaborn histplot function. **kwargs (Any): keyword arguments passed on to seaborn histplot function. Returns: FacetGrid: Seaborn FacetGrid of density density plot. \"\"\" plot = sns . displot ( self . data [ self . measurement ], kind = \"kde\" , palette = \"pastel\" , label = self . measurement , * args , ** kwargs ) # Plot formatting plt . legend ( prop = { 'size' : 10 }) plt . title ( 'Density Plot of Measurement' ) plt . xlabel ( 'Measurement' ) plt . ylabel ( 'Density' ) return plot","title":"density_plot()"},{"location":"api/#arcos4py.plotting.plotting.dataPlots.histogram","text":"Histogram of tracklenght. Uses seaborn histplot function to plot tracklenght histogram. Parameters: Name Type Description Default bins str number or width of bins in histogram 'auto' *args Any arguments passed on to seaborn histplot function. () **kwargs Any keyword arguments passed on to seaborn histplot function. {} Returns: Type Description AxesSubplot Matplotlib AxesSubplot of histogram. Source code in arcos4py/plotting/plotting.py def histogram ( self , bins : str = 'auto' , * args , ** kwargs ): \"\"\"Histogram of tracklenght. Uses seaborn histplot function to plot tracklenght histogram. Arguments: bins (str): number or width of bins in histogram *args (Any): arguments passed on to seaborn histplot function. **kwargs (Any): keyword arguments passed on to seaborn histplot function. Returns: AxesSubplot: Matplotlib AxesSubplot of histogram. \"\"\" # Draw histogram track_length = self . data . groupby ( self . id ) . size () axes = sns . histplot ( track_length , label = \"Track Length\" , bins = bins , * args , ** kwargs ) # Plot formatting plt . title ( 'Track length Histogram' ) axes . set_xlabel ( 'Track Length' ) axes . set_ylabel ( 'Count' ) return axes","title":"histogram()"},{"location":"api/#arcos4py.plotting.plotting.dataPlots.position_t_plot","text":"Plots X and Y over T to visualize tracklength. Parameters: Name Type Description Default posCol set containing names of position columns in data. {'x'} n int number of samples to plot. 20 Returns: Type Description fig, axes Matplotlib fig and axes of density plot. Source code in arcos4py/plotting/plotting.py def position_t_plot ( self , posCol : set [ str ] = { 'x' }, n : int = 20 ): \"\"\"Plots X and Y over T to visualize tracklength. Arguments: posCol (set): containing names of position columns in data. n (int): number of samples to plot. Returns: fig, axes: Matplotlib fig and axes of density plot. \"\"\" sample = pd . Series ( self . data [ self . id ] . unique ()) . sample ( n ) pd_from_r_df = self . data . loc [ self . data [ self . id ] . isin ( sample )] fig , axes = plt . subplots ( 1 , len ( posCol ), figsize = ( 6 , 3 )) for label , df in pd_from_r_df . groupby ( self . id ): for index , value in enumerate ( posCol ): if len ( posCol ) > 1 : df . plot ( x = self . frame , y = value , ax = axes [ index ], legend = None ) else : df . plot ( x = self . frame , y = value , ax = axes , legend = None ) if len ( posCol ) > 1 : for index , value in enumerate ( posCol ): axes [ index ] . set_title ( value ) else : axes . set_title ( value ) return fig , axes","title":"position_t_plot()"},{"location":"api/#arcos4py.plotting.plotting.plotOriginalDetrended","text":"Plot different detrended vs original data. Attributes: Name Type Description data Dataframe containing ARCOS data. frame str name of frame column in data. measurement str name of measurement column in data. detrended str name of detrended column with detrended data. id str name of track id column. Source code in arcos4py/plotting/plotting.py class plotOriginalDetrended : \"\"\"Plot different detrended vs original data. Attributes: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. detrended (str): name of detrended column with detrended data. id (str): name of track id column. \"\"\" def __init__ ( self , data : pd . DataFrame , frame : str , measurement : str , detrended : str , id : str ): \"\"\"Plot detrended vs original data. Arguments: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. detrended (str): name of detrended column with detrended data. id (str): name of track id column. \"\"\" self . data = data self . measurement = measurement self . detrended = detrended self . id = id self . frame = frame def plot_detrended ( self , n_samples : int = 25 , subplots : tuple = ( 5 , 5 ), plotsize : tuple = ( 20 , 10 ) ) -> matplotlib . axes . Axes : \"\"\"Method to plot detrended vs original data. Arguments: n_samples (int): Number of tracks to plot. subplots (tuple): Number of subplots, should be approx. one per sample. plotsize (tuple): Size of generated plot. Returns: Fig, Axes: Matplotlib figure and axes2d of detrended vs original data. \"\"\" vals = np . random . choice ( self . data [ self . id ] . unique (), n_samples , replace = False ) self . data = self . data . set_index ( self . id ) . loc [ vals ] . reset_index () grouped = self . data . groupby ( self . id ) ncols = subplots [ 0 ] nrows = subplots [ 1 ] fig , axes2d = plt . subplots ( nrows = nrows , ncols = ncols , figsize = plotsize , sharey = True ) for ( key , ax ) in zip ( grouped . groups . keys (), axes2d . flatten ()): grouped . get_group ( key ) . plot ( x = self . frame , y = [ self . measurement , self . detrended ], ax = ax ) ax . get_legend () . remove () handles , labels = ax . get_legend_handles_labels () fig . legend ( handles , labels , loc = \"lower right\" ) return fig , axes2d","title":"plotOriginalDetrended"},{"location":"api/#arcos4py.plotting.plotting.plotOriginalDetrended.__init__","text":"Plot detrended vs original data. Parameters: Name Type Description Default data Dataframe containing ARCOS data. required frame str name of frame column in data. required measurement str name of measurement column in data. required detrended str name of detrended column with detrended data. required id str name of track id column. required Source code in arcos4py/plotting/plotting.py def __init__ ( self , data : pd . DataFrame , frame : str , measurement : str , detrended : str , id : str ): \"\"\"Plot detrended vs original data. Arguments: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. detrended (str): name of detrended column with detrended data. id (str): name of track id column. \"\"\" self . data = data self . measurement = measurement self . detrended = detrended self . id = id self . frame = frame","title":"__init__()"},{"location":"api/#arcos4py.plotting.plotting.plotOriginalDetrended.plot_detrended","text":"Method to plot detrended vs original data. Parameters: Name Type Description Default n_samples int Number of tracks to plot. 25 subplots tuple Number of subplots, should be approx. one per sample. (5, 5) plotsize tuple Size of generated plot. (20, 10) Returns: Type Description Fig, Axes Matplotlib figure and axes2d of detrended vs original data. Source code in arcos4py/plotting/plotting.py def plot_detrended ( self , n_samples : int = 25 , subplots : tuple = ( 5 , 5 ), plotsize : tuple = ( 20 , 10 ) ) -> matplotlib . axes . Axes : \"\"\"Method to plot detrended vs original data. Arguments: n_samples (int): Number of tracks to plot. subplots (tuple): Number of subplots, should be approx. one per sample. plotsize (tuple): Size of generated plot. Returns: Fig, Axes: Matplotlib figure and axes2d of detrended vs original data. \"\"\" vals = np . random . choice ( self . data [ self . id ] . unique (), n_samples , replace = False ) self . data = self . data . set_index ( self . id ) . loc [ vals ] . reset_index () grouped = self . data . groupby ( self . id ) ncols = subplots [ 0 ] nrows = subplots [ 1 ] fig , axes2d = plt . subplots ( nrows = nrows , ncols = ncols , figsize = plotsize , sharey = True ) for ( key , ax ) in zip ( grouped . groups . keys (), axes2d . flatten ()): grouped . get_group ( key ) . plot ( x = self . frame , y = [ self . measurement , self . detrended ], ax = ax ) ax . get_legend () . remove () handles , labels = ax . get_legend_handles_labels () fig . legend ( handles , labels , loc = \"lower right\" ) return fig , axes2d","title":"plot_detrended()"},{"location":"api/#arcos4py.plotting.plotting.statsPlots","text":"Plot data generated by the stats module. Attributes: Name Type Description data DataFrame containing ARCOS stats data. Source code in arcos4py/plotting/plotting.py class statsPlots : \"\"\"Plot data generated by the stats module. Attributes: data (DataFrame): containing ARCOS stats data. \"\"\" def __init__ ( self , data : pd . DataFrame ): \"\"\"Plot detrended vs original data. Arguments: data (DataFrame): containing ARCOS stats data. \"\"\" self . data = data def plot_events_duration ( self , total_size : str , duration : str , point_size : int = 40 , * args , ** kwargs ): \"\"\"Scatterplot of collective event duration. Arguments: total_size (str): name of total size column. duration (str):, name of column with collective event duration. point_size (int): scatterplot point size. *args (Any): Arguments passed on to seaborn scatterplot function. **kwargs (Any): Keyword arguments passed on to seaborn scatterplot function. Returns: Axes: Matplotlib Axes object of scatterplot \"\"\" plot = sns . scatterplot ( x = self . data [ total_size ], y = self . data [ duration ], s = point_size , * args , ** kwargs ) return plot","title":"statsPlots"},{"location":"api/#arcos4py.plotting.plotting.statsPlots.__init__","text":"Plot detrended vs original data. Parameters: Name Type Description Default data DataFrame containing ARCOS stats data. required Source code in arcos4py/plotting/plotting.py def __init__ ( self , data : pd . DataFrame ): \"\"\"Plot detrended vs original data. Arguments: data (DataFrame): containing ARCOS stats data. \"\"\" self . data = data","title":"__init__()"},{"location":"api/#arcos4py.plotting.plotting.statsPlots.plot_events_duration","text":"Scatterplot of collective event duration. Parameters: Name Type Description Default total_size str name of total size column. required duration str , name of column with collective event duration. required point_size int scatterplot point size. 40 *args Any Arguments passed on to seaborn scatterplot function. () **kwargs Any Keyword arguments passed on to seaborn scatterplot function. {} Returns: Type Description Axes Matplotlib Axes object of scatterplot Source code in arcos4py/plotting/plotting.py def plot_events_duration ( self , total_size : str , duration : str , point_size : int = 40 , * args , ** kwargs ): \"\"\"Scatterplot of collective event duration. Arguments: total_size (str): name of total size column. duration (str):, name of column with collective event duration. point_size (int): scatterplot point size. *args (Any): Arguments passed on to seaborn scatterplot function. **kwargs (Any): Keyword arguments passed on to seaborn scatterplot function. Returns: Axes: Matplotlib Axes object of scatterplot \"\"\" plot = sns . scatterplot ( x = self . data [ total_size ], y = self . data [ duration ], s = point_size , * args , ** kwargs ) return plot","title":"plot_events_duration()"},{"location":"api/#arcos4py.tools","text":"Tools for detecting collective events.","title":"tools"},{"location":"api/#arcos4py.tools.binarize_detrend","text":"Module containing binarization and detrending classes. Examples: >>> from arcos4py.tools import binData >>> binarizer = binData ( biasMet = \"lm\" , polyDeg = 1 ) >>> data_rescaled = binarizer . run ( data , colMeas = \"ERK_KTR\" , colGroup = \"trackID\" )","title":"binarize_detrend"},{"location":"api/#arcos4py.tools.binarize_detrend.binData","text":"Smooth, de-trend, and binarise the input data. First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold. Attributes: Name Type Description smoothK int Size of the short-term median smoothing filter. biasK int Size of the long-term de-trending median filter. peakThr float Threshold for rescaling of the de-trended signal. binThr float Threshold for binarizing the de-trended signal. polyDeg int Sets the degree of the polynomial for lm fitting. biasMet str De-trending method, one of ['runmed', 'lm', 'none']. Source code in arcos4py/tools/binarize_detrend.py class binData ( detrender ): \"\"\"Smooth, de-trend, and binarise the input data. First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold. Attributes: smoothK (int): Size of the short-term median smoothing filter. biasK (int): Size of the long-term de-trending median filter. peakThr (float): Threshold for rescaling of the de-trended signal. binThr (float): Threshold for binarizing the de-trended signal. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): De-trending method, one of ['runmed', 'lm', 'none']. \"\"\" def __init__ ( self , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , binThr : float = 0.1 , polyDeg : int = 1 , biasMet : str = \"runmed\" , ) -> None : \"\"\"Smooth, de-trend, and binarise the input data. Arguments: smoothK (int): Size of the short-term median smoothing filter. biasK (int): Size of the long-term de-trending median filter. peakThr (float): Threshold for rescaling of the de-trended signal. binThr (float): Threshold for binarizing the de-trended signal. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): De-trending method, one of ['runmed', 'lm', 'none']. \"\"\" super () . __init__ ( smoothK , biasK , peakThr , polyDeg , biasMet ) self . binThr = binThr def _rescale_data ( self , x : np . ndarray , group_index : int , meas_index : int , feat_range : tuple = ( 0 , 1 )) -> np . ndarray : grouped_array = np . split ( x [ meas_index , :], np . unique ( x [ group_index , :], axis = 0 , return_index = True )[ 1 ][ 1 :]) out = [ minmax_scale ( i , feature_range = feat_range ) for i in grouped_array ] rescaled = [ item for sublist in out for item in sublist ] # rescaled = minmax_scale(x[:,1], feature_range=feat_range) x [:, meas_index ] = rescaled return x def _bin_data ( self , x : np . ndarray ) -> np . ndarray : bin = ( x > self . binThr ) . astype ( np . int_ ) return bin def run ( self , x : pd . DataFrame , colGroup : str , colMeas : str , colFrame : str ) -> pd . DataFrame : \"\"\"Runs binarization and detrending. If the bias Method is 'none', first it rescales the data to between [0,1], then local smoothing is applied to the measurement by groups, followed by binarization. If biasMeth is one of ['lm', 'runmed'], first the data is detrended locally with a median filter and then detrended globally, for 'lm' with a linear model and for 'runmed' with a median filter. Followed by binarization of the data. Arguments: x (DataFrame): The time-series data for smoothing, detrending and binarization. colGroup (str): Object id column in x. Detrending and rescaling is performed on a per-object basis. colMeas (str): Measurement column in x on which detrending and rescaling is performed. colFrame (str): Frame column in Time-series data. Used for sorting. Returns: DataFrame: Dataframe containing binarized data, rescaled data and the original columns. \"\"\" col_resc = f \" { colMeas } .resc\" col_bin = f \" { colMeas } .bin\" cols = [ colGroup , colMeas ] x . sort_values ([ colGroup , colFrame ], inplace = True ) data_np = x [ cols ] . to_numpy () if self . biasMet == \"none\" : rescaled_data = self . _rescale_data ( data_np , group_index = 0 , meas_index = 1 ) detrended_data = self . detrend ( rescaled_data , 0 , 1 ) binarized_data = self . _bin_data ( detrended_data ) else : detrended_data = self . detrend ( data_np , group_index = 0 , meas_index = 1 ) binarized_data = self . _bin_data ( detrended_data ) x [ col_resc ] = detrended_data x [ col_bin ] = binarized_data return x","title":"binData"},{"location":"api/#arcos4py.tools.binarize_detrend.binData.__init__","text":"Smooth, de-trend, and binarise the input data. Parameters: Name Type Description Default smoothK int Size of the short-term median smoothing filter. 3 biasK int Size of the long-term de-trending median filter. 51 peakThr float Threshold for rescaling of the de-trended signal. 0.2 binThr float Threshold for binarizing the de-trended signal. 0.1 polyDeg int Sets the degree of the polynomial for lm fitting. 1 biasMet str De-trending method, one of ['runmed', 'lm', 'none']. 'runmed' Source code in arcos4py/tools/binarize_detrend.py def __init__ ( self , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , binThr : float = 0.1 , polyDeg : int = 1 , biasMet : str = \"runmed\" , ) -> None : \"\"\"Smooth, de-trend, and binarise the input data. Arguments: smoothK (int): Size of the short-term median smoothing filter. biasK (int): Size of the long-term de-trending median filter. peakThr (float): Threshold for rescaling of the de-trended signal. binThr (float): Threshold for binarizing the de-trended signal. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): De-trending method, one of ['runmed', 'lm', 'none']. \"\"\" super () . __init__ ( smoothK , biasK , peakThr , polyDeg , biasMet ) self . binThr = binThr","title":"__init__()"},{"location":"api/#arcos4py.tools.binarize_detrend.binData.run","text":"Runs binarization and detrending. If the bias Method is 'none', first it rescales the data to between [0,1], then local smoothing is applied to the measurement by groups, followed by binarization. If biasMeth is one of ['lm', 'runmed'], first the data is detrended locally with a median filter and then detrended globally, for 'lm' with a linear model and for 'runmed' with a median filter. Followed by binarization of the data. Parameters: Name Type Description Default x DataFrame The time-series data for smoothing, detrending and binarization. required colGroup str Object id column in x. Detrending and rescaling is performed on a per-object basis. required colMeas str Measurement column in x on which detrending and rescaling is performed. required colFrame str Frame column in Time-series data. Used for sorting. required Returns: Type Description DataFrame Dataframe containing binarized data, rescaled data and the original columns. Source code in arcos4py/tools/binarize_detrend.py def run ( self , x : pd . DataFrame , colGroup : str , colMeas : str , colFrame : str ) -> pd . DataFrame : \"\"\"Runs binarization and detrending. If the bias Method is 'none', first it rescales the data to between [0,1], then local smoothing is applied to the measurement by groups, followed by binarization. If biasMeth is one of ['lm', 'runmed'], first the data is detrended locally with a median filter and then detrended globally, for 'lm' with a linear model and for 'runmed' with a median filter. Followed by binarization of the data. Arguments: x (DataFrame): The time-series data for smoothing, detrending and binarization. colGroup (str): Object id column in x. Detrending and rescaling is performed on a per-object basis. colMeas (str): Measurement column in x on which detrending and rescaling is performed. colFrame (str): Frame column in Time-series data. Used for sorting. Returns: DataFrame: Dataframe containing binarized data, rescaled data and the original columns. \"\"\" col_resc = f \" { colMeas } .resc\" col_bin = f \" { colMeas } .bin\" cols = [ colGroup , colMeas ] x . sort_values ([ colGroup , colFrame ], inplace = True ) data_np = x [ cols ] . to_numpy () if self . biasMet == \"none\" : rescaled_data = self . _rescale_data ( data_np , group_index = 0 , meas_index = 1 ) detrended_data = self . detrend ( rescaled_data , 0 , 1 ) binarized_data = self . _bin_data ( detrended_data ) else : detrended_data = self . detrend ( data_np , group_index = 0 , meas_index = 1 ) binarized_data = self . _bin_data ( detrended_data ) x [ col_resc ] = detrended_data x [ col_bin ] = binarized_data return x","title":"run()"},{"location":"api/#arcos4py.tools.binarize_detrend.detrender","text":"Smooth and de-trend input data. First, a short-term median filter with size smoothK is applied to remove fast noise from the time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. Attributes: Name Type Description smoothK int Representing the size of the short-term median smoothing filter. biasK int Representing the size of the long-term de-trending median filter. peakThr float Threshold for rescaling of the de-trended signal. polyDeg int Sets the degree of the polynomial for lm fitting. biasMet str Indicating de-trending method, one of ['runmed', 'lm', 'none']. Source code in arcos4py/tools/binarize_detrend.py class detrender : \"\"\"Smooth and de-trend input data. First, a short-term median filter with size smoothK is applied to remove fast noise from the time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. Attributes: smoothK (int): Representing the size of the short-term median smoothing filter. biasK (int): Representing the size of the long-term de-trending median filter. peakThr (float): Threshold for rescaling of the de-trended signal. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): Indicating de-trending method, one of ['runmed', 'lm', 'none']. \"\"\" def __init__ ( self , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , polyDeg : int = 1 , biasMet : str = \"runmed\" , ) -> None : \"\"\"Smooth and de-trend input data. Arguments: smoothK (int): Representing the size of the short-term median smoothing filter. biasK (int): Representing the size of the long-term de-trending median filter. peakThr (float): Threshold for rescaling of the de-trended signal. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): Indicating de-trending method, one of ['runmed', 'lm', 'none']. \"\"\" # check if biasmethod contains one of these three types biasMet_types = [ \"runmed\" , \"lm\" , \"none\" ] if biasMet not in biasMet_types : raise ValueError ( f \"Invalid bias method. Expected one of: { biasMet_types } \" ) self . smoothK = smoothK self . biasK = biasK self . peakThr = peakThr self . polyDeg = polyDeg self . biasMet = biasMet def _detrend_runnmed ( self , x , filter_size , endrule_mode ): local_smoothing = median_filter ( input = x , size = filter_size , mode = endrule_mode ) return local_smoothing def _detrend_lm ( self , x , polynomial_degree ): x = np . linspace ( 1 , x . size , x . size ) . astype ( int ) . reshape (( - 1 , 1 )) transformer = PolynomialFeatures ( degree = polynomial_degree , include_bias = False ) data_ = transformer . fit_transform ( x ) model = LinearRegression () . fit ( X = data_ , y = x ) predicted_value = model . predict ( data_ ) return predicted_value def _run_detrend ( self , x : np . ndarray ) -> np . ndarray : if x . size : local_smoothed = self . _detrend_runnmed ( x , self . smoothK , \"constant\" ) if self . biasMet != \"none\" : if self . biasMet == \"runmed\" : global_smoothed = self . _detrend_runnmed ( x = local_smoothed , filter_size = self . biasK , endrule_mode = \"constant\" , ) elif self . biasMet == \"lm\" : global_smoothed = self . _detrend_lm ( local_smoothed , self . polyDeg ) local_smoothed = np . subtract ( local_smoothed , global_smoothed ) local_smoothed = np . clip ( local_smoothed , 0 , None ) if ( local_smoothed . max () - local_smoothed . min ()) > self . peakThr : local_smoothed = np . divide ( local_smoothed , local_smoothed . max ()) local_smoothed = np . nan_to_num ( local_smoothed ) else : local_smoothed = None return local_smoothed def detrend ( self , x : np . ndarray , group_index : int , meas_index : int ) -> np . ndarray : \"\"\"Run detrinding on input data. The method applies detrending to each group defined in group_col and outputs it into the resc_column. Arguments: x (np.ndarray): Time series data for smoothing. group_index (int): Index of measurement column in x. meas_index (int): Index of id column in x. Returns (np.ndarray): Dataframe containing rescaled column. \"\"\" grouped_array = np . split ( x [:, meas_index ], np . unique ( x [:, group_index ], axis = 0 , return_index = True )[ 1 ][ 1 :]) out = [ self . _run_detrend ( x ) for x in grouped_array ] out_list = [ item for sublist in out for item in sublist ] return np . array ( out_list )","title":"detrender"},{"location":"api/#arcos4py.tools.binarize_detrend.detrender.__init__","text":"Smooth and de-trend input data. Parameters: Name Type Description Default smoothK int Representing the size of the short-term median smoothing filter. 3 biasK int Representing the size of the long-term de-trending median filter. 51 peakThr float Threshold for rescaling of the de-trended signal. 0.2 polyDeg int Sets the degree of the polynomial for lm fitting. 1 biasMet str Indicating de-trending method, one of ['runmed', 'lm', 'none']. 'runmed' Source code in arcos4py/tools/binarize_detrend.py def __init__ ( self , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , polyDeg : int = 1 , biasMet : str = \"runmed\" , ) -> None : \"\"\"Smooth and de-trend input data. Arguments: smoothK (int): Representing the size of the short-term median smoothing filter. biasK (int): Representing the size of the long-term de-trending median filter. peakThr (float): Threshold for rescaling of the de-trended signal. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): Indicating de-trending method, one of ['runmed', 'lm', 'none']. \"\"\" # check if biasmethod contains one of these three types biasMet_types = [ \"runmed\" , \"lm\" , \"none\" ] if biasMet not in biasMet_types : raise ValueError ( f \"Invalid bias method. Expected one of: { biasMet_types } \" ) self . smoothK = smoothK self . biasK = biasK self . peakThr = peakThr self . polyDeg = polyDeg self . biasMet = biasMet","title":"__init__()"},{"location":"api/#arcos4py.tools.binarize_detrend.detrender.detrend","text":"Run detrinding on input data. The method applies detrending to each group defined in group_col and outputs it into the resc_column. Parameters: Name Type Description Default x np.ndarray Time series data for smoothing. required group_index int Index of measurement column in x. required meas_index int Index of id column in x. required Returns (np.ndarray): Dataframe containing rescaled column. Source code in arcos4py/tools/binarize_detrend.py def detrend ( self , x : np . ndarray , group_index : int , meas_index : int ) -> np . ndarray : \"\"\"Run detrinding on input data. The method applies detrending to each group defined in group_col and outputs it into the resc_column. Arguments: x (np.ndarray): Time series data for smoothing. group_index (int): Index of measurement column in x. meas_index (int): Index of id column in x. Returns (np.ndarray): Dataframe containing rescaled column. \"\"\" grouped_array = np . split ( x [:, meas_index ], np . unique ( x [:, group_index ], axis = 0 , return_index = True )[ 1 ][ 1 :]) out = [ self . _run_detrend ( x ) for x in grouped_array ] out_list = [ item for sublist in out for item in sublist ] return np . array ( out_list )","title":"detrend()"},{"location":"api/#arcos4py.tools.cleandata","text":"Module containing clipping and interpolation classes. Examples: >>> # Interpolation >>> from arcos4py.tools import interpolation >>> a = interpolation ( data ) >>> data_interp = a . interpolate () >>> # clipping >>> from arcos4py.tools import clipMeas >>> a = clipMeas ( data ) >>> data_clipped = a . clip ( 0.001 , 0.999 )","title":"cleandata"},{"location":"api/#arcos4py.tools.cleandata.clipMeas","text":"Clip input array. Source code in arcos4py/tools/cleandata.py class clipMeas : \"\"\"Clip input array.\"\"\" def __init__ ( self , data : np . ndarray ) -> None : \"\"\"Clips array to quantilles. Arguments: data (ndarray): To be clipped. \"\"\" self . data = data def _calculate_percentile ( self , data : np . ndarray , clip_low : float , clip_high : float ): \"\"\"Calculate upper and lower quantille. Arguments: data (ndarray): To calculate upper and lower quantile on. clip_low (float): Lower clipping boundary (quantile). clip_high (float): Upper clipping boundry (quantille). Returns: np.ndarray: Array with lower quantile and array with upper quantile. \"\"\" quantille_low = np . quantile ( data , clip_low , keepdims = True ) quantille_high = np . quantile ( data , clip_high , keepdims = True ) return quantille_low , quantille_high def clip ( self , clip_low : float = 0.001 , clip_high : float = 0.999 ) -> np . ndarray : \"\"\"Clip input array to upper and lower quantiles defined in clip_low and clip_high. Arguments: clip_low (float): Lower clipping boundary (quantile). clip_high (float): Upper clipping boundry (quantille). Returns: np.ndarray: A clipped array of the input data. \"\"\" low , high = self . _calculate_percentile ( self . data , clip_low , clip_high ) out = self . data . clip ( low , high ) return out","title":"clipMeas"},{"location":"api/#arcos4py.tools.cleandata.clipMeas.__init__","text":"Clips array to quantilles. Parameters: Name Type Description Default data ndarray To be clipped. required Source code in arcos4py/tools/cleandata.py def __init__ ( self , data : np . ndarray ) -> None : \"\"\"Clips array to quantilles. Arguments: data (ndarray): To be clipped. \"\"\" self . data = data","title":"__init__()"},{"location":"api/#arcos4py.tools.cleandata.clipMeas.clip","text":"Clip input array to upper and lower quantiles defined in clip_low and clip_high. Parameters: Name Type Description Default clip_low float Lower clipping boundary (quantile). 0.001 clip_high float Upper clipping boundry (quantille). 0.999 Returns: Type Description np.ndarray A clipped array of the input data. Source code in arcos4py/tools/cleandata.py def clip ( self , clip_low : float = 0.001 , clip_high : float = 0.999 ) -> np . ndarray : \"\"\"Clip input array to upper and lower quantiles defined in clip_low and clip_high. Arguments: clip_low (float): Lower clipping boundary (quantile). clip_high (float): Upper clipping boundry (quantille). Returns: np.ndarray: A clipped array of the input data. \"\"\" low , high = self . _calculate_percentile ( self . data , clip_low , clip_high ) out = self . data . clip ( low , high ) return out","title":"clip()"},{"location":"api/#arcos4py.tools.cleandata.interpolation","text":"Interpolate nan values in a numpy array. Attributes: Name Type Description data DataFrame Where NaN should be replaced with interpolated values. Source code in arcos4py/tools/cleandata.py class interpolation : \"\"\"Interpolate nan values in a numpy array. Attributes: data (DataFrame): Where NaN should be replaced with interpolated values. \"\"\" def __init__ ( self , data : pd . DataFrame ): \"\"\"Interpolate nan values in a pandas dataframe. Uses pandas.interpolate with liner interpolation. Arguments: data (DataFrame): Where NaN should be replaced with interpolated values. \"\"\" self . data = data def interpolate ( self ) -> pd . DataFrame : \"\"\"Interpolate nan and missing values. Returns: DataFrame: Interpolated input data. \"\"\" self . data = self . data . interpolate ( axis = 0 ) return self . data","title":"interpolation"},{"location":"api/#arcos4py.tools.cleandata.interpolation.__init__","text":"Interpolate nan values in a pandas dataframe. Uses pandas.interpolate with liner interpolation. Parameters: Name Type Description Default data DataFrame Where NaN should be replaced with interpolated values. required Source code in arcos4py/tools/cleandata.py def __init__ ( self , data : pd . DataFrame ): \"\"\"Interpolate nan values in a pandas dataframe. Uses pandas.interpolate with liner interpolation. Arguments: data (DataFrame): Where NaN should be replaced with interpolated values. \"\"\" self . data = data","title":"__init__()"},{"location":"api/#arcos4py.tools.cleandata.interpolation.interpolate","text":"Interpolate nan and missing values. Returns: Type Description DataFrame Interpolated input data. Source code in arcos4py/tools/cleandata.py def interpolate ( self ) -> pd . DataFrame : \"\"\"Interpolate nan and missing values. Returns: DataFrame: Interpolated input data. \"\"\" self . data = self . data . interpolate ( axis = 0 ) return self . data","title":"interpolate()"},{"location":"api/#arcos4py.tools.detect_events","text":"Module to track and detect collective events. Examples: >>> from arcos4py.tools import detectCollev >>> ts = detectCollev ( data ) >>> events_df = ts . run ()","title":"detect_events"},{"location":"api/#arcos4py.tools.detect_events.detectCollev","text":"Identifies and tracks collective signalling events. Requires binarized measurement column. Makes use of the dbscan algorithm, applies this to every timeframe and subsequently connects collective events between frames located within eps distance of each other. Attributes: Name Type Description input_data DataFrame Input data to be processed. Must contain a binarized measurement column. eps float The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. Value is also used to connect collective events across multiple frames. minClSz int Minimum size for a cluster to be identified as a collective event. nPrev int Number of previous frames the tracking algorithm looks back to connect collective events. posCols list List of position columns contained in the data. Must at least contain one frame_column str Indicating the frame column in input_data. id_column str Indicating the track id/id column in input_data. bin_meas_column str Indicating the bin_meas_column in input_data or None. clid_column str Indicating the column name containing the ids of collective events. Source code in arcos4py/tools/detect_events.py class detectCollev : \"\"\"Identifies and tracks collective signalling events. Requires binarized measurement column. Makes use of the dbscan algorithm, applies this to every timeframe and subsequently connects collective events between frames located within eps distance of each other. Attributes: input_data (DataFrame): Input data to be processed. Must contain a binarized measurement column. eps (float): The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. Value is also used to connect collective events across multiple frames. minClSz (int): Minimum size for a cluster to be identified as a collective event. nPrev (int): Number of previous frames the tracking algorithm looks back to connect collective events. posCols (list): List of position columns contained in the data. Must at least contain one frame_column (str): Indicating the frame column in input_data. id_column (str): Indicating the track id/id column in input_data. bin_meas_column (str): Indicating the bin_meas_column in input_data or None. clid_column (str): Indicating the column name containing the ids of collective events. \"\"\" def __init__ ( self , input_data : pd . DataFrame , eps : float = 1 , minClSz : int = 1 , nPrev : int = 1 , posCols : list = [ \"x\" ], frame_column : str = 'time' , id_column : Union [ str , None ] = None , bin_meas_column : Union [ str , None ] = 'meas' , clid_column : str = 'clTrackID' , ) -> None : \"\"\"Constructs class with input parameters. Arguments: input_data (DataFrame): Input data to be processed. Must contain a binarized measurement column. eps (float): The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. Value is also used to connect collective events across multiple frames. minClSz (int): Minimum size for a cluster to be identified as a collective event. nPrev (int): Number of previous frames the tracking algorithm looks back to connect collective events. posCols (list): List of position columns contained in the data. Must at least contain one frame_column (str): Indicating the frame column in input_data. id_column (str | None): Indicating the track id/id column in input_data, optional. bin_meas_column (str): Indicating the bin_meas_column in input_data or None. clid_column (str): Indicating the column name containing the ids of collective events. \"\"\" # assign some variables passed in as arguments to the object self . input_data = input_data self . eps = eps self . minClSz = minClSz self . nPrev = nPrev self . frame_column = frame_column self . id_column = id_column self . bin_meas_column = bin_meas_column self . clid_column = clid_column self . posCols = posCols self . columns_input = self . input_data . columns self . clidFrame = f ' { clid_column } .frame' self . pos_cols_inputdata = [ col for col in self . posCols if col in self . columns_input ] # run input checks self . _run_input_checks () def _check_input_data ( self ): \"\"\"Checks if input contains data\\ raises error if not.\"\"\" if self . input_data is None : raise noDataError ( \"Input is None\" ) elif self . input_data . empty : raise noDataError ( \"Input is empty\" ) def _check_pos_columns ( self ): \"\"\"Checks if Input contains correct columns\\ raises Exception if not.\"\"\" if not all ( item in self . columns_input for item in self . posCols ): raise columnError ( \"Input data does not have the indicated position columns!\" ) def _check_frame_column ( self ): if self . frame_column not in self . columns_input : raise columnError ( \"Input data does not have the indicated frame column!\" ) def _check_eps ( self ): \"\"\"Checks if eps is greater than 0.\"\"\" if self . eps <= 0 : raise epsError ( \"eps has to be greater than 0\" ) def _check_minClSz ( self ): \"\"\"Checks if minClSiz is greater than 0.\"\"\" if self . minClSz <= 0 : raise minClSzError ( \"Parameter minClSiz has to be greater than 0!\" ) def _check_nPrev ( self ): \"\"\"Checks if nPrev is greater than 0.\"\"\" if self . nPrev <= 0 and isinstance ( self . nPrev , int ): raise nPrevError ( \"Parameter nPrev has to be an integer greater than 0 and an integer!\" ) def _run_input_checks ( self ): \"\"\"Run input checks.\"\"\" self . _check_input_data () self . _check_pos_columns () self . _check_eps () self . _check_minClSz () self . _check_nPrev () self . _check_frame_column () def _select_necessary_columns ( self , data : pd . DataFrame , frame_col : str , id_col : Union [ str , None ], pos_col : list , bin_col : Union [ str , None ] ) -> pd . DataFrame : \"\"\"Select necessary input colums from input data into dataframe. Arguments: data (DataFrame): Containing necessary columns. frame_col (str): Frame column in data. id_col (str): Id column in data. pos_col (list): string representation of position columns in data. bin_col (str): Name of binary column. Returns: DataFrame: Filtered columns necessary for calculation. \"\"\" columns = [ frame_col , id_col , bin_col ] columns = [ col for col in columns if col ] columns . extend ( pos_col ) neccessary_data = data [ columns ] . copy ( deep = True ) return neccessary_data def _filter_active ( self , data : pd . DataFrame , bin_meas_col : Union [ str , None ]) -> pd . DataFrame : \"\"\"Selects rows with binary value of greater than 0. Arguments: data (DataFrame): Dataframe containing necessary columns. bin_meas_col (str|None): Either name of the binary column or None if no such column exists. Returns: DataFrame: Filtered pandas DataFrame. \"\"\" if bin_meas_col is not None : data = data [ data [ bin_meas_col ] > 0 ] return data def _dbscan ( self , x : np . ndarray ) -> list : \"\"\"Dbscan method to run and merge the cluster id labels to the original dataframe. Arguments: x (np.ndarray): With unique frame and position columns. collid_col (str): Column to be created containing cluster-id labels. Returns: list[np.ndarray]: list with added collective id column detected by DBSCAN. \"\"\" db_array = DBSCAN ( eps = self . eps , min_samples = self . minClSz , algorithm = \"kd_tree\" ) . fit ( x [:, 1 :]) cluster_labels = db_array . labels_ cluster_list = [ id + 1 if id > - 1 else np . nan for id in cluster_labels ] return cluster_list def _run_dbscan ( self , data : pd . DataFrame , frame : str , clid_frame : str , id_column : Union [ str , None ]) -> pd . DataFrame : \"\"\"Apply dbscan method to every group i.e. frame. Arguments: data (DataFrame): Must contain position columns and frame columns. frame (str): Name of frame column in data. clid_frame (str): column to be created containing the output cluster ids from dbscan. id_column (str | None): track_id column Returns: DataFrame: Dataframe with added collective id column detected by DBSCAN for every frame. \"\"\" if self . id_column : data = data . sort_values ([ frame , id_column ]) . reset_index ( drop = True ) else : data = data . sort_values ([ frame ]) . reset_index ( drop = True ) subset = [ frame ] + self . pos_cols_inputdata data_np = data [ subset ] . to_numpy ( dtype = np . float64 ) grouped_array = np . split ( data_np , np . unique ( data_np [:, 0 ], axis = 0 , return_index = True )[ 1 ][ 1 :]) # map dbscan to grouped_array out = [ self . _dbscan ( i ) for i in grouped_array ] out_list = [ item for sublist in out for item in sublist ] data [ clid_frame ] = out_list data = data . dropna () return data def _make_db_id_unique ( self , db_data : pd . DataFrame , frame : str , clid_frame , clid ) -> pd . DataFrame : \"\"\"Make db_scan cluster id labels unique by adding the\\ cummulative sum of previous group to next group. Arguments: db_data (DataFrame): Returned by _run_dbscan function with non-unique cluster ids. frame (str): Frame column. clid_frame (str): Column name of cluster-id per frame. clid (str): Column name of unique cluster ids to be returned. Returns: DataFrame: Dataframe with unique collective events. \"\"\" db_data_np = db_data [[ frame , clid_frame ]] . to_numpy () grouped_array = np . split ( db_data_np [:, 1 ], np . unique ( db_data_np [:, 0 ], axis = 0 , return_index = True )[ 1 ][ 1 :]) max_array = [ 0 ] + [ np . max ( i ) for i in grouped_array if i . size != 0 ] out = [ np . add ( value , np . cumsum ( max_array )[ i ]) for i , value in enumerate ( grouped_array )] db_gp = np . concatenate ( out ) db_data [ clid ] = db_gp . astype ( np . int64 ) return db_data def _nearest_neighbour ( self , data_a : np . ndarray , data_b : np . ndarray , nbr_nearest_neighbours : int = 1 , ): \"\"\"Calculates nearest neighbour in from data_a\\ to data_b nearest_neighbours in data_b. Arguments: data_a (DataFrame): containing position values. data_b (DataFrame): containing position values. nbr_nearest_neighbours (int): of the number of nearest neighbours to be calculated. Returns: tuple(np.ndarray, np.ndarray): Returns tuple of 2 arrays containing nearest neighbour indices and distances. \"\"\" kdB = KDTree ( data = data_a ) nearest_neighbours = kdB . query ( data_b , k = nbr_nearest_neighbours ) return nearest_neighbours def _link_clusters_between_frames ( self , data : pd . DataFrame , frame : str , colid : str ) -> pd . DataFrame : \"\"\"Tracks clusters detected with DBSCAN along a frame axis,\\ returns tracked collective events as a pandas dataframe. Arguments: data (DataFrame): Output from dbscan. frame (str): Frame column. colid (str): Colid column. Returns: DataFrame: Pandas dataframe with tracked collective ids. \"\"\" essential_cols = [ frame , colid ] + self . posCols data_essential = data [ essential_cols ] data_np = data_essential . to_numpy () data_np_frame = data_np [:, 0 ] # loop over all frames to link detected clusters iteratively for t in np . unique ( data_np_frame , return_index = False )[ 1 :]: prev_frame = data_np [( data_np_frame >= ( t - self . nPrev )) & ( data_np_frame < t )] current_frame = data_np [ data_np_frame == t ] # only continue if objects were detected in previous frame if prev_frame . size : colid_current = current_frame [:, 1 ] # loop over unique cluster in frame for cluster in np . unique ( colid_current , return_index = False ): pos_current = current_frame [:, 2 :][ colid_current == cluster ] pos_previous = prev_frame [:, 2 :] # calculate nearest neighbour between previoius and current frame nn_dist , nn_indices = self . _nearest_neighbour ( pos_previous , pos_current ) prev_cluster_nbr_all = prev_frame [ nn_indices , 1 ] prev_cluster_nbr_eps = prev_cluster_nbr_all [( nn_dist <= self . eps )] # only continue if neighbours # were detected within eps distance if prev_cluster_nbr_eps . size : prev_clusternbr_eps_unique = np . unique ( prev_cluster_nbr_eps , return_index = False ) if prev_clusternbr_eps_unique . size > 0 : # propagate cluster id from previous frame data_np [(( data_np_frame == t ) & ( data_np [:, 1 ] == cluster )), 1 ] = prev_cluster_nbr_all np_out = data_np [:, 1 ] sorter = np_out . argsort ()[:: 1 ] grouped_array = np . split ( np_out [ sorter ], np . unique ( np_out [ sorter ], axis = 0 , return_index = True )[ 1 ][ 1 :]) np_grouped_consecutive = ( np . repeat ( i + 1 , value . size ) for i , value in enumerate ( grouped_array )) out_array = np . array ([ item for sublist in np_grouped_consecutive for item in sublist ]) data [ colid ] = out_array [ sorter . argsort ()] . astype ( 'int64' ) return data def _get_export_columns ( self ): \"\"\"Get columns that will contained in the pandas dataframe returned by the run method.\"\"\" self . pos_cols_inputdata = [ col for col in self . posCols if col in self . columns_input ] if self . id_column : columns = [ self . frame_column , self . id_column ] else : columns = [ self . frame_column ] columns . extend ( self . pos_cols_inputdata ) columns . append ( self . clid_column ) return columns def run ( self ) -> pd . DataFrame : \"\"\"Method to execute the different steps necessary for tracking. 1. Selects columns. 2. filters data on binary column > 1. 3. Applies dbscan algorithm to every frame. 4. Makes cluster ids unique across frames. 5. Tracks collective events i.e. links cluster ids across frames. 6. Creates final DataFrame. Returns (Dataframe): Dataframe with tracked collective events is returned. \"\"\" filtered_cols = self . _select_necessary_columns ( self . input_data , self . frame_column , self . id_column , self . pos_cols_inputdata , self . bin_meas_column , ) active_data = self . _filter_active ( filtered_cols , self . bin_meas_column ) db_data = self . _run_dbscan ( data = active_data , frame = self . frame_column , clid_frame = self . clidFrame , id_column = self . id_column , ) db_data = self . _make_db_id_unique ( db_data , frame = self . frame_column , clid_frame = self . clidFrame , clid = self . clid_column , ) tracked_events = self . _link_clusters_between_frames ( db_data , self . frame_column , self . clid_column ) return_columns = self . _get_export_columns () tracked_events = tracked_events [ return_columns ] if self . clid_column in self . input_data . columns : df_to_merge = self . input_data . drop ( columns = [ self . clid_column ]) else : df_to_merge = self . input_data tracked_events = tracked_events . merge ( df_to_merge , how = \"left\" ) tracked_events = tracked_events return tracked_events","title":"detectCollev"},{"location":"api/#arcos4py.tools.detect_events.detectCollev.__init__","text":"Constructs class with input parameters. Parameters: Name Type Description Default input_data DataFrame Input data to be processed. Must contain a binarized measurement column. required eps float The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. Value is also used to connect collective events across multiple frames. 1 minClSz int Minimum size for a cluster to be identified as a collective event. 1 nPrev int Number of previous frames the tracking algorithm looks back to connect collective events. 1 posCols list List of position columns contained in the data. Must at least contain one ['x'] frame_column str Indicating the frame column in input_data. 'time' id_column str | None Indicating the track id/id column in input_data, optional. None bin_meas_column str Indicating the bin_meas_column in input_data or None. 'meas' clid_column str Indicating the column name containing the ids of collective events. 'clTrackID' Source code in arcos4py/tools/detect_events.py def __init__ ( self , input_data : pd . DataFrame , eps : float = 1 , minClSz : int = 1 , nPrev : int = 1 , posCols : list = [ \"x\" ], frame_column : str = 'time' , id_column : Union [ str , None ] = None , bin_meas_column : Union [ str , None ] = 'meas' , clid_column : str = 'clTrackID' , ) -> None : \"\"\"Constructs class with input parameters. Arguments: input_data (DataFrame): Input data to be processed. Must contain a binarized measurement column. eps (float): The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. Value is also used to connect collective events across multiple frames. minClSz (int): Minimum size for a cluster to be identified as a collective event. nPrev (int): Number of previous frames the tracking algorithm looks back to connect collective events. posCols (list): List of position columns contained in the data. Must at least contain one frame_column (str): Indicating the frame column in input_data. id_column (str | None): Indicating the track id/id column in input_data, optional. bin_meas_column (str): Indicating the bin_meas_column in input_data or None. clid_column (str): Indicating the column name containing the ids of collective events. \"\"\" # assign some variables passed in as arguments to the object self . input_data = input_data self . eps = eps self . minClSz = minClSz self . nPrev = nPrev self . frame_column = frame_column self . id_column = id_column self . bin_meas_column = bin_meas_column self . clid_column = clid_column self . posCols = posCols self . columns_input = self . input_data . columns self . clidFrame = f ' { clid_column } .frame' self . pos_cols_inputdata = [ col for col in self . posCols if col in self . columns_input ] # run input checks self . _run_input_checks ()","title":"__init__()"},{"location":"api/#arcos4py.tools.detect_events.detectCollev.run","text":"Method to execute the different steps necessary for tracking. Selects columns. filters data on binary column > 1. Applies dbscan algorithm to every frame. Makes cluster ids unique across frames. Tracks collective events i.e. links cluster ids across frames. Creates final DataFrame. Returns (Dataframe): Dataframe with tracked collective events is returned. Source code in arcos4py/tools/detect_events.py def run ( self ) -> pd . DataFrame : \"\"\"Method to execute the different steps necessary for tracking. 1. Selects columns. 2. filters data on binary column > 1. 3. Applies dbscan algorithm to every frame. 4. Makes cluster ids unique across frames. 5. Tracks collective events i.e. links cluster ids across frames. 6. Creates final DataFrame. Returns (Dataframe): Dataframe with tracked collective events is returned. \"\"\" filtered_cols = self . _select_necessary_columns ( self . input_data , self . frame_column , self . id_column , self . pos_cols_inputdata , self . bin_meas_column , ) active_data = self . _filter_active ( filtered_cols , self . bin_meas_column ) db_data = self . _run_dbscan ( data = active_data , frame = self . frame_column , clid_frame = self . clidFrame , id_column = self . id_column , ) db_data = self . _make_db_id_unique ( db_data , frame = self . frame_column , clid_frame = self . clidFrame , clid = self . clid_column , ) tracked_events = self . _link_clusters_between_frames ( db_data , self . frame_column , self . clid_column ) return_columns = self . _get_export_columns () tracked_events = tracked_events [ return_columns ] if self . clid_column in self . input_data . columns : df_to_merge = self . input_data . drop ( columns = [ self . clid_column ]) else : df_to_merge = self . input_data tracked_events = tracked_events . merge ( df_to_merge , how = \"left\" ) tracked_events = tracked_events return tracked_events","title":"run()"},{"location":"api/#arcos4py.tools.filter_events","text":"Module to filter collective events. Examples: >>> from arcos4py.tools import filterCollev >>> f = filterCollev ( data , 'time' , 'collid' ) >>> df = f . filter ( coll_duration = 9 , coll_total_size = 10 )","title":"filter_events"},{"location":"api/#arcos4py.tools.filter_events.filterCollev","text":"Select Collective events that last longer than coll_duration and have a larger total size than coll_total_size. Attributes: Name Type Description data Dataframe With detected collective events. frame_column str Indicating the frame column in data. collid_column str Indicating the collective event id column in data. obj_id_column str Inidicating the object identifier column such as cell track id. Source code in arcos4py/tools/filter_events.py class filterCollev : \"\"\"Select Collective events that last longer than coll_duration\\ and have a larger total size than coll_total_size. Attributes: data (Dataframe): With detected collective events. frame_column (str): Indicating the frame column in data. collid_column (str): Indicating the collective event id column in data. obj_id_column (str): Inidicating the object identifier column such as cell track id. \"\"\" def __init__ ( self , data : pd . DataFrame , frame_column : str = \"time\" , collid_column : str = \"collid\" , obj_id_column : str = \"trackID\" , ): \"\"\"Constructs filterCollev class with Parameters. Arguments: data (Dataframe): With detected collective events. frame_column (str): Indicating the frame column in data. collid_column (str): Indicating the collective event id column in data. obj_id_column (str): Inidicating the object identifier column such as cell track id. \"\"\" self . data = data self . frame_column = frame_column self . collid_column = collid_column self . obj_id_column = obj_id_column def _filter_collev ( self , data : pd . DataFrame , collev_stats : pd . DataFrame , collev_id : str , min_duration : int , min_size : int , ): \"\"\"Uses the dataframe generated by self._get_collev_duration()\\ to filter collective events that last longer than\\ min_duration and are larger than min_size. Arguments: data (DataFrame): Containing unfiltered collective events. collev_stats (DataFrame): Containing stats of collective events. collev_id (str): Indicating the contained collective id column. min_duration (str): minimal duration of a collective event for it to be returned. min_size (int): minimal size for a collective event to be returned. Returns: DataFrame: Dataframe containing filtered collective events. \"\"\" collev_stats = collev_stats [ ( collev_stats [ \"duration\" ] >= min_duration ) & ( collev_stats [ \"total_size\" ] >= min_size ) ] data = data [ data [ collev_id ] . isin ( collev_stats [ collev_id ])] return data def filter ( self , coll_duration : int = 9 , coll_total_size : int = 10 ) -> pd . DataFrame : \"\"\"Filter collective events. Method to filter collective events according to the parameters specified in the object instance. Arguments: coll_duration (int): Minimal duration of collective events to be selected. coll_total_size (int): Minimal total size of collective events to be selected. Returns: Returns pandas dataframe containing filtered collective events \"\"\" if self . data . empty : return self . data stats = calcCollevStats () colev_duration = stats . calculate ( self . data , self . frame_column , self . collid_column , self . obj_id_column ) filtered_df = self . _filter_collev ( self . data , colev_duration , self . collid_column , coll_duration , coll_total_size , ) return filtered_df","title":"filterCollev"},{"location":"api/#arcos4py.tools.filter_events.filterCollev.__init__","text":"Constructs filterCollev class with Parameters. Parameters: Name Type Description Default data Dataframe With detected collective events. required frame_column str Indicating the frame column in data. 'time' collid_column str Indicating the collective event id column in data. 'collid' obj_id_column str Inidicating the object identifier column such as cell track id. 'trackID' Source code in arcos4py/tools/filter_events.py def __init__ ( self , data : pd . DataFrame , frame_column : str = \"time\" , collid_column : str = \"collid\" , obj_id_column : str = \"trackID\" , ): \"\"\"Constructs filterCollev class with Parameters. Arguments: data (Dataframe): With detected collective events. frame_column (str): Indicating the frame column in data. collid_column (str): Indicating the collective event id column in data. obj_id_column (str): Inidicating the object identifier column such as cell track id. \"\"\" self . data = data self . frame_column = frame_column self . collid_column = collid_column self . obj_id_column = obj_id_column","title":"__init__()"},{"location":"api/#arcos4py.tools.filter_events.filterCollev.filter","text":"Filter collective events. Method to filter collective events according to the parameters specified in the object instance. Parameters: Name Type Description Default coll_duration int Minimal duration of collective events to be selected. 9 coll_total_size int Minimal total size of collective events to be selected. 10 Returns: Type Description DataFrame Returns pandas dataframe containing filtered collective events Source code in arcos4py/tools/filter_events.py def filter ( self , coll_duration : int = 9 , coll_total_size : int = 10 ) -> pd . DataFrame : \"\"\"Filter collective events. Method to filter collective events according to the parameters specified in the object instance. Arguments: coll_duration (int): Minimal duration of collective events to be selected. coll_total_size (int): Minimal total size of collective events to be selected. Returns: Returns pandas dataframe containing filtered collective events \"\"\" if self . data . empty : return self . data stats = calcCollevStats () colev_duration = stats . calculate ( self . data , self . frame_column , self . collid_column , self . obj_id_column ) filtered_df = self . _filter_collev ( self . data , colev_duration , self . collid_column , coll_duration , coll_total_size , ) return filtered_df","title":"filter()"},{"location":"api/#arcos4py.tools.stats","text":"Module containing tools to calculate statistics of collective events. Examples: >>> from arcos4py.tools import calcCollevStats >>> test = calcCollevStats () >>> out = test . calculate ( data = data , frame_column = \"frame\" , collid_column = \"collid\" )","title":"stats"},{"location":"api/#arcos4py.tools.stats.calcCollevStats","text":"Class to calculate statistics of collective events. Source code in arcos4py/tools/stats.py class calcCollevStats : \"\"\"Class to calculate statistics of collective events.\"\"\" def __init__ ( self ) -> None : \"\"\"Class to calculate statistics of collective events.\"\"\" pass def _calculate_duration_size_group ( self , data : np . ndarray ) -> np . ndarray : \"\"\"Calculates duration and size for the collective event in the dataframe. Arguments: data (np.ndarray): Containing a single collective event. Returns: np.ndarray: Array containing collid, duration, tot_size, min_size, max_size, \"nd_frame, first_frame_centroid and last_frame_centroid of the current collective event. \"\"\" coll_dur = max ( data [:, 0 ]) - min ( data [:, 0 ]) + 1 coll_total_size = np . unique ( data [:, 1 ]) . size ( unique , counts ) = np . unique ( data [:, 0 ], return_counts = True ) frequencies = np . asarray (( unique , counts )) . T coll_min_size = np . min ( frequencies [:, 1 ]) coll_max_size = np . max ( frequencies [:, 1 ]) coll_start_frame = np . min ( data [:, 0 ]) coll_end_frame = np . max ( data [:, 0 ]) if data . shape [ 1 ] > 3 : coll_start_coord = np . mean ( data [( data [:, 0 ] == coll_start_frame )][:, 3 :], axis = 0 ) coll_end_coord = np . mean ( data [( data [:, 0 ] == coll_end_frame )][:, 3 :], axis = 0 ) else : coll_start_coord = np . nan coll_end_coord = np . nan d = np . array ( [ data [ 0 , 2 ], coll_dur , coll_total_size , coll_min_size , coll_max_size , coll_start_frame , coll_end_frame , coll_start_coord , coll_end_coord , ], dtype = object , ) return d def _get_collev_duration ( self , data : pd . DataFrame , frame_column : str , collev_id : str , obj_id_column : str , posCol : Union [ list , None ], ) -> pd . DataFrame : \"\"\"Applies self._calculate_duration_size_group() to every group\\ i.e. every collective event. Arguments: data (DataFrame): Containing unfiltered collective events. collev_id (str): Indicating the contained collective id column. frame_column (str): Indicating the contained frame column. obj_id_column (str): Indicating object id. posCol (list | None): Contains names of position columns. If None coordinates of start and end frame are not calcualted Returns: DataFrame: DataFrame containing \"collid\", \"duration\", \"total_size\", \"min_size\",\"max_size\", \"start_frame\", \"end_frame\", \"first_frame_centroid\" and \"last_frame_centroid\" of all collective events. \"\"\" cols = [ 'collid' , \"duration\" , \"total_size\" , \"min_size\" , \"max_size\" , \"start_frame\" , \"end_frame\" , \"first_frame_centroid\" , \"last_frame_centroid\" , ] subset = [ frame_column , obj_id_column , collev_id ] if posCol : subset . extend ( posCol ) data_np = data [ subset ] . to_numpy ( dtype = np . float64 ) data_np = data_np [ ~ np . isnan ( data_np ) . any ( axis = 1 )] data_np_sorted = data_np [ data_np [:, 2 ] . argsort ()] grouped_array = np . split ( data_np_sorted , np . unique ( data_np_sorted [:, 2 ], axis = 0 , return_index = True )[ 1 ][ 1 :]) # map dbscan to grouped_array out = map ( self . _calculate_duration_size_group , grouped_array ) out_list = [ i for i in out ] df = pd . DataFrame ( out_list , columns = cols ) return df def calculate ( self , data : pd . DataFrame , frame_column : str , collid_column : str , obj_id_column : str , posCol : Union [ list , None ] = None , ) -> pd . DataFrame : \"\"\"Calculate statistics of collective events. Arguments: data (DataFrame): Containing collective events. frame_column (str): Indicating the frame column in data. collid_column (str): Indicating the collective event id column in data. obj_id_column (str): Indicating object id. posCol (list | None): Contains names of position columns. If None coordinates of start and end frame are not calcualted Returns: DataFrame: DataFrame containing \"collid\", \"duration\", \"total_size\", \"min_size\",\"max_size\", \"start_frame\", \"end_frame\", \"first_frame_centroid\" and \"last_frame_centroid\" of all collective events. \"\"\" if data . empty : return data colev_stats = self . _get_collev_duration ( data , frame_column , collid_column , obj_id_column , posCol ) return colev_stats","title":"calcCollevStats"},{"location":"api/#arcos4py.tools.stats.calcCollevStats.__init__","text":"Class to calculate statistics of collective events. Source code in arcos4py/tools/stats.py def __init__ ( self ) -> None : \"\"\"Class to calculate statistics of collective events.\"\"\" pass","title":"__init__()"},{"location":"api/#arcos4py.tools.stats.calcCollevStats.calculate","text":"Calculate statistics of collective events. Parameters: Name Type Description Default data DataFrame Containing collective events. required frame_column str Indicating the frame column in data. required collid_column str Indicating the collective event id column in data. required obj_id_column str Indicating object id. required posCol list | None Contains names of position columns. If None coordinates of start and end frame are not calcualted None Returns: Type Description DataFrame DataFrame containing \"collid\", \"duration\", \"total_size\", \"min_size\",\"max_size\", \"start_frame\", \"end_frame\", \"first_frame_centroid\" and \"last_frame_centroid\" of all collective events. Source code in arcos4py/tools/stats.py def calculate ( self , data : pd . DataFrame , frame_column : str , collid_column : str , obj_id_column : str , posCol : Union [ list , None ] = None , ) -> pd . DataFrame : \"\"\"Calculate statistics of collective events. Arguments: data (DataFrame): Containing collective events. frame_column (str): Indicating the frame column in data. collid_column (str): Indicating the collective event id column in data. obj_id_column (str): Indicating object id. posCol (list | None): Contains names of position columns. If None coordinates of start and end frame are not calcualted Returns: DataFrame: DataFrame containing \"collid\", \"duration\", \"total_size\", \"min_size\",\"max_size\", \"start_frame\", \"end_frame\", \"first_frame_centroid\" and \"last_frame_centroid\" of all collective events. \"\"\" if data . empty : return data colev_stats = self . _get_collev_duration ( data , frame_column , collid_column , obj_id_column , posCol ) return colev_stats","title":"calculate()"},{"location":"changelog/","text":"Changelog \u00b6 [0.1.2] - 2022-05-03 \u00b6 Added \u00b6 NoodlePlot for collective events Changed \u00b6 binarize_detrend: converted pandas operations to numpy for performance improvements detect_events: converted pandas operations to numpy for performance imporovements stats: converted pandas operations to numpy for performance improvements various small changes updated docstrings to match changes Fixed \u00b6 numpy warning caused by stats module [0.1.1] - 2022-04-04 \u00b6 Added \u00b6 More plotting functionallity to the plotting module. Measurment density plot Tracklength histogram Position/T plot Collective event statistcs plot Changed \u00b6 Interpolation class in tools now uses pandas.interpolate to interpolate missing values. Interpolation now interpolates all values in all columns of the dataframe. Improved usage section in the documentation. Fixed \u00b6 Bug in trackCollev class that would lead to an error message in some cases. Spelling in docstrings. [0.1.0] - 2022-03-26 \u00b6 Added \u00b6 First release on PyPI.","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#012---2022-05-03","text":"","title":"[0.1.2] - 2022-05-03"},{"location":"changelog/#added","text":"NoodlePlot for collective events","title":"Added"},{"location":"changelog/#changed","text":"binarize_detrend: converted pandas operations to numpy for performance improvements detect_events: converted pandas operations to numpy for performance imporovements stats: converted pandas operations to numpy for performance improvements various small changes updated docstrings to match changes","title":"Changed"},{"location":"changelog/#fixed","text":"numpy warning caused by stats module","title":"Fixed"},{"location":"changelog/#011---2022-04-04","text":"","title":"[0.1.1] - 2022-04-04"},{"location":"changelog/#added_1","text":"More plotting functionallity to the plotting module. Measurment density plot Tracklength histogram Position/T plot Collective event statistcs plot","title":"Added"},{"location":"changelog/#changed_1","text":"Interpolation class in tools now uses pandas.interpolate to interpolate missing values. Interpolation now interpolates all values in all columns of the dataframe. Improved usage section in the documentation.","title":"Changed"},{"location":"changelog/#fixed_1","text":"Bug in trackCollev class that would lead to an error message in some cases. Spelling in docstrings.","title":"Fixed"},{"location":"changelog/#010---2022-03-26","text":"","title":"[0.1.0] - 2022-03-26"},{"location":"changelog/#added_2","text":"First release on PyPI.","title":"Added"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/bgraedel/arcos4py/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation \u00b6 arcos4py could always use more documentation, whether as part of the official arcos4py docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/bgraedel/arcos4py/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible to make it easier to implement. Remember that this is a volunteer-driven project and that contributions are welcome :) Get Started! \u00b6 Ready to contribute? Here's how to set up arcos4py for local development. Fork the arcos4py repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/arcos4py.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ poetry run tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.7, 3.8, and 3.9. Check https://github.com/bgraedel/arcos4py/actions and make sure that the tests pass for all supported Python versions. Tips \u00b6 $ poetry run pytest tests/test_arcos4py.py To run a subset of tests. Deploying \u00b6 A reminder for the maintainers on how to deploy: Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: $ poetry run bump2version patch # possible: major / minor / patch $ git push $ git push --tags GitHub Actions will then deploy to PyPI if tests pass.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/bgraedel/arcos4py/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"arcos4py could always use more documentation, whether as part of the official arcos4py docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/bgraedel/arcos4py/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible to make it easier to implement. Remember that this is a volunteer-driven project and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up arcos4py for local development. Fork the arcos4py repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/arcos4py.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ poetry run tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.7, 3.8, and 3.9. Check https://github.com/bgraedel/arcos4py/actions and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"contributing/#tips","text":"$ poetry run pytest tests/test_arcos4py.py To run a subset of tests.","title":"Tips"},{"location":"contributing/#deploying","text":"A reminder for the maintainers on how to deploy: Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: $ poetry run bump2version patch # possible: major / minor / patch $ git push $ git push --tags GitHub Actions will then deploy to PyPI if tests pass.","title":"Deploying"},{"location":"installation/","text":"Installation \u00b6 Stable release \u00b6 To install arcos4py, run this command in your terminal: $ pip install arcos4py This is the preferred method to install arcos4py, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From source \u00b6 The source for arcos4py can be downloaded from the Github repo . You can either clone the public repository: $ git clone https://github.com/bgraedel/arcos4py Or download the tarball : $ curl -OJL https://github.com/bgraedel/arcos4py/tarball/master Once you have a copy of the source, you can install it with: $ pip install .","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install arcos4py, run this command in your terminal: $ pip install arcos4py This is the preferred method to install arcos4py, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-source","text":"The source for arcos4py can be downloaded from the Github repo . You can either clone the public repository: $ git clone https://github.com/bgraedel/arcos4py Or download the tarball : $ curl -OJL https://github.com/bgraedel/arcos4py/tarball/master Once you have a copy of the source, you can install it with: $ pip install .","title":"From source"},{"location":"usage/","text":"Usage \u00b6 To use arcos4py in a project import arcos4py Basic usage example of the main module \u00b6 To use the main class, generate a new class instance of ARCOS from arcos4py import ARCOS ts = ARCOS(data,[\"x, y\"], 't', 'id', 'm', 'clTrackID') Data has to be a time-series provided as a pandas DataFrame in the long format, containing at least a measurement column, a frame/index column, and an id column. t x y m id Position 0 1 0.228724716134052 -0.158939933368972 0 1 0 1 1 0.880322831777765 -0.117711550077457 0 2 0 2 1 1.93057074895645 0.0786037381335957 0 3 0 3 1 2.95877070488632 0.189801493820322 0 4 0 4 1 3.90293266588805 -0.0413798066471996 0 5 0 .. . ............... ................. . .. . Prepare the input data. \u00b6 interpolate Measurments \u00b6 If the measurement column contains missing values, run interpolate_measurements() first. ts.interpolate_measurements() Clip measurement to provided quantile range \u00b6 Clipping can be performed to remove extreme outliers from the dataset, but it is not necessary. ts.clip_meas(clip_low: = 0.001, clip_high=0.999) Rescale and Binarize the measurement \u00b6 Rescaling and detrending are optional for the algorithm to work but recommended. There are three options available: ['none', 'lm', 'runmed']. Rumned is the default. However, ARCOS requires binarized data to detect and track collective event clusters. Binarization is done by setting a threshold (binThr) and defining measurements below this threshold as 0 and above as 1. ts.bin_measurements(smoothK: int = 1, biasK = 1, peakThr = 1,binThr = 1, polyDeg = 1, biasMet = \"runmed\",) Detect collective events \u00b6 events_df = ts.trackCollev(eps = 1, minClsz = 1, nPrev = 1) print(events_df) t id x y clTrackID m Position 0 2 41 4.15698907764003 3.91461390425413 1 1 0 1 3 32 3.89042167730585 2.98886585399189 1 1 0 2 3 40 3.08624924975602 4.193936843095 1 1 0 3 3 41 3.99750905085216 3.9553900675078 1 1 0 4 3 42 5.06006349489829 4.0631364410516 1 1 0 .. . .. ... .. . . . TrackCollev returns a pandas DataFrame object containing a column with the collecive event id. Perform calculations without main class \u00b6 All functions from the ARCOS class are also accessible individually through the tools module, such as: from arcos4py.tools import trackCollev Additional modules \u00b6 In addition to the ARCOS algorithm and its helper classes, plots are generated with the plotting module, collective event statistics using the stats module. Please see the Modules Page for further details.","title":"Usage"},{"location":"usage/#usage","text":"To use arcos4py in a project import arcos4py","title":"Usage"},{"location":"usage/#basic-usage-example-of-the-main-module","text":"To use the main class, generate a new class instance of ARCOS from arcos4py import ARCOS ts = ARCOS(data,[\"x, y\"], 't', 'id', 'm', 'clTrackID') Data has to be a time-series provided as a pandas DataFrame in the long format, containing at least a measurement column, a frame/index column, and an id column. t x y m id Position 0 1 0.228724716134052 -0.158939933368972 0 1 0 1 1 0.880322831777765 -0.117711550077457 0 2 0 2 1 1.93057074895645 0.0786037381335957 0 3 0 3 1 2.95877070488632 0.189801493820322 0 4 0 4 1 3.90293266588805 -0.0413798066471996 0 5 0 .. . ............... ................. . .. .","title":"Basic usage example of the main module"},{"location":"usage/#prepare-the-input-data","text":"","title":"Prepare the input data."},{"location":"usage/#interpolate-measurments","text":"If the measurement column contains missing values, run interpolate_measurements() first. ts.interpolate_measurements()","title":"interpolate Measurments"},{"location":"usage/#clip-measurement-to-provided-quantile-range","text":"Clipping can be performed to remove extreme outliers from the dataset, but it is not necessary. ts.clip_meas(clip_low: = 0.001, clip_high=0.999)","title":"Clip measurement to provided quantile range"},{"location":"usage/#rescale-and-binarize-the-measurement","text":"Rescaling and detrending are optional for the algorithm to work but recommended. There are three options available: ['none', 'lm', 'runmed']. Rumned is the default. However, ARCOS requires binarized data to detect and track collective event clusters. Binarization is done by setting a threshold (binThr) and defining measurements below this threshold as 0 and above as 1. ts.bin_measurements(smoothK: int = 1, biasK = 1, peakThr = 1,binThr = 1, polyDeg = 1, biasMet = \"runmed\",)","title":"Rescale and Binarize the measurement"},{"location":"usage/#detect-collective-events","text":"events_df = ts.trackCollev(eps = 1, minClsz = 1, nPrev = 1) print(events_df) t id x y clTrackID m Position 0 2 41 4.15698907764003 3.91461390425413 1 1 0 1 3 32 3.89042167730585 2.98886585399189 1 1 0 2 3 40 3.08624924975602 4.193936843095 1 1 0 3 3 41 3.99750905085216 3.9553900675078 1 1 0 4 3 42 5.06006349489829 4.0631364410516 1 1 0 .. . .. ... .. . . . TrackCollev returns a pandas DataFrame object containing a column with the collecive event id.","title":"Detect collective events"},{"location":"usage/#perform-calculations-without-main-class","text":"All functions from the ARCOS class are also accessible individually through the tools module, such as: from arcos4py.tools import trackCollev","title":"Perform calculations without main class"},{"location":"usage/#additional-modules","text":"In addition to the ARCOS algorithm and its helper classes, plots are generated with the plotting module, collective event statistics using the stats module. Please see the Modules Page for further details.","title":"Additional modules"}]}