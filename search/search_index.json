{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"arcos4py \u00b6 Arcos4py is a python package to detect collective Spatio-temporal phenomena. Documentation: https://bgraedel.github.io/arcos4py GitHub: https://github.com/bgraedel/arcos4py PyPI: https://pypi.org/project/arcos4py/ Free software: MIT Features \u00b6 Automated Recognition of Collective Signalling for python (arcos4py) aims to identify collective spatial events in time-series data. The software identifies collective protein activation in 2- and 3D cell cultures and can track events over time. Such collective waves have been recently identified in various biological systems and have been demonstrated to play a crucial role in the maintenance of epithelial homeostasis (Gagliardi et al., 2020, Takeuchi et al., 2020, Aikin et al., 2020), in the acinar morphogenesis (Ender et al., 2020), osteoblast regeneration (De Simone et al., 2021), and the coordination of collective cell migration (Aoki et al., 2017, Hino et al., 2020). Arcos4py is the python equivalent of the R package ARCOS (https://github.com/dmattek/ARCOS). Despite its focus on cell signaling, the framework can also be applied to other spatiotemporally correlated phenomena. Todo's \u00b6 Add additional tests for binarization and de-biasing modules. Add local indicators of spatial autocorrelation (LISA) as a binarization method option. Data Format \u00b6 The time series should be arranged in a long table format where each row defines the object's location, time, and optionally the measurement value. ARCOS defines an ARCOS object on which several class methods can be used to prepare the data and calculate collective events. Optionally the objects used in the ARCOS class can be used individually by importing them from arcos.tools Installation \u00b6 Arcos4py can be installed from PyPI with: pip install arcos4py Napari Plugin \u00b6 Arcos4py is also available as a Napari Plugin arcos-gui . arcos-gui can simplify parameter finding and visualization. Credits \u00b6 Maciej Dobrzynski created the original ARCOS algorithm. This package was created with Cookiecutter and the waynerv/cookiecutter-pypackage project template.","title":"Home"},{"location":"#arcos4py","text":"Arcos4py is a python package to detect collective Spatio-temporal phenomena. Documentation: https://bgraedel.github.io/arcos4py GitHub: https://github.com/bgraedel/arcos4py PyPI: https://pypi.org/project/arcos4py/ Free software: MIT","title":"arcos4py"},{"location":"#features","text":"Automated Recognition of Collective Signalling for python (arcos4py) aims to identify collective spatial events in time-series data. The software identifies collective protein activation in 2- and 3D cell cultures and can track events over time. Such collective waves have been recently identified in various biological systems and have been demonstrated to play a crucial role in the maintenance of epithelial homeostasis (Gagliardi et al., 2020, Takeuchi et al., 2020, Aikin et al., 2020), in the acinar morphogenesis (Ender et al., 2020), osteoblast regeneration (De Simone et al., 2021), and the coordination of collective cell migration (Aoki et al., 2017, Hino et al., 2020). Arcos4py is the python equivalent of the R package ARCOS (https://github.com/dmattek/ARCOS). Despite its focus on cell signaling, the framework can also be applied to other spatiotemporally correlated phenomena.","title":"Features"},{"location":"#todos","text":"Add additional tests for binarization and de-biasing modules. Add local indicators of spatial autocorrelation (LISA) as a binarization method option.","title":"Todo's"},{"location":"#data-format","text":"The time series should be arranged in a long table format where each row defines the object's location, time, and optionally the measurement value. ARCOS defines an ARCOS object on which several class methods can be used to prepare the data and calculate collective events. Optionally the objects used in the ARCOS class can be used individually by importing them from arcos.tools","title":"Data Format"},{"location":"#installation","text":"Arcos4py can be installed from PyPI with: pip install arcos4py","title":"Installation"},{"location":"#napari-plugin","text":"Arcos4py is also available as a Napari Plugin arcos-gui . arcos-gui can simplify parameter finding and visualization.","title":"Napari Plugin"},{"location":"#credits","text":"Maciej Dobrzynski created the original ARCOS algorithm. This package was created with Cookiecutter and the waynerv/cookiecutter-pypackage project template.","title":"Credits"},{"location":"api/","text":"Arcos4py top level module. This package is a python implementation of the Arcos algorithm for the detection and tracking of collective events intime-series data. ARCOS \u00b6 Detects and tracks collective events in a tracked time-series dataset. Requires binarized measurement column, that can be generated with the bin_measurements method. Tracking makes use of the dbscan algorithm, which is applied to every frame and subsequently connects collective events between frames located within eps distance of each other. Attributes: Name Type Description data DataFrame Data of tracked time-series in \"long format\". Can be used to acess modified dataframe at any point. posCols list List containing position column names strings inside data e.g. At least one dimension is required. frame_column str Indicating the frame column in input_data. id_column str Indicating the track id/id column in input_data. measurement_column str Indicating the measurement column in input_data. clid_column str Indicating the column name containing the collective event ids. bin_col str | None Name of the binary column. This is generated based on the name of the measurement_column after binarization. Optionally can be set in order to provide a already binarized column to skip ARCOS binarization. Source code in arcos4py/_arcos4py.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 class ARCOS : \"\"\"Detects and tracks collective events in a tracked time-series dataset. Requires binarized measurement column, that can be generated with the bin_measurements method. Tracking makes use of the dbscan algorithm, which is applied to every frame and subsequently connects collective events between frames located within eps distance of each other. Attributes: data (DataFrame): Data of tracked time-series in \"long format\". Can be used to acess modified dataframe at any point. posCols (list): List containing position column names strings inside data e.g. At least one dimension is required. frame_column (str): Indicating the frame column in input_data. id_column (str): Indicating the track id/id column in input_data. measurement_column (str): Indicating the measurement column in input_data. clid_column (str): Indicating the column name containing the collective event ids. bin_col (str | None): Name of the binary column. This is generated based on the name of the measurement_column after binarization. Optionally can be set in order to provide a already binarized column to skip ARCOS binarization. \"\"\" def __init__ ( self , data : pd . DataFrame , posCols : list = [ \"x\" ], frame_column : str = 'time' , id_column : str | None = 'id' , measurement_column : str = 'meas' , clid_column : str = 'clTrackID' , n_jobs : int = 1 , ) -> None : \"\"\"Constructs class with provided arguments. Arguments: data (DataFrame): Input Data of tracked time-series in \"long format\" containing position columns, a measurement and an object ID column. posCols (list): List ontaining position column names strings inside data e.g. At least one dimension is required. frame_column (str): Indicating the frame column in input_data. id_column (str): Indicating the track id/object id column in input_data. If None, the data is assumed to not have a tracking column. Binarization can only be performed without detrending. measurement_column (str): Indicating the measurement column in input_data. clid_column (str): Indicating the column name containing the collective event ids. n_jobs (str): Number of workers to spawn, -1 uses all available cpus. \"\"\" self . data = data self . posCols = posCols self . frame_column = frame_column self . id_column = id_column self . measurement_column = measurement_column self . clid_column = clid_column self . n_jobs = n_jobs self . bin_col : Union [ str , None ] = None # to check if no measurement was provided assign None if self . id_column is None : self . data = self . data . sort_values ( by = [ self . frame_column ]) else : self . data = self . data . sort_values ( by = [ self . frame_column , self . id_column ]) self . _check_col () if self . measurement_column is not None : self . resc_col = f \" { self . measurement_column } .resc\" self . bin_col = f \" { self . measurement_column } .bin\" def __repr__ ( self ) -> pd . DataFrame : \"\"\"Set __repr__ to return self.data.\"\"\" return repr ( self . data ) def _check_col ( self ): \"\"\"Checks that self.cols contains all required columns.\"\"\" columns = self . data . columns input_columns = [ self . frame_column , self . id_column , self . id_column , self . measurement_column ] input_columns = [ col for col in input_columns if col is not None ] if not all ( item in columns for item in input_columns ): raise ValueError ( f \"Columns { input_columns } do not match with column in dataframe.\" ) def interpolate_measurements ( self ) -> pd . DataFrame : \"\"\"Interpolates NaN's in place in measurement column. Returns: Dataframe with interpolated measurement column. \"\"\" meas_interp = interpolation ( self . data ) . interpolate () self . data = meas_interp return self . data def clip_meas ( self , clip_low : float = 0.001 , clip_high : float = 0.999 ) -> pd . DataFrame : \"\"\"Clip measurement column to upper and lower quantiles defined in clip_low and clip_high. Arguments: clip_low (float): Lower clipping boundary (quantile). clip_high (float): Upper clipping boundary (quantile). Returns: Dataframe with in place clipped measurement column. \"\"\" meas_column = self . data [ self . measurement_column ] . to_numpy () meas_clipped = clipMeas ( meas_column ) . clip ( clip_low , clip_high ) self . data [ self . measurement_column ] = meas_clipped return self . data def bin_measurements ( self , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , binThr : float = 0.1 , polyDeg : int = 1 , biasMet : str = \"runmed\" , ) -> pd . DataFrame : r \"\"\"Smooth, de-trend, and binarise the input data. First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold Arguments: smoothK (int): Size of the short-term median smoothing filter. biasK (int): Size of the long-term de-trending median filter peakThr (float): Threshold for rescaling of the de-trended signal. binThr (float): Threshold for binary classification. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): De-trending method, one of ['runmed', 'lm', 'none']. If no id_column is provided, only 'none' is allowed. Returns: DataFrame with detrended/smoothed and binarized measurement column. \"\"\" self . data = binData ( smoothK , biasK , peakThr , binThr , polyDeg , biasMet , n_jobs = self . n_jobs , ) . run ( self . data , colMeas = self . measurement_column , colGroup = self . id_column , colFrame = self . frame_column ) return self . data def trackCollev ( self , eps : float = 1 , epsPrev : Union [ int , None ] = None , minClsz : int = 1 , nPrev : int = 1 , clusteringMethod : str = \"dbscan\" , linkingMethod : str = \"nearest\" , minSamples : int | None = None , ) -> pd . DataFrame : \"\"\"Requires binarized measurement column. Makes use of the dbscan algorithm, applies this to every timeframe and subsequently connects collective events between frames located within eps distance of each other. Arguments: eps (float): The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. epsPrev (float | None): Frame to frame distance, value is used to connect collective events across multiple frames.If \"None\", same value as eps is used. minClsz (str): The minimum size for a cluster to be identified as a collective event nPrev (int): Number of previous frames the tracking algorithm looks back to connect collective events clusteringMethod (str): Clustering method, one of ['dbscan', 'hdbscan']. minSamples (int | None): The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. linkingMethod (str): Linking method, one of ['nearest', 'transportation']. Returns: DataFrame with detected collective events across time. \"\"\" data_events = track_events_dataframe ( X = self . data , coordinates_column = self . posCols , frame_column = self . frame_column , id_column = self . id_column , bin_meas_column = self . bin_col , eps = eps , epsPrev = epsPrev , minClSz = minClsz , nPrev = nPrev , collid_column = self . clid_column , linkingMethod = linkingMethod , clusteringMethod = clusteringMethod , minSamples = minSamples , nJobs = self . n_jobs , ) return data_events __init__ ( data , posCols = [ 'x' ], frame_column = 'time' , id_column = 'id' , measurement_column = 'meas' , clid_column = 'clTrackID' , n_jobs = 1 ) \u00b6 Constructs class with provided arguments. Parameters: Name Type Description Default data DataFrame Input Data of tracked time-series in \"long format\" containing position columns, a measurement and an object ID column. required posCols list List ontaining position column names strings inside data e.g. At least one dimension is required. ['x'] frame_column str Indicating the frame column in input_data. 'time' id_column str Indicating the track id/object id column in input_data. If None, the data is assumed to not have a tracking column. Binarization can only be performed without detrending. 'id' measurement_column str Indicating the measurement column in input_data. 'meas' clid_column str Indicating the column name containing the collective event ids. 'clTrackID' n_jobs str Number of workers to spawn, -1 uses all available cpus. 1 Source code in arcos4py/_arcos4py.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , data : pd . DataFrame , posCols : list = [ \"x\" ], frame_column : str = 'time' , id_column : str | None = 'id' , measurement_column : str = 'meas' , clid_column : str = 'clTrackID' , n_jobs : int = 1 , ) -> None : \"\"\"Constructs class with provided arguments. Arguments: data (DataFrame): Input Data of tracked time-series in \"long format\" containing position columns, a measurement and an object ID column. posCols (list): List ontaining position column names strings inside data e.g. At least one dimension is required. frame_column (str): Indicating the frame column in input_data. id_column (str): Indicating the track id/object id column in input_data. If None, the data is assumed to not have a tracking column. Binarization can only be performed without detrending. measurement_column (str): Indicating the measurement column in input_data. clid_column (str): Indicating the column name containing the collective event ids. n_jobs (str): Number of workers to spawn, -1 uses all available cpus. \"\"\" self . data = data self . posCols = posCols self . frame_column = frame_column self . id_column = id_column self . measurement_column = measurement_column self . clid_column = clid_column self . n_jobs = n_jobs self . bin_col : Union [ str , None ] = None # to check if no measurement was provided assign None if self . id_column is None : self . data = self . data . sort_values ( by = [ self . frame_column ]) else : self . data = self . data . sort_values ( by = [ self . frame_column , self . id_column ]) self . _check_col () if self . measurement_column is not None : self . resc_col = f \" { self . measurement_column } .resc\" self . bin_col = f \" { self . measurement_column } .bin\" __repr__ () \u00b6 Set repr to return self.data. Source code in arcos4py/_arcos4py.py 97 98 99 def __repr__ ( self ) -> pd . DataFrame : \"\"\"Set __repr__ to return self.data.\"\"\" return repr ( self . data ) bin_measurements ( smoothK = 3 , biasK = 51 , peakThr = 0.2 , binThr = 0.1 , polyDeg = 1 , biasMet = 'runmed' ) \u00b6 Smooth, de-trend, and binarise the input data. First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold Parameters: Name Type Description Default smoothK int Size of the short-term median smoothing filter. 3 biasK int Size of the long-term de-trending median filter 51 peakThr float Threshold for rescaling of the de-trended signal. 0.2 binThr float Threshold for binary classification. 0.1 polyDeg int Sets the degree of the polynomial for lm fitting. 1 biasMet str De-trending method, one of ['runmed', 'lm', 'none']. If no id_column is provided, only 'none' is allowed. 'runmed' Returns: Type Description pd . DataFrame DataFrame with detrended/smoothed and binarized measurement column. Source code in arcos4py/_arcos4py.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def bin_measurements ( self , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , binThr : float = 0.1 , polyDeg : int = 1 , biasMet : str = \"runmed\" , ) -> pd . DataFrame : r \"\"\"Smooth, de-trend, and binarise the input data. First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold Arguments: smoothK (int): Size of the short-term median smoothing filter. biasK (int): Size of the long-term de-trending median filter peakThr (float): Threshold for rescaling of the de-trended signal. binThr (float): Threshold for binary classification. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): De-trending method, one of ['runmed', 'lm', 'none']. If no id_column is provided, only 'none' is allowed. Returns: DataFrame with detrended/smoothed and binarized measurement column. \"\"\" self . data = binData ( smoothK , biasK , peakThr , binThr , polyDeg , biasMet , n_jobs = self . n_jobs , ) . run ( self . data , colMeas = self . measurement_column , colGroup = self . id_column , colFrame = self . frame_column ) return self . data clip_meas ( clip_low = 0.001 , clip_high = 0.999 ) \u00b6 Clip measurement column to upper and lower quantiles defined in clip_low and clip_high. Parameters: Name Type Description Default clip_low float Lower clipping boundary (quantile). 0.001 clip_high float Upper clipping boundary (quantile). 0.999 Returns: Type Description pd . DataFrame Dataframe with in place clipped measurement column. Source code in arcos4py/_arcos4py.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def clip_meas ( self , clip_low : float = 0.001 , clip_high : float = 0.999 ) -> pd . DataFrame : \"\"\"Clip measurement column to upper and lower quantiles defined in clip_low and clip_high. Arguments: clip_low (float): Lower clipping boundary (quantile). clip_high (float): Upper clipping boundary (quantile). Returns: Dataframe with in place clipped measurement column. \"\"\" meas_column = self . data [ self . measurement_column ] . to_numpy () meas_clipped = clipMeas ( meas_column ) . clip ( clip_low , clip_high ) self . data [ self . measurement_column ] = meas_clipped return self . data interpolate_measurements () \u00b6 Interpolates NaN's in place in measurement column. Returns: Type Description pd . DataFrame Dataframe with interpolated measurement column. Source code in arcos4py/_arcos4py.py 109 110 111 112 113 114 115 116 117 def interpolate_measurements ( self ) -> pd . DataFrame : \"\"\"Interpolates NaN's in place in measurement column. Returns: Dataframe with interpolated measurement column. \"\"\" meas_interp = interpolation ( self . data ) . interpolate () self . data = meas_interp return self . data trackCollev ( eps = 1 , epsPrev = None , minClsz = 1 , nPrev = 1 , clusteringMethod = 'dbscan' , linkingMethod = 'nearest' , minSamples = None ) \u00b6 Requires binarized measurement column. Makes use of the dbscan algorithm, applies this to every timeframe and subsequently connects collective events between frames located within eps distance of each other. Parameters: Name Type Description Default eps float The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. 1 epsPrev float | None Frame to frame distance, value is used to connect collective events across multiple frames.If \"None\", same value as eps is used. None minClsz str The minimum size for a cluster to be identified as a collective event 1 nPrev int Number of previous frames the tracking algorithm looks back to connect collective events 1 clusteringMethod str Clustering method, one of ['dbscan', 'hdbscan']. 'dbscan' minSamples int | None The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. None linkingMethod str Linking method, one of ['nearest', 'transportation']. 'nearest' Returns: Type Description pd . DataFrame DataFrame with detected collective events across time. Source code in arcos4py/_arcos4py.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def trackCollev ( self , eps : float = 1 , epsPrev : Union [ int , None ] = None , minClsz : int = 1 , nPrev : int = 1 , clusteringMethod : str = \"dbscan\" , linkingMethod : str = \"nearest\" , minSamples : int | None = None , ) -> pd . DataFrame : \"\"\"Requires binarized measurement column. Makes use of the dbscan algorithm, applies this to every timeframe and subsequently connects collective events between frames located within eps distance of each other. Arguments: eps (float): The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. epsPrev (float | None): Frame to frame distance, value is used to connect collective events across multiple frames.If \"None\", same value as eps is used. minClsz (str): The minimum size for a cluster to be identified as a collective event nPrev (int): Number of previous frames the tracking algorithm looks back to connect collective events clusteringMethod (str): Clustering method, one of ['dbscan', 'hdbscan']. minSamples (int | None): The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. linkingMethod (str): Linking method, one of ['nearest', 'transportation']. Returns: DataFrame with detected collective events across time. \"\"\" data_events = track_events_dataframe ( X = self . data , coordinates_column = self . posCols , frame_column = self . frame_column , id_column = self . id_column , bin_meas_column = self . bin_col , eps = eps , epsPrev = epsPrev , minClSz = minClsz , nPrev = nPrev , collid_column = self . clid_column , linkingMethod = linkingMethod , clusteringMethod = clusteringMethod , minSamples = minSamples , nJobs = self . n_jobs , ) return data_events plotting \u00b6 Tools for plotting collective events. NoodlePlot \u00b6 Create Noodle Plot of cell tracks, colored by collective event id. Attributes: Name Type Description df pd . DataFrame DataFrame containing collective events from arcos. colev str Name of the collective event column in df. trackid str Name of the track column in df. frame str Name of the frame column in df. posx str Name of the X coordinate column in df. posy str Name of the Y coordinate column in df. posz str Name of the Z coordinate column in df, or None if no z column. Source code in arcos4py/plotting/_plotting.py 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 class NoodlePlot : \"\"\"Create Noodle Plot of cell tracks, colored by collective event id. Attributes: df (pd.DataFrame): DataFrame containing collective events from arcos. colev (str): Name of the collective event column in df. trackid (str): Name of the track column in df. frame (str): Name of the frame column in df. posx (str): Name of the X coordinate column in df. posy (str): Name of the Y coordinate column in df. posz (str): Name of the Z coordinate column in df, or None if no z column. \"\"\" def __init__ ( self , df : pd . DataFrame , colev : str , trackid : str , frame : str , posx : str , posy : str , posz : Union [ str , None ] = None , ): \"\"\"Constructs class with given parameters. Arguments: df (pd.DataFrame): DataFrame containing collective events from arcos. colev (str): Name of the collective event column in df. trackid (str): Name of the track column in df. frame (str): Name of the frame column in df. posx (str): Name of the X coordinate column in df. posy (str): Name of the Y coordinate column in df. posz (str | None): Name of the Z coordinate column in df, or None if no z column. \"\"\" self . df = df self . colev = colev self . trackid = trackid self . frame = frame self . posx = posx self . posy = posy self . posz = posz def _prepare_data_noodleplot ( self , df : pd . DataFrame , color_cylce : list [ str ], colev : str , trackid : str , frame : str , posx : str , posy : str , posz : Union [ str , None ] = None , ) -> tuple [ list [ np . ndarray ], np . ndarray ]: \"\"\"From arcos collective event data,\\ generates a list of numpy arrays, one for each event. Arguments: df (pd.DataFrame): DataFrame containing collective events from arcos. color_cylce (list[str]): list of colors used to color trackid's for individual collective events. colev (str): Name of the collective event column in df. trackid (str): Name of the track column in df. frame: (str): Name of the frame column in df. posx (str): Name of the X coordinate column in df. posy (str): Name of the Y coordinate column in df. posz (str): Name of the Z coordinate column in df, or None if no z column. Returns: grouped_array (list[np.ndarray]): List of collective events data colors (np.ndarray): colors for each collective event. \"\"\" # values need to be sorted to group with numpy df = df . sort_values ([ colev , trackid ]) if posz : array = df [[ colev , trackid , frame , posx , posy , posz ]] . to_numpy () else : array = df [[ colev , trackid , frame , posx , posy ]] . to_numpy () # generate goroups for each unique value grouped_array = np . split ( array , np . unique ( array [:, 0 ], axis = 0 , return_index = True )[ 1 ][ 1 :]) # make collids sequential seq_colids = np . concatenate ( [ np . repeat ( i , value . shape [ 0 ]) for i , value in enumerate ( grouped_array )], axis = 0 , ) array_seq_colids = np . column_stack (( array , seq_colids )) # split sequential collids array by trackid and collid grouped_array = np . split ( array_seq_colids , np . unique ( array_seq_colids [:, : 2 ], axis = 0 , return_index = True )[ 1 ][ 1 :], ) # generate colors for each collective event, wrap arround the color cycle colors = np . take ( np . array ( color_cylce ), [ i + 1 for i in np . unique ( seq_colids )], mode = \"wrap\" ) return grouped_array , colors def _create_noodle_plot ( self , grouped_data : list [ np . ndarray ], colors : np . ndarray ): \"\"\"Plots the noodle plot.\"\"\" fig , ax = plt . subplots () ax . set_xlabel ( \"Time Point\" ) ax . set_ylabel ( \"Position\" ) for dat in grouped_data : ax . plot ( dat [:, 2 ], dat [:, self . projection_index ], c = colors [ int ( dat [ 0 , - 1 ])], ) return fig , ax def plot ( self , projection_axis : str , color_cylce : list [ str ] = TAB20 ): \"\"\"Create Noodle Plot of cell tracks, colored by collective event id. Arguments: projection_axis (str): Specify with witch coordinate the noodle plot should be drawn. Has to be one of the posx, posy or posz arguments passed in during the class instantiation process. color_cylce (list[str]): List of hex color values or string names (i.e. ['red', 'yellow']) used to color collecitve events. Cycles through list. Returns: fig (matplotlib.figure.Figure): Matplotlib figure object for the noodle plot. axes (matplotlib.axes.Axes): Matplotlib axes for the nooble plot. \"\"\" if projection_axis not in [ self . posx , self . posy , self . posz ]: raise ValueError ( f \"projection_axis has to be one of { [ self . posx , self . posy , self . posz ] } \" ) if projection_axis == self . posx : self . projection_index = 3 elif projection_axis == self . posy : self . projection_index = 4 elif projection_axis == self . posz : self . projection_index = 5 grpd_data , colors = self . _prepare_data_noodleplot ( self . df , color_cylce , self . colev , self . trackid , self . frame , self . posx , self . posy , self . posz ) fig , axes = self . _create_noodle_plot ( grpd_data , colors ) return fig , axes __init__ ( df , colev , trackid , frame , posx , posy , posz = None ) \u00b6 Constructs class with given parameters. Parameters: Name Type Description Default df pd . DataFrame DataFrame containing collective events from arcos. required colev str Name of the collective event column in df. required trackid str Name of the track column in df. required frame str Name of the frame column in df. required posx str Name of the X coordinate column in df. required posy str Name of the Y coordinate column in df. required posz str | None Name of the Z coordinate column in df, or None if no z column. None Source code in arcos4py/plotting/_plotting.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def __init__ ( self , df : pd . DataFrame , colev : str , trackid : str , frame : str , posx : str , posy : str , posz : Union [ str , None ] = None , ): \"\"\"Constructs class with given parameters. Arguments: df (pd.DataFrame): DataFrame containing collective events from arcos. colev (str): Name of the collective event column in df. trackid (str): Name of the track column in df. frame (str): Name of the frame column in df. posx (str): Name of the X coordinate column in df. posy (str): Name of the Y coordinate column in df. posz (str | None): Name of the Z coordinate column in df, or None if no z column. \"\"\" self . df = df self . colev = colev self . trackid = trackid self . frame = frame self . posx = posx self . posy = posy self . posz = posz plot ( projection_axis , color_cylce = TAB20 ) \u00b6 Create Noodle Plot of cell tracks, colored by collective event id. Parameters: Name Type Description Default projection_axis str Specify with witch coordinate the noodle plot should be drawn. Has to be one of the posx, posy or posz arguments passed in during the class instantiation process. required color_cylce list [ str ] List of hex color values or string names (i.e. ['red', 'yellow']) used to color collecitve events. Cycles through list. TAB20 Returns: Name Type Description fig matplotlib . figure . Figure Matplotlib figure object for the noodle plot. axes matplotlib . axes . Axes Matplotlib axes for the nooble plot. Source code in arcos4py/plotting/_plotting.py 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 def plot ( self , projection_axis : str , color_cylce : list [ str ] = TAB20 ): \"\"\"Create Noodle Plot of cell tracks, colored by collective event id. Arguments: projection_axis (str): Specify with witch coordinate the noodle plot should be drawn. Has to be one of the posx, posy or posz arguments passed in during the class instantiation process. color_cylce (list[str]): List of hex color values or string names (i.e. ['red', 'yellow']) used to color collecitve events. Cycles through list. Returns: fig (matplotlib.figure.Figure): Matplotlib figure object for the noodle plot. axes (matplotlib.axes.Axes): Matplotlib axes for the nooble plot. \"\"\" if projection_axis not in [ self . posx , self . posy , self . posz ]: raise ValueError ( f \"projection_axis has to be one of { [ self . posx , self . posy , self . posz ] } \" ) if projection_axis == self . posx : self . projection_index = 3 elif projection_axis == self . posy : self . projection_index = 4 elif projection_axis == self . posz : self . projection_index = 5 grpd_data , colors = self . _prepare_data_noodleplot ( self . df , color_cylce , self . colev , self . trackid , self . frame , self . posx , self . posy , self . posz ) fig , axes = self . _create_noodle_plot ( grpd_data , colors ) return fig , axes dataPlots \u00b6 Plot different metrics of input data. Attributes: Name Type Description data Dataframe containing ARCOS data. frame str name of frame column in data. measurement str name of measurement column in data. id str name of track id column. Source code in arcos4py/plotting/_plotting.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class dataPlots : \"\"\"Plot different metrics of input data. Attributes: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. id (str): name of track id column. \"\"\" def __init__ ( self , data : pd . DataFrame , frame : str , measurement : str , id : str ): \"\"\"Plot different metrics such as histogram, position-t and density. Arguments: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. id (str): name of track id column. \"\"\" self . data = data self . id = id self . frame = frame self . measurement = measurement def position_t_plot ( self , posCol : set [ str ] = { 'x' }, n : int = 20 ) -> Union [ plt . Axes , plt . Figure ]: \"\"\"Plots X and Y over T to visualize tracklength. Arguments: posCol (set): containing names of position columns in data. n (int): number of samples to plot. Returns: fig (matplotlib.figure.Figure): Matplotlib figure object of density plot. axes (matplotlib.axes.Axes): Matplotlib axes of density plot. \"\"\" sample = pd . Series ( self . data [ self . id ] . unique ()) . sample ( n ) pd_from_r_df = self . data . loc [ self . data [ self . id ] . isin ( sample )] fig , axes = plt . subplots ( 1 , len ( posCol ), figsize = ( 6 , 3 )) for label , df in pd_from_r_df . groupby ( self . id ): for index , value in enumerate ( posCol ): if len ( posCol ) > 1 : df . plot ( x = self . frame , y = value , ax = axes [ index ], legend = None ) else : df . plot ( x = self . frame , y = value , ax = axes , legend = None ) if len ( posCol ) > 1 : for index , value in enumerate ( posCol ): axes [ index ] . set_title ( value ) else : axes . set_title ( value ) return fig , axes def density_plot ( self , * args , ** kwargs ): \"\"\"Density plot of measurement. Uses Seaborn distplot to plot measurement density. Arguments: *args (Any): arguments passed on to seaborn histplot function. **kwargs (Any): keyword arguments passed on to seaborn histplot function. Returns: FacetGrid (seaborn.FacetGrid): Seaborn FacetGrid of density density plot. \"\"\" plot = sns . displot ( self . data [ self . measurement ], kind = \"kde\" , palette = \"pastel\" , label = self . measurement , * args , ** kwargs ) # Plot formatting plt . legend ( prop = { 'size' : 10 }) plt . title ( 'Density Plot of Measurement' ) plt . xlabel ( 'Measurement' ) plt . ylabel ( 'Density' ) return plot def histogram ( self , bins : str = 'auto' , * args , ** kwargs ) -> plt . Axes . subplot : \"\"\"Histogram of tracklenght. Uses seaborn histplot function to plot tracklenght histogram. Arguments: bins (str): number or width of bins in histogram *args (Any): arguments passed on to seaborn histplot function. **kwargs (Any): keyword arguments passed on to seaborn histplot function. Returns: AxesSubplot: Matplotlib AxesSubplot of histogram. \"\"\" # Draw histogram track_length = self . data . groupby ( self . id ) . size () axes = sns . histplot ( track_length , label = \"Track Length\" , bins = bins , * args , ** kwargs ) # Plot formatting plt . title ( 'Track length Histogram' ) axes . set_xlabel ( 'Track Length' ) axes . set_ylabel ( 'Count' ) return axes __init__ ( data , frame , measurement , id ) \u00b6 Plot different metrics such as histogram, position-t and density. Parameters: Name Type Description Default data Dataframe containing ARCOS data. required frame str name of frame column in data. required measurement str name of measurement column in data. required id str name of track id column. required Source code in arcos4py/plotting/_plotting.py 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , data : pd . DataFrame , frame : str , measurement : str , id : str ): \"\"\"Plot different metrics such as histogram, position-t and density. Arguments: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. id (str): name of track id column. \"\"\" self . data = data self . id = id self . frame = frame self . measurement = measurement density_plot ( * args , ** kwargs ) \u00b6 Density plot of measurement. Uses Seaborn distplot to plot measurement density. Parameters: Name Type Description Default *args Any arguments passed on to seaborn histplot function. () **kwargs Any keyword arguments passed on to seaborn histplot function. {} Returns: Name Type Description FacetGrid seaborn . FacetGrid Seaborn FacetGrid of density density plot. Source code in arcos4py/plotting/_plotting.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def density_plot ( self , * args , ** kwargs ): \"\"\"Density plot of measurement. Uses Seaborn distplot to plot measurement density. Arguments: *args (Any): arguments passed on to seaborn histplot function. **kwargs (Any): keyword arguments passed on to seaborn histplot function. Returns: FacetGrid (seaborn.FacetGrid): Seaborn FacetGrid of density density plot. \"\"\" plot = sns . displot ( self . data [ self . measurement ], kind = \"kde\" , palette = \"pastel\" , label = self . measurement , * args , ** kwargs ) # Plot formatting plt . legend ( prop = { 'size' : 10 }) plt . title ( 'Density Plot of Measurement' ) plt . xlabel ( 'Measurement' ) plt . ylabel ( 'Density' ) return plot histogram ( bins = 'auto' , * args , ** kwargs ) \u00b6 Histogram of tracklenght. Uses seaborn histplot function to plot tracklenght histogram. Parameters: Name Type Description Default bins str number or width of bins in histogram 'auto' *args Any arguments passed on to seaborn histplot function. () **kwargs Any keyword arguments passed on to seaborn histplot function. {} Returns: Name Type Description AxesSubplot plt . Axes . subplot Matplotlib AxesSubplot of histogram. Source code in arcos4py/plotting/_plotting.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def histogram ( self , bins : str = 'auto' , * args , ** kwargs ) -> plt . Axes . subplot : \"\"\"Histogram of tracklenght. Uses seaborn histplot function to plot tracklenght histogram. Arguments: bins (str): number or width of bins in histogram *args (Any): arguments passed on to seaborn histplot function. **kwargs (Any): keyword arguments passed on to seaborn histplot function. Returns: AxesSubplot: Matplotlib AxesSubplot of histogram. \"\"\" # Draw histogram track_length = self . data . groupby ( self . id ) . size () axes = sns . histplot ( track_length , label = \"Track Length\" , bins = bins , * args , ** kwargs ) # Plot formatting plt . title ( 'Track length Histogram' ) axes . set_xlabel ( 'Track Length' ) axes . set_ylabel ( 'Count' ) return axes position_t_plot ( posCol = { 'x' }, n = 20 ) \u00b6 Plots X and Y over T to visualize tracklength. Parameters: Name Type Description Default posCol set containing names of position columns in data. {'x'} n int number of samples to plot. 20 Returns: Name Type Description fig matplotlib . figure . Figure Matplotlib figure object of density plot. axes matplotlib . axes . Axes Matplotlib axes of density plot. Source code in arcos4py/plotting/_plotting.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def position_t_plot ( self , posCol : set [ str ] = { 'x' }, n : int = 20 ) -> Union [ plt . Axes , plt . Figure ]: \"\"\"Plots X and Y over T to visualize tracklength. Arguments: posCol (set): containing names of position columns in data. n (int): number of samples to plot. Returns: fig (matplotlib.figure.Figure): Matplotlib figure object of density plot. axes (matplotlib.axes.Axes): Matplotlib axes of density plot. \"\"\" sample = pd . Series ( self . data [ self . id ] . unique ()) . sample ( n ) pd_from_r_df = self . data . loc [ self . data [ self . id ] . isin ( sample )] fig , axes = plt . subplots ( 1 , len ( posCol ), figsize = ( 6 , 3 )) for label , df in pd_from_r_df . groupby ( self . id ): for index , value in enumerate ( posCol ): if len ( posCol ) > 1 : df . plot ( x = self . frame , y = value , ax = axes [ index ], legend = None ) else : df . plot ( x = self . frame , y = value , ax = axes , legend = None ) if len ( posCol ) > 1 : for index , value in enumerate ( posCol ): axes [ index ] . set_title ( value ) else : axes . set_title ( value ) return fig , axes plotOriginalDetrended \u00b6 Plot different detrended vs original data. Attributes: Name Type Description data Dataframe containing ARCOS data. frame str name of frame column in data. measurement str name of measurement column in data. detrended str name of detrended column with detrended data. id str name of track id column. Source code in arcos4py/plotting/_plotting.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 class plotOriginalDetrended : \"\"\"Plot different detrended vs original data. Attributes: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. detrended (str): name of detrended column with detrended data. id (str): name of track id column. \"\"\" def __init__ ( self , data : pd . DataFrame , frame : str , measurement : str , detrended : str , id : str ): \"\"\"Plot detrended vs original data. Arguments: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. detrended (str): name of detrended column with detrended data. id (str): name of track id column. \"\"\" self . data = data self . measurement = measurement self . detrended = detrended self . id = id self . frame = frame def plot_detrended ( self , n_samples : int = 25 , subplots : tuple = ( 5 , 5 ), plotsize : tuple = ( 20 , 10 )): \"\"\"Method to plot detrended vs original data. Arguments: n_samples (int): Number of tracks to plot. subplots (tuple): Number of subplots, should be approx. one per sample. plotsize (tuple): Size of generated plot. Returns: Fig (matplotlib.axes.Axes): Matplotlib axes2d of detrended vs original data. Axes (matplotlib.axes.Axes): Matplotlib figure of detrended vs original data. \"\"\" vals = np . random . choice ( self . data [ self . id ] . unique (), n_samples , replace = False ) self . data = self . data . set_index ( self . id ) . loc [ vals ] . reset_index () grouped = self . data . groupby ( self . id ) ncols = subplots [ 0 ] nrows = subplots [ 1 ] fig , axes2d = plt . subplots ( nrows = nrows , ncols = ncols , figsize = plotsize , sharey = True ) for ( key , ax ) in zip ( grouped . groups . keys (), axes2d . flatten ()): grouped . get_group ( key ) . plot ( x = self . frame , y = [ self . measurement , self . detrended ], ax = ax ) ax . get_legend () . remove () handles , labels = ax . get_legend_handles_labels () fig . legend ( handles , labels , loc = \"lower right\" ) return fig , axes2d __init__ ( data , frame , measurement , detrended , id ) \u00b6 Plot detrended vs original data. Parameters: Name Type Description Default data Dataframe containing ARCOS data. required frame str name of frame column in data. required measurement str name of measurement column in data. required detrended str name of detrended column with detrended data. required id str name of track id column. required Source code in arcos4py/plotting/_plotting.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def __init__ ( self , data : pd . DataFrame , frame : str , measurement : str , detrended : str , id : str ): \"\"\"Plot detrended vs original data. Arguments: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. detrended (str): name of detrended column with detrended data. id (str): name of track id column. \"\"\" self . data = data self . measurement = measurement self . detrended = detrended self . id = id self . frame = frame plot_detrended ( n_samples = 25 , subplots = ( 5 , 5 ), plotsize = ( 20 , 10 )) \u00b6 Method to plot detrended vs original data. Parameters: Name Type Description Default n_samples int Number of tracks to plot. 25 subplots tuple Number of subplots, should be approx. one per sample. (5, 5) plotsize tuple Size of generated plot. (20, 10) Returns: Name Type Description Fig matplotlib . axes . Axes Matplotlib axes2d of detrended vs original data. Axes matplotlib . axes . Axes Matplotlib figure of detrended vs original data. Source code in arcos4py/plotting/_plotting.py 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def plot_detrended ( self , n_samples : int = 25 , subplots : tuple = ( 5 , 5 ), plotsize : tuple = ( 20 , 10 )): \"\"\"Method to plot detrended vs original data. Arguments: n_samples (int): Number of tracks to plot. subplots (tuple): Number of subplots, should be approx. one per sample. plotsize (tuple): Size of generated plot. Returns: Fig (matplotlib.axes.Axes): Matplotlib axes2d of detrended vs original data. Axes (matplotlib.axes.Axes): Matplotlib figure of detrended vs original data. \"\"\" vals = np . random . choice ( self . data [ self . id ] . unique (), n_samples , replace = False ) self . data = self . data . set_index ( self . id ) . loc [ vals ] . reset_index () grouped = self . data . groupby ( self . id ) ncols = subplots [ 0 ] nrows = subplots [ 1 ] fig , axes2d = plt . subplots ( nrows = nrows , ncols = ncols , figsize = plotsize , sharey = True ) for ( key , ax ) in zip ( grouped . groups . keys (), axes2d . flatten ()): grouped . get_group ( key ) . plot ( x = self . frame , y = [ self . measurement , self . detrended ], ax = ax ) ax . get_legend () . remove () handles , labels = ax . get_legend_handles_labels () fig . legend ( handles , labels , loc = \"lower right\" ) return fig , axes2d statsPlots \u00b6 Plot data generated by the stats module. Attributes: Name Type Description data DataFrame containing ARCOS stats data. Source code in arcos4py/plotting/_plotting.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 class statsPlots : \"\"\"Plot data generated by the stats module. Attributes: data (DataFrame): containing ARCOS stats data. \"\"\" def __init__ ( self , data : pd . DataFrame ): \"\"\"Plot detrended vs original data. Arguments: data (DataFrame): containing ARCOS stats data. \"\"\" self . data = data def plot_events_duration ( self , total_size : str , duration : str , point_size : int = 40 , * args , ** kwargs ) -> matplotlib . axes . Axes : \"\"\"Scatterplot of collective event duration. Arguments: total_size (str): name of total size column. duration (str):, name of column with collective event duration. point_size (int): scatterplot point size. *args (Any): Arguments passed on to seaborn scatterplot function. **kwargs (Any): Keyword arguments passed on to seaborn scatterplot function. Returns: Axes (matplotlib.axes.Axes): Matplotlib Axes object of scatterplot \"\"\" plot = sns . scatterplot ( x = self . data [ total_size ], y = self . data [ duration ], s = point_size , * args , ** kwargs ) return plot __init__ ( data ) \u00b6 Plot detrended vs original data. Parameters: Name Type Description Default data DataFrame containing ARCOS stats data. required Source code in arcos4py/plotting/_plotting.py 221 222 223 224 225 226 227 def __init__ ( self , data : pd . DataFrame ): \"\"\"Plot detrended vs original data. Arguments: data (DataFrame): containing ARCOS stats data. \"\"\" self . data = data plot_events_duration ( total_size , duration , point_size = 40 , * args , ** kwargs ) \u00b6 Scatterplot of collective event duration. Parameters: Name Type Description Default total_size str name of total size column. required duration str , name of column with collective event duration. required point_size int scatterplot point size. 40 *args Any Arguments passed on to seaborn scatterplot function. () **kwargs Any Keyword arguments passed on to seaborn scatterplot function. {} Returns: Name Type Description Axes matplotlib . axes . Axes Matplotlib Axes object of scatterplot Source code in arcos4py/plotting/_plotting.py 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def plot_events_duration ( self , total_size : str , duration : str , point_size : int = 40 , * args , ** kwargs ) -> matplotlib . axes . Axes : \"\"\"Scatterplot of collective event duration. Arguments: total_size (str): name of total size column. duration (str):, name of column with collective event duration. point_size (int): scatterplot point size. *args (Any): Arguments passed on to seaborn scatterplot function. **kwargs (Any): Keyword arguments passed on to seaborn scatterplot function. Returns: Axes (matplotlib.axes.Axes): Matplotlib Axes object of scatterplot \"\"\" plot = sns . scatterplot ( x = self . data [ total_size ], y = self . data [ duration ], s = point_size , * args , ** kwargs ) return plot tools \u00b6 Tools for detecting collective events. binData \u00b6 Bases: detrender Smooth, de-trend, and binarise the input data. First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold. Attributes: Name Type Description smoothK int Size of the short-term median smoothing filter. biasK int Size of the long-term de-trending median filter. peakThr float Threshold for rescaling of the de-trended signal. binThr float Threshold for binarizing the de-trended signal. polyDeg int Sets the degree of the polynomial for lm fitting. biasMet str De-trending method, one of ['runmed', 'lm', 'none']. Source code in arcos4py/tools/_binarize_detrend.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 class binData ( detrender ): \"\"\"Smooth, de-trend, and binarise the input data. First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold. Attributes: smoothK (int): Size of the short-term median smoothing filter. biasK (int): Size of the long-term de-trending median filter. peakThr (float): Threshold for rescaling of the de-trended signal. binThr (float): Threshold for binarizing the de-trended signal. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): De-trending method, one of ['runmed', 'lm', 'none']. \"\"\" def __init__ ( self , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , binThr : float = 0.1 , polyDeg : int = 1 , biasMet : str = \"runmed\" , n_jobs : int = 1 , ) -> None : \"\"\"Smooth, de-trend, and binarise the input data. Arguments: smoothK (int): Size of the short-term median smoothing filter. biasK (int): Size of the long-term de-trending median filter. peakThr (float): Threshold for rescaling of the de-trended signal. binThr (float): Threshold for binarizing the de-trended signal. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): De-trending method, one of ['runmed', 'lm', 'none']. n_jobs (int): Number of jobs to run in parallel. \"\"\" super () . __init__ ( smoothK , biasK , peakThr , polyDeg , biasMet , n_jobs ) self . binThr = binThr def _rescale_data ( self , x : np . ndarray , meas_index : int , feat_range : tuple = ( 0 , 1 )) -> np . ndarray : \"\"\"Rescale data to a given range.\"\"\" meas_array = x [:, meas_index ] . astype ( 'float64' ) rescaled = minmax_scale ( meas_array , feature_range = feat_range ) x [:, meas_index ] = rescaled return x def _bin_data ( self , x : np . ndarray ) -> np . ndarray : bin = ( x >= self . binThr ) . astype ( np . int_ ) return bin def run ( self , x : pd . DataFrame , colGroup : str | None , colMeas : str , colFrame : str ) -> pd . DataFrame : \"\"\"Runs binarization and detrending. If the bias Method is 'none', first it rescales the data to between [0,1], then local smoothing is applied to the measurement by groups, followed by binarization. If biasMeth is one of ['lm', 'runmed'], first the data is detrended locally with a median filter and then detrended globally, for 'lm' with a linear model and for 'runmed' with a median filter. Followed by binarization of the data. Arguments: x (DataFrame): The time-series data for smoothing, detrending and binarization. colGroup (str | None): Object id column in x. Detrending and rescaling is performed on a per-object basis. If None, no detrending is performed, only rescaling and bias method is ignored. colMeas (str): Measurement column in x on which detrending and rescaling is performed. colFrame (str): Frame column in Time-series data. Used for sorting. Returns: DataFrame: Dataframe containing binarized data, rescaled data and the original columns. \"\"\" if colGroup is None : return self . _run_without_groupcol ( x , colMeas , colFrame ) else : return self . _run_with_groupcol ( x , colGroup , colMeas , colFrame ) def _run_without_groupcol ( self , x , colMeas , colFrame ): col_resc = f \" { colMeas } .resc\" col_bin = f \" { colMeas } .bin\" cols = [ colMeas ] x = x . sort_values ( colFrame ) data_np = x [ cols ] . to_numpy () if self . biasMet == \"none\" : rescaled_data = self . _rescale_data ( data_np , meas_index = 0 ) binarized_data = self . _bin_data ( rescaled_data ) else : warn ( \"No detrending is performed, only rescaling. To run detrending, set colGroup.\" ) rescaled_data = self . _rescale_data ( data_np , meas_index = 0 ) binarized_data = self . _bin_data ( rescaled_data ) x [ col_resc ] = rescaled_data [:, 0 ] x [ col_bin ] = binarized_data [:, 0 ] return x def _run_with_groupcol ( self , x , colGroup , colMeas , colFrame ): col_resc = f \" { colMeas } .resc\" col_bin = f \" { colMeas } .bin\" col_fact = f ' { colGroup } _factorized' cols = [ col_fact , colMeas ] x = x . sort_values ([ colGroup , colFrame ]) # factorize column in order to prevent numpy grouping error in detrending value , label = x [ colGroup ] . factorize () x [ col_fact ] = value data_np = x [ cols ] . to_numpy () if self . biasMet == \"none\" : rescaled_data = self . _rescale_data ( data_np , meas_index = 1 ) detrended_data = self . detrend ( rescaled_data , 0 , 1 ) binarized_data = self . _bin_data ( detrended_data ) else : detrended_data = self . detrend ( data_np , group_index = 0 , meas_index = 1 ) binarized_data = self . _bin_data ( detrended_data ) x = x . drop ([ col_fact ], axis = 1 ) x [ col_resc ] = detrended_data x [ col_bin ] = binarized_data return x __init__ ( smoothK = 3 , biasK = 51 , peakThr = 0.2 , binThr = 0.1 , polyDeg = 1 , biasMet = 'runmed' , n_jobs = 1 ) \u00b6 Smooth, de-trend, and binarise the input data. Parameters: Name Type Description Default smoothK int Size of the short-term median smoothing filter. 3 biasK int Size of the long-term de-trending median filter. 51 peakThr float Threshold for rescaling of the de-trended signal. 0.2 binThr float Threshold for binarizing the de-trended signal. 0.1 polyDeg int Sets the degree of the polynomial for lm fitting. 1 biasMet str De-trending method, one of ['runmed', 'lm', 'none']. 'runmed' n_jobs int Number of jobs to run in parallel. 1 Source code in arcos4py/tools/_binarize_detrend.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def __init__ ( self , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , binThr : float = 0.1 , polyDeg : int = 1 , biasMet : str = \"runmed\" , n_jobs : int = 1 , ) -> None : \"\"\"Smooth, de-trend, and binarise the input data. Arguments: smoothK (int): Size of the short-term median smoothing filter. biasK (int): Size of the long-term de-trending median filter. peakThr (float): Threshold for rescaling of the de-trended signal. binThr (float): Threshold for binarizing the de-trended signal. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): De-trending method, one of ['runmed', 'lm', 'none']. n_jobs (int): Number of jobs to run in parallel. \"\"\" super () . __init__ ( smoothK , biasK , peakThr , polyDeg , biasMet , n_jobs ) self . binThr = binThr run ( x , colGroup , colMeas , colFrame ) \u00b6 Runs binarization and detrending. If the bias Method is 'none', first it rescales the data to between [0,1], then local smoothing is applied to the measurement by groups, followed by binarization. If biasMeth is one of ['lm', 'runmed'], first the data is detrended locally with a median filter and then detrended globally, for 'lm' with a linear model and for 'runmed' with a median filter. Followed by binarization of the data. Parameters: Name Type Description Default x DataFrame The time-series data for smoothing, detrending and binarization. required colGroup str | None Object id column in x. Detrending and rescaling is performed on a per-object basis. If None, no detrending is performed, only rescaling and bias method is ignored. required colMeas str Measurement column in x on which detrending and rescaling is performed. required colFrame str Frame column in Time-series data. Used for sorting. required Returns: Name Type Description DataFrame pd . DataFrame Dataframe containing binarized data, rescaled data and the original columns. Source code in arcos4py/tools/_binarize_detrend.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def run ( self , x : pd . DataFrame , colGroup : str | None , colMeas : str , colFrame : str ) -> pd . DataFrame : \"\"\"Runs binarization and detrending. If the bias Method is 'none', first it rescales the data to between [0,1], then local smoothing is applied to the measurement by groups, followed by binarization. If biasMeth is one of ['lm', 'runmed'], first the data is detrended locally with a median filter and then detrended globally, for 'lm' with a linear model and for 'runmed' with a median filter. Followed by binarization of the data. Arguments: x (DataFrame): The time-series data for smoothing, detrending and binarization. colGroup (str | None): Object id column in x. Detrending and rescaling is performed on a per-object basis. If None, no detrending is performed, only rescaling and bias method is ignored. colMeas (str): Measurement column in x on which detrending and rescaling is performed. colFrame (str): Frame column in Time-series data. Used for sorting. Returns: DataFrame: Dataframe containing binarized data, rescaled data and the original columns. \"\"\" if colGroup is None : return self . _run_without_groupcol ( x , colMeas , colFrame ) else : return self . _run_with_groupcol ( x , colGroup , colMeas , colFrame ) calcCollevStats \u00b6 Class to calculate statistics of collective events. Source code in arcos4py/tools/_stats.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 class calcCollevStats : \"\"\"Class to calculate statistics of collective events.\"\"\" def __init__ ( self ) -> None : \"\"\"Class to calculate statistics of collective events.\"\"\" pass def _calculate_stats_group ( self , data : np . ndarray ) -> np . ndarray : \"\"\"Calculates duration and size for the collective event in the dataframe. Arguments: data (np.ndarray): Containing a single collective event. Returns: np.ndarray: Array containing the collective event id, duration, tot_size, min_size, max_size, \"nd_frame, first_frame_centroid and last_frame_centroid of the current collective event. \"\"\" coll_dur = max ( data [:, 0 ]) - min ( data [:, 0 ]) + 1 coll_total_size = np . unique ( data [:, 1 ]) . size ( unique , counts ) = np . unique ( data [:, 0 ], return_counts = True ) frequencies = np . asarray (( unique , counts )) . T coll_min_size = np . min ( frequencies [:, 1 ]) coll_max_size = np . max ( frequencies [:, 1 ]) coll_start_frame = np . min ( data [:, 0 ]) coll_end_frame = np . max ( data [:, 0 ]) if data . shape [ 1 ] > 3 : coll_start_coord = np . mean ( data [( data [:, 0 ] == coll_start_frame )][:, 3 :], axis = 0 ) coll_end_coord = np . mean ( data [( data [:, 0 ] == coll_end_frame )][:, 3 :], axis = 0 ) else : coll_start_coord = np . nan coll_end_coord = np . nan d = np . array ( [ data [ 0 , 2 ], coll_dur , coll_total_size , coll_min_size , coll_max_size , coll_start_frame , coll_end_frame , coll_start_coord , coll_end_coord , ], dtype = object , ) return d def _get_collev_stats ( self , data : pd . DataFrame , frame_column : str , collev_id : str , obj_id_column : str , posCol : Union [ list , None ], ) -> pd . DataFrame : \"\"\"Applies self._calculate_duration_size_group() to every group\\ i.e. every collective event. Arguments: data (DataFrame): Containing unfiltered collective events. collev_id (str): Indicating the contained collective id column. frame_column (str): Indicating the contained frame column. obj_id_column (str): Indicating object id. posCol (list | None): Contains names of position columns. If None coordinates of start and end frame are not calcualted Returns: DataFrame: DataFrame containing \"collid\", \"duration\", \"total_size\", \"min_size\",\"max_size\", \"start_frame\", \"end_frame\", \"first_frame_centroid\" and \"last_frame_centroid\" of all collective events. \"\"\" cols = [ collev_id , \"duration\" , \"total_size\" , \"min_size\" , \"max_size\" , \"start_frame\" , \"end_frame\" , \"first_frame_centroid\" , \"last_frame_centroid\" , ] if data . empty : return pd . DataFrame ( data = { i : [ np . nan ] for i in cols }) subset = [ frame_column , obj_id_column , collev_id ] if posCol : subset . extend ( posCol ) # if object id cannot be converted to a float, generate unique labels with pd.factorize try : data_np = data [ subset ] . to_numpy ( dtype = np . float64 ) except ValueError : labels , levels = pd . factorize ( data [ obj_id_column ]) new_obj_id = f ' { obj_id_column } _labels' data [ new_obj_id ] = labels subset [ 1 ] = new_obj_id data_np = data [ subset ] . to_numpy ( dtype = np . float64 ) data_np = data_np [ ~ np . isnan ( data_np ) . any ( axis = 1 )] data_np_sorted = data_np [ data_np [:, 2 ] . argsort ()] grouped_array = np . split ( data_np_sorted , np . unique ( data_np_sorted [:, 2 ], axis = 0 , return_index = True )[ 1 ][ 1 :]) # map to grouped_array out = map ( self . _calculate_stats_group , grouped_array ) out_list = [ i for i in out ] df = pd . DataFrame ( out_list , columns = cols ) return df def calculate ( self , data : pd . DataFrame , frame_column : str , collid_column : str , obj_id_column : str , posCol : Union [ list , None ] = None , ) -> pd . DataFrame : \"\"\"Calculate statistics of collective events. Arguments: data (DataFrame): Containing collective events. frame_column (str): Indicating the frame column in data. collid_column (str): Indicating the collective event id column in data. obj_id_column (str): Indicating object id. posCol (list | None): Contains names of position columns. If None coordinates of start and end frame are not calcualted Returns: DataFrame: DataFrame containing \"collid\", \"duration\", \"total_size\", \"min_size\",\"max_size\", \"start_frame\", \"end_frame\", \"first_frame_centroid\" and \"last_frame_centroid\" of all collective events. \"\"\" colev_stats = self . _get_collev_stats ( data , frame_column , collid_column , obj_id_column , posCol ) return colev_stats __init__ () \u00b6 Class to calculate statistics of collective events. Source code in arcos4py/tools/_stats.py 18 19 20 def __init__ ( self ) -> None : \"\"\"Class to calculate statistics of collective events.\"\"\" pass calculate ( data , frame_column , collid_column , obj_id_column , posCol = None ) \u00b6 Calculate statistics of collective events. Parameters: Name Type Description Default data DataFrame Containing collective events. required frame_column str Indicating the frame column in data. required collid_column str Indicating the collective event id column in data. required obj_id_column str Indicating object id. required posCol list | None Contains names of position columns. If None coordinates of start and end frame are not calcualted None Returns: Name Type Description DataFrame pd . DataFrame DataFrame containing \"collid\", \"duration\", \"total_size\", \"min_size\",\"max_size\", \"start_frame\", \"end_frame\", \"first_frame_centroid\" and \"last_frame_centroid\" of all collective events. Source code in arcos4py/tools/_stats.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def calculate ( self , data : pd . DataFrame , frame_column : str , collid_column : str , obj_id_column : str , posCol : Union [ list , None ] = None , ) -> pd . DataFrame : \"\"\"Calculate statistics of collective events. Arguments: data (DataFrame): Containing collective events. frame_column (str): Indicating the frame column in data. collid_column (str): Indicating the collective event id column in data. obj_id_column (str): Indicating object id. posCol (list | None): Contains names of position columns. If None coordinates of start and end frame are not calcualted Returns: DataFrame: DataFrame containing \"collid\", \"duration\", \"total_size\", \"min_size\",\"max_size\", \"start_frame\", \"end_frame\", \"first_frame_centroid\" and \"last_frame_centroid\" of all collective events. \"\"\" colev_stats = self . _get_collev_stats ( data , frame_column , collid_column , obj_id_column , posCol ) return colev_stats clipMeas \u00b6 Clip input array. Source code in arcos4py/tools/_cleandata.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 class clipMeas : \"\"\"Clip input array.\"\"\" def __init__ ( self , data : np . ndarray ) -> None : \"\"\"Clips array to quantilles. Arguments: data (ndarray): To be clipped. \"\"\" self . data = data def _calculate_percentile ( self , data : np . ndarray , clip_low : float , clip_high : float ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Calculate upper and lower quantille. Arguments: data (ndarray): To calculate upper and lower quantile on. clip_low (float): Lower clipping boundary (quantile). clip_high (float): Upper clipping boundry (quantille). Returns: np.ndarray: Array with lower quantile and array with upper quantile. \"\"\" quantille_low = np . quantile ( data , clip_low , keepdims = True ) quantille_high = np . quantile ( data , clip_high , keepdims = True ) return quantille_low , quantille_high def clip ( self , clip_low : float = 0.001 , clip_high : float = 0.999 ) -> np . ndarray : \"\"\"Clip input array to upper and lower quantiles defined in clip_low and clip_high. Arguments: clip_low (float): Lower clipping boundary (quantile). clip_high (float): Upper clipping boundry (quantille). Returns: np.ndarray (np.ndarray): A clipped array of the input data. \"\"\" low , high = self . _calculate_percentile ( self . data , clip_low , clip_high ) out = self . data . clip ( low , high ) return out __init__ ( data ) \u00b6 Clips array to quantilles. Parameters: Name Type Description Default data ndarray To be clipped. required Source code in arcos4py/tools/_cleandata.py 54 55 56 57 58 59 60 def __init__ ( self , data : np . ndarray ) -> None : \"\"\"Clips array to quantilles. Arguments: data (ndarray): To be clipped. \"\"\" self . data = data clip ( clip_low = 0.001 , clip_high = 0.999 ) \u00b6 Clip input array to upper and lower quantiles defined in clip_low and clip_high. Parameters: Name Type Description Default clip_low float Lower clipping boundary (quantile). 0.001 clip_high float Upper clipping boundry (quantille). 0.999 Returns: Type Description np . ndarray np.ndarray (np.ndarray): A clipped array of the input data. Source code in arcos4py/tools/_cleandata.py 80 81 82 83 84 85 86 87 88 89 90 91 92 def clip ( self , clip_low : float = 0.001 , clip_high : float = 0.999 ) -> np . ndarray : \"\"\"Clip input array to upper and lower quantiles defined in clip_low and clip_high. Arguments: clip_low (float): Lower clipping boundary (quantile). clip_high (float): Upper clipping boundry (quantille). Returns: np.ndarray (np.ndarray): A clipped array of the input data. \"\"\" low , high = self . _calculate_percentile ( self . data , clip_low , clip_high ) out = self . data . clip ( low , high ) return out detectCollev \u00b6 Class to detect collective events. Attributes: Name Type Description input_data Union [ pd . DataFrame , np . ndarray ] The input data to track. eps float Maximum distance for clustering, default is 1. epsPrev Union [ float , None] Maximum distance for linking previous clusters, if None, eps is used. Default is None. minClSz int Minimum cluster size. Default is 3. nPrev int Number of previous frames to consider. Default is 1. posCols list List of column names for the position columns. Default is [\"x\"]. frame_column str Name of the column containing the frame number. Default is 'time'. id_column Union [ str , None] Name of the column containing the id. Default is None. bin_meas_column Union [ str , None] Name of the column containing the binary measurement. Default is 'meas'. clid_column str Name of the column containing the cluster id. Default is 'clTrackID'. dims str String of dimensions in order, such as. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", \"Z\". method str The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\". min_samples int | None The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. linkingMethod str The method used for linking. Default is 'nearest'. n_jobs int Number of jobs to run in parallel. Default is 1. predictor bool | Callable Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details. show_progress bool Whether or not to show progress bar. Default is True. Source code in arcos4py/tools/_detect_events.py 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 class detectCollev : \"\"\"Class to detect collective events. Attributes: input_data (Union[pd.DataFrame, np.ndarray]): The input data to track. eps (float): Maximum distance for clustering, default is 1. epsPrev (Union[float, None]): Maximum distance for linking previous clusters, if None, eps is used. Default is None. minClSz (int): Minimum cluster size. Default is 3. nPrev (int): Number of previous frames to consider. Default is 1. posCols (list): List of column names for the position columns. Default is [\"x\"]. frame_column (str): Name of the column containing the frame number. Default is 'time'. id_column (Union[str, None]): Name of the column containing the id. Default is None. bin_meas_column (Union[str, None]): Name of the column containing the binary measurement. Default is 'meas'. clid_column (str): Name of the column containing the cluster id. Default is 'clTrackID'. dims (str): String of dimensions in order, such as. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", \"Z\". method (str): The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\". min_samples (int | None): The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. linkingMethod (str): The method used for linking. Default is 'nearest'. n_jobs (int): Number of jobs to run in parallel. Default is 1. predictor (bool | Callable): Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details. show_progress (bool): Whether or not to show progress bar. Default is True. \"\"\" def __init__ ( self , input_data : Union [ pd . DataFrame , np . ndarray ], eps : float = 1 , epsPrev : Union [ int , None ] = None , minClSz : int = 1 , nPrev : int = 1 , posCols : list = [ \"x\" ], frame_column : str = 'time' , id_column : Union [ str , None ] = None , bin_meas_column : Union [ str , None ] = 'meas' , clid_column : str = 'clTrackID' , dims : str = \"TXY\" , method : str = \"dbscan\" , min_samples : int | None = None , linkingMethod = 'nearest' , n_jobs : int = 1 , predictor : bool | Callable = False , show_progress : bool = True , ) -> None : \"\"\"Constructs class with input parameters. Arguments: input_data (DataFrame): Input data to be processed. Must contain a binarized measurement column. eps (float): The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. epsPrev (int | None): Frame to frame distance, value is used to connect collective events across multiple frames.If \"None\", same value as eps is used. minClSz (int): Minimum size for a cluster to be identified as a collective event. nPrev (int): Number of previous frames the tracking algorithm looks back to connect collective events. posCols (list): List of position columns contained in the data. Must at least contain one. frame_column (str): Indicating the frame column in input_data. id_column (str | None): Indicating the track id/id column in input_data, optional. bin_meas_column (str): Indicating the bin_meas_column in input_data or None. clid_column (str): Indicating the column name containing the ids of collective events. dims (str): String of dimensions in order, used if input_data is a numpy array. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", \"Z\". method (str): The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\". min_samples (int | None): The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. linkingMethod (str): The method used for linking. Default is 'nearest'. n_jobs (int): Number of paralell workers to spawn, -1 uses all available cpus. predictor (bool | Callable): Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details. show_progress (bool): Whether or not to show progress bar. Default is True. \"\"\" self . input_data = input_data self . eps = eps self . epsPrev = epsPrev self . minClSz = minClSz self . nPrev = nPrev self . posCols = posCols self . frame_column = frame_column self . id_column = id_column self . bin_meas_column = bin_meas_column self . clid_column = clid_column self . dims = dims self . method = method self . linkingMethod = linkingMethod self . min_samples = min_samples self . predictor = predictor self . n_jobs = n_jobs self . show_progress = show_progress warnings . warn ( \"This class is deprecated and will be removed a future release, use the track_events_dataframe or track_events_image functions directly.\" , # noqa: E501 DeprecationWarning , ) def run ( self , copy : bool = True ) -> pd . DataFrame : \"\"\"Runs the collective event detection algorithm. Arguments: copy (bool): Whether or not to copy the input data. Default is True. Returns: DataFrame: Input data with added collective event ids. \"\"\" if isinstance ( self . input_data , pd . DataFrame ): if copy : self . input_data = self . input_data . copy () return track_events_dataframe ( X = self . input_data , coordinates_column = self . posCols , frame_column = self . frame_column , id_column = self . id_column , bin_meas_column = self . bin_meas_column , collid_column = self . clid_column , eps = self . eps , epsPrev = self . epsPrev , minClSz = self . minClSz , minSamples = self . min_samples , clusteringMethod = self . method , linkingMethod = self . linkingMethod , nPrev = self . nPrev , predictor = self . predictor , nJobs = self . n_jobs , showProgress = self . show_progress , ) elif isinstance ( self . input_data , np . ndarray ): if copy : self . input_data = np . copy ( self . input_data ) return track_events_image ( X = self . input_data , eps = self . eps , epsPrev = self . epsPrev , minClSz = self . minClSz , minSamples = self . min_samples , clusteringMethod = self . method , nPrev = self . nPrev , predictor = self . predictor , linkingMethod = self . linkingMethod , dims = self . dims , nJobs = self . n_jobs , showProgress = self . show_progress , ) __init__ ( input_data , eps = 1 , epsPrev = None , minClSz = 1 , nPrev = 1 , posCols = [ 'x' ], frame_column = 'time' , id_column = None , bin_meas_column = 'meas' , clid_column = 'clTrackID' , dims = 'TXY' , method = 'dbscan' , min_samples = None , linkingMethod = 'nearest' , n_jobs = 1 , predictor = False , show_progress = True ) \u00b6 Constructs class with input parameters. Parameters: Name Type Description Default input_data DataFrame Input data to be processed. Must contain a binarized measurement column. required eps float The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. 1 epsPrev int | None Frame to frame distance, value is used to connect collective events across multiple frames.If \"None\", same value as eps is used. None minClSz int Minimum size for a cluster to be identified as a collective event. 1 nPrev int Number of previous frames the tracking algorithm looks back to connect collective events. 1 posCols list List of position columns contained in the data. Must at least contain one. ['x'] frame_column str Indicating the frame column in input_data. 'time' id_column str | None Indicating the track id/id column in input_data, optional. None bin_meas_column str Indicating the bin_meas_column in input_data or None. 'meas' clid_column str Indicating the column name containing the ids of collective events. 'clTrackID' dims str String of dimensions in order, used if input_data is a numpy array. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", \"Z\". 'TXY' method str The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\". 'dbscan' min_samples int | None The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. None linkingMethod str The method used for linking. Default is 'nearest'. 'nearest' n_jobs int Number of paralell workers to spawn, -1 uses all available cpus. 1 predictor bool | Callable Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details. False show_progress bool Whether or not to show progress bar. Default is True. True Source code in arcos4py/tools/_detect_events.py 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 def __init__ ( self , input_data : Union [ pd . DataFrame , np . ndarray ], eps : float = 1 , epsPrev : Union [ int , None ] = None , minClSz : int = 1 , nPrev : int = 1 , posCols : list = [ \"x\" ], frame_column : str = 'time' , id_column : Union [ str , None ] = None , bin_meas_column : Union [ str , None ] = 'meas' , clid_column : str = 'clTrackID' , dims : str = \"TXY\" , method : str = \"dbscan\" , min_samples : int | None = None , linkingMethod = 'nearest' , n_jobs : int = 1 , predictor : bool | Callable = False , show_progress : bool = True , ) -> None : \"\"\"Constructs class with input parameters. Arguments: input_data (DataFrame): Input data to be processed. Must contain a binarized measurement column. eps (float): The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. epsPrev (int | None): Frame to frame distance, value is used to connect collective events across multiple frames.If \"None\", same value as eps is used. minClSz (int): Minimum size for a cluster to be identified as a collective event. nPrev (int): Number of previous frames the tracking algorithm looks back to connect collective events. posCols (list): List of position columns contained in the data. Must at least contain one. frame_column (str): Indicating the frame column in input_data. id_column (str | None): Indicating the track id/id column in input_data, optional. bin_meas_column (str): Indicating the bin_meas_column in input_data or None. clid_column (str): Indicating the column name containing the ids of collective events. dims (str): String of dimensions in order, used if input_data is a numpy array. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", \"Z\". method (str): The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\". min_samples (int | None): The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. linkingMethod (str): The method used for linking. Default is 'nearest'. n_jobs (int): Number of paralell workers to spawn, -1 uses all available cpus. predictor (bool | Callable): Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details. show_progress (bool): Whether or not to show progress bar. Default is True. \"\"\" self . input_data = input_data self . eps = eps self . epsPrev = epsPrev self . minClSz = minClSz self . nPrev = nPrev self . posCols = posCols self . frame_column = frame_column self . id_column = id_column self . bin_meas_column = bin_meas_column self . clid_column = clid_column self . dims = dims self . method = method self . linkingMethod = linkingMethod self . min_samples = min_samples self . predictor = predictor self . n_jobs = n_jobs self . show_progress = show_progress warnings . warn ( \"This class is deprecated and will be removed a future release, use the track_events_dataframe or track_events_image functions directly.\" , # noqa: E501 DeprecationWarning , ) run ( copy = True ) \u00b6 Runs the collective event detection algorithm. Parameters: Name Type Description Default copy bool Whether or not to copy the input data. Default is True. True Returns: Name Type Description DataFrame pd . DataFrame Input data with added collective event ids. Source code in arcos4py/tools/_detect_events.py 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 def run ( self , copy : bool = True ) -> pd . DataFrame : \"\"\"Runs the collective event detection algorithm. Arguments: copy (bool): Whether or not to copy the input data. Default is True. Returns: DataFrame: Input data with added collective event ids. \"\"\" if isinstance ( self . input_data , pd . DataFrame ): if copy : self . input_data = self . input_data . copy () return track_events_dataframe ( X = self . input_data , coordinates_column = self . posCols , frame_column = self . frame_column , id_column = self . id_column , bin_meas_column = self . bin_meas_column , collid_column = self . clid_column , eps = self . eps , epsPrev = self . epsPrev , minClSz = self . minClSz , minSamples = self . min_samples , clusteringMethod = self . method , linkingMethod = self . linkingMethod , nPrev = self . nPrev , predictor = self . predictor , nJobs = self . n_jobs , showProgress = self . show_progress , ) elif isinstance ( self . input_data , np . ndarray ): if copy : self . input_data = np . copy ( self . input_data ) return track_events_image ( X = self . input_data , eps = self . eps , epsPrev = self . epsPrev , minClSz = self . minClSz , minSamples = self . min_samples , clusteringMethod = self . method , nPrev = self . nPrev , predictor = self . predictor , linkingMethod = self . linkingMethod , dims = self . dims , nJobs = self . n_jobs , showProgress = self . show_progress , ) filterCollev \u00b6 Select Collective events that last longer than coll_duration and have a larger total size than coll_total_size. Attributes: Name Type Description data Dataframe With detected collective events. frame_column str Indicating the frame column in data. collid_column str Indicating the collective event id column in data. obj_id_column str Inidicating the object identifier column such as cell track id. Source code in arcos4py/tools/_filter_events.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 class filterCollev : \"\"\"Select Collective events that last longer than coll_duration\\ and have a larger total size than coll_total_size. Attributes: data (Dataframe): With detected collective events. frame_column (str): Indicating the frame column in data. collid_column (str): Indicating the collective event id column in data. obj_id_column (str): Inidicating the object identifier column such as cell track id. \"\"\" def __init__ ( self , data : pd . DataFrame , frame_column : str = \"time\" , collid_column : str = \"collid\" , obj_id_column : str = \"trackID\" , ): \"\"\"Constructs filterCollev class with Parameters. Arguments: data (Dataframe): With detected collective events. frame_column (str): Indicating the frame column in data. collid_column (str): Indicating the collective event id column in data. obj_id_column (str): Inidicating the object identifier column such as cell track id. \"\"\" self . data = data self . frame_column = frame_column self . collid_column = collid_column self . obj_id_column = obj_id_column def _filter_collev ( self , data : pd . DataFrame , collev_stats : pd . DataFrame , collev_id : str , min_duration : int , min_size : int , ): \"\"\"Uses the dataframe generated by self._get_collev_duration()\\ to filter collective events that last longer than\\ min_duration and are larger than min_size. Arguments: data (DataFrame): Containing unfiltered collective events. collev_stats (DataFrame): Containing stats of collective events. collev_id (str): Indicating the contained collective id column. min_duration (str): minimal duration of a collective event for it to be returned. min_size (int): minimal size for a collective event to be returned. Returns: DataFrame (DataFrame): Dataframe containing filtered collective events. \"\"\" collev_stats = collev_stats [ ( collev_stats [ \"duration\" ] >= min_duration ) & ( collev_stats [ \"total_size\" ] >= min_size ) ] data = data [ data [ collev_id ] . isin ( collev_stats [ collev_id ])] return data def filter ( self , coll_duration : int = 9 , coll_total_size : int = 10 ) -> pd . DataFrame : \"\"\"Filter collective events. Method to filter collective events according to the parameters specified in the object instance. Arguments: coll_duration (int): Minimal duration of collective events to be selected. coll_total_size (int): Minimal total size of collective events to be selected. Returns: Returns pandas dataframe containing filtered collective events \"\"\" if self . data . empty : return self . data stats = calcCollevStats () colev_duration = stats . calculate ( self . data , self . frame_column , self . collid_column , self . obj_id_column ) filtered_df = self . _filter_collev ( self . data , colev_duration , self . collid_column , coll_duration , coll_total_size , ) return filtered_df __init__ ( data , frame_column = 'time' , collid_column = 'collid' , obj_id_column = 'trackID' ) \u00b6 Constructs filterCollev class with Parameters. Parameters: Name Type Description Default data Dataframe With detected collective events. required frame_column str Indicating the frame column in data. 'time' collid_column str Indicating the collective event id column in data. 'collid' obj_id_column str Inidicating the object identifier column such as cell track id. 'trackID' Source code in arcos4py/tools/_filter_events.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , data : pd . DataFrame , frame_column : str = \"time\" , collid_column : str = \"collid\" , obj_id_column : str = \"trackID\" , ): \"\"\"Constructs filterCollev class with Parameters. Arguments: data (Dataframe): With detected collective events. frame_column (str): Indicating the frame column in data. collid_column (str): Indicating the collective event id column in data. obj_id_column (str): Inidicating the object identifier column such as cell track id. \"\"\" self . data = data self . frame_column = frame_column self . collid_column = collid_column self . obj_id_column = obj_id_column filter ( coll_duration = 9 , coll_total_size = 10 ) \u00b6 Filter collective events. Method to filter collective events according to the parameters specified in the object instance. Parameters: Name Type Description Default coll_duration int Minimal duration of collective events to be selected. 9 coll_total_size int Minimal total size of collective events to be selected. 10 Returns: Type Description pd . DataFrame Returns pandas dataframe containing filtered collective events Source code in arcos4py/tools/_filter_events.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def filter ( self , coll_duration : int = 9 , coll_total_size : int = 10 ) -> pd . DataFrame : \"\"\"Filter collective events. Method to filter collective events according to the parameters specified in the object instance. Arguments: coll_duration (int): Minimal duration of collective events to be selected. coll_total_size (int): Minimal total size of collective events to be selected. Returns: Returns pandas dataframe containing filtered collective events \"\"\" if self . data . empty : return self . data stats = calcCollevStats () colev_duration = stats . calculate ( self . data , self . frame_column , self . collid_column , self . obj_id_column ) filtered_df = self . _filter_collev ( self . data , colev_duration , self . collid_column , coll_duration , coll_total_size , ) return filtered_df interpolation \u00b6 Interpolate nan values in a numpy array. Attributes: Name Type Description data DataFrame Where NaN should be replaced with interpolated values. Source code in arcos4py/tools/_cleandata.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class interpolation : \"\"\"Interpolate nan values in a numpy array. Attributes: data (DataFrame): Where NaN should be replaced with interpolated values. \"\"\" def __init__ ( self , data : pd . DataFrame ): \"\"\"Interpolate nan values in a pandas dataframe. Uses pandas.interpolate with liner interpolation. Arguments: data (DataFrame): Where NaN should be replaced with interpolated values. \"\"\" self . data = data def interpolate ( self ) -> pd . DataFrame : \"\"\"Interpolate nan and missing values. Returns: DataFrame: Interpolated input data. \"\"\" self . data = self . data . interpolate ( axis = 0 ) return self . data __init__ ( data ) \u00b6 Interpolate nan values in a pandas dataframe. Uses pandas.interpolate with liner interpolation. Parameters: Name Type Description Default data DataFrame Where NaN should be replaced with interpolated values. required Source code in arcos4py/tools/_cleandata.py 30 31 32 33 34 35 36 37 38 def __init__ ( self , data : pd . DataFrame ): \"\"\"Interpolate nan values in a pandas dataframe. Uses pandas.interpolate with liner interpolation. Arguments: data (DataFrame): Where NaN should be replaced with interpolated values. \"\"\" self . data = data interpolate () \u00b6 Interpolate nan and missing values. Returns: Name Type Description DataFrame pd . DataFrame Interpolated input data. Source code in arcos4py/tools/_cleandata.py 40 41 42 43 44 45 46 47 48 def interpolate ( self ) -> pd . DataFrame : \"\"\"Interpolate nan and missing values. Returns: DataFrame: Interpolated input data. \"\"\" self . data = self . data . interpolate ( axis = 0 ) return self . data estimate_eps ( data , method = 'kneepoint' , pos_cols = [ 'x,y' ], frame_col = 't' , n_neighbors = 5 , plot = True , plt_size = ( 5 , 5 ), max_samples = 50000 , ** kwargs ) \u00b6 Estimates eps parameter in DBSCAN. Estimates the eps parameter for the DBSCAN clustering method, as used by ARCOS, by calculating the nearest neighbour distances for each point in the data. N_neighbours should be chosen to match the minimum point size in DBSCAN or the minimum clustersize in detect_events respectively. The method argument determines how the eps parameter is estimated. 'kneepoint' estimates the knee of the nearest neighbour distribution. 'mean' and 'median' return (by default) 1.5 times the mean or median of the nearest neighbour distances respectively. Parameters: Name Type Description Default data pd . DataFrame DataFrame containing the data. required method str Method to use for estimating eps. Defaults to 'kneepoint'. Can be one of ['kneepoint', 'mean', 'median'].'kneepoint' estimates the knee of the nearest neighbour distribution to to estimate eps. 'mean' and 'median' use the 1.5 times the mean or median of the nearest neighbour distances respectively. 'kneepoint' pos_cols list [ str ] List of column names containing the position data. ['x,y'] frame_col str Name of the column containing the frame number. Defaults to 't'. 't' n_neighbors int Number of nearest neighbours to consider. Defaults to 5. 5 plot bool Whether to plot the results. Defaults to True. True plt_size tuple [ int , int ] Size of the plot. Defaults to (5, 5). (5, 5) kwargs dict Keyword arguments for the method. Modify behaviour of respecitve method. For kneepoint: [S online, curve, direction, interp_method,polynomial_degree; For mean: [mean_multiplier] For median [median_multiplier] {} Returns: Name Type Description Eps float eps parameter for DBSCAN. Source code in arcos4py/tools/_detect_events.py 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 def estimate_eps ( data : pd . DataFrame , method : str = 'kneepoint' , pos_cols : list [ str ] = [ 'x,y' ], frame_col : str = 't' , n_neighbors : int = 5 , plot : bool = True , plt_size : tuple [ int , int ] = ( 5 , 5 ), max_samples = 50_000 , ** kwargs : dict , ): \"\"\"Estimates eps parameter in DBSCAN. Estimates the eps parameter for the DBSCAN clustering method, as used by ARCOS, by calculating the nearest neighbour distances for each point in the data. N_neighbours should be chosen to match the minimum point size in DBSCAN or the minimum clustersize in detect_events respectively. The method argument determines how the eps parameter is estimated. 'kneepoint' estimates the knee of the nearest neighbour distribution. 'mean' and 'median' return (by default) 1.5 times the mean or median of the nearest neighbour distances respectively. Arguments: data (pd.DataFrame): DataFrame containing the data. method (str, optional): Method to use for estimating eps. Defaults to 'kneepoint'. Can be one of ['kneepoint', 'mean', 'median'].'kneepoint' estimates the knee of the nearest neighbour distribution to to estimate eps. 'mean' and 'median' use the 1.5 times the mean or median of the nearest neighbour distances respectively. pos_cols (list[str]): List of column names containing the position data. frame_col (str, optional): Name of the column containing the frame number. Defaults to 't'. n_neighbors (int, optional): Number of nearest neighbours to consider. Defaults to 5. plot (bool, optional): Whether to plot the results. Defaults to True. plt_size (tuple[int, int], optional): Size of the plot. Defaults to (5, 5). kwargs: Keyword arguments for the method. Modify behaviour of respecitve method. For kneepoint: [S online, curve, direction, interp_method,polynomial_degree; For mean: [mean_multiplier] For median [median_multiplier] Returns: Eps (float): eps parameter for DBSCAN. \"\"\" subset = [ frame_col ] + pos_cols for i in subset : if i not in data . columns : raise ValueError ( f \"Column { i } not in data\" ) method_option = [ 'kneepoint' , 'mean' , 'median' ] if method not in method_option : raise ValueError ( f \"Method must be one of { method_option } \" ) allowedtypes : dict [ str , str ] = { 'kneepoint' : 'kneepoint_values' , 'mean' : 'mean_values' , 'median' : 'median_values' , } kwdefaults : dict [ str , Any ] = { 'S' : 1 , 'online' : True , 'curve' : 'convex' , 'direction' : 'increasing' , 'interp_method' : 'polynomial' , 'mean_multiplier' : 1.5 , 'median_multiplier' : 1.5 , 'polynomial_degree' : 7 , } kwtypes : dict [ str , Any ] = { 'S' : int , 'online' : bool , 'curve' : str , 'direction' : str , 'interp_method' : str , 'polynomial_degree' : int , 'mean_multiplier' : ( float , int ), 'median_multiplier' : ( float , int ), } allowedkwargs : dict [ str , list [ str ]] = { 'kneepoint_values' : [ 'S' , 'online' , 'curve' , 'interp_method' , 'direction' , 'polynomial_degree' ], 'mean_values' : [ 'mean_multiplier' ], 'median_values' : [ 'median_multiplier' ], } for key in kwargs : if key not in allowedkwargs [ allowedtypes [ method ]]: raise ValueError ( f ' { key } keyword not in allowed keywords { allowedkwargs [ allowedtypes [ method ]] } ' ) if not isinstance ( kwargs [ key ], kwtypes [ key ]): raise ValueError ( f ' { key } must be of type { kwtypes [ key ] } ' ) # Set kwarg defaults for kw in allowedkwargs [ allowedtypes [ method ]]: kwargs . setdefault ( kw , kwdefaults [ kw ]) subset = [ frame_col ] + pos_cols data_np = data [ subset ] . to_numpy ( dtype = np . float64 ) # sort by frame data_np = data_np [ data_np [:, 0 ] . argsort ()] grouped_array = np . split ( data_np [:, 1 :], np . unique ( data_np [:, 0 ], axis = 0 , return_index = True )[ 1 ][ 1 :]) # map nearest_neighbours to grouped_array distances = np . concatenate ([ _nearest_neighbour_eps ( i , n_neighbors ) for i in grouped_array if i . shape [ 0 ] > 1 ]) # flatten array distances_flat = distances . flatten () distances_flat = distances_flat [ np . isfinite ( distances_flat )] distances_flat_selection = np . random . choice ( distances_flat , min ( max_samples , distances_flat . shape [ 0 ]), replace = False ) distances_sorted = np . sort ( distances_flat_selection ) if method == 'kneepoint' : k1 = KneeLocator ( np . arange ( 0 , distances_sorted . shape [ 0 ]), distances_sorted , S = kwargs [ 'S' ], online = kwargs [ 'online' ], curve = kwargs [ 'curve' ], interp_method = kwargs [ 'interp_method' ], direction = kwargs [ 'direction' ], polynomial_degree = kwargs [ 'polynomial_degree' ], ) eps = distances_sorted [ k1 . knee ] elif method == 'mean' : eps = np . mean ( distances_sorted ) * kwargs [ 'mean_multiplier' ] elif method == 'median' : eps = np . median ( distances_sorted ) * kwargs [ 'median_multiplier' ] if plot : fig , ax = plt . subplots ( figsize = plt_size ) ax . plot ( distances_sorted ) ax . axhline ( eps , color = 'r' , linestyle = '--' ) ax . set_xlabel ( 'Sorted Distance Index' ) ax . set_ylabel ( 'Nearest Neighbour Distance' ) ax . set_title ( f 'Estimated eps: { eps : .4f } ' ) plt . show () return eps track_events_dataframe ( X , coordinates_column , frame_column , id_column , bin_meas_column = None , collid_column = 'collid' , eps = 1.0 , epsPrev = None , minClSz = 3 , minSamples = None , clusteringMethod = 'dbscan' , linkingMethod = 'nearest' , nPrev = 1 , predictor = False , nJobs = 1 , showProgress = True ) \u00b6 Function to track collective events in a dataframe. Parameters: Name Type Description Default X pd . DataFrame The input dataframe containing the data to track. required coordinates_column List [ str ] The names of the columns representing coordinates. required frame_column str The name of the column containing frame ids. required id_column str | None The name of the column representing IDs. None if no such column. required bin_meas_column str | None The name of the column representing binarized measurements, if None all measurements are used. None collid_column str The name of the output column representing collective events, will be generated. 'collid' eps float Maximum distance for clustering, default is 1. 1.0 epsPrev float | None Maximum distance for linking previous clusters, if None, eps is used. Default is None. None minClSz int Minimum cluster size. Default is 3. 3 minSamples int The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. None clusteringMethod str The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\". 'dbscan' linkingMethod str The method used for linking, one of ['nearest', 'transportsolver']. Default is 'nearest'. 'nearest' nPrev int Number of previous frames to consider. Default is 1. 1 predictor bool | Callable Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details. False nJobs int Number of jobs to run in parallel. Default is 1. 1 showProgress bool Whether or not to show progress bar. Default is True. True Returns: Type Description pd . DataFrame pd.DataFrame: Dataframe with tracked events. Source code in arcos4py/tools/_detect_events.py 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 def track_events_dataframe ( X : pd . DataFrame , coordinates_column : List [ str ], frame_column : str , id_column : str | None , bin_meas_column : str | None = None , collid_column : str = \"collid\" , eps : float = 1.0 , epsPrev : int | None = None , minClSz : int = 3 , minSamples : int | None = None , clusteringMethod : str = \"dbscan\" , linkingMethod : str = 'nearest' , nPrev : int = 1 , predictor : bool | Callable = False , nJobs : int = 1 , showProgress : bool = True , ) -> pd . DataFrame : \"\"\"Function to track collective events in a dataframe. Arguments: X (pd.DataFrame): The input dataframe containing the data to track. coordinates_column (List[str]): The names of the columns representing coordinates. frame_column (str): The name of the column containing frame ids. id_column (str | None): The name of the column representing IDs. None if no such column. bin_meas_column (str | None): The name of the column representing binarized measurements, if None all measurements are used. collid_column (str): The name of the output column representing collective events, will be generated. eps (float): Maximum distance for clustering, default is 1. epsPrev (float | None): Maximum distance for linking previous clusters, if None, eps is used. Default is None. minClSz (int): Minimum cluster size. Default is 3. minSamples (int): The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. clusteringMethod (str): The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\". linkingMethod (str): The method used for linking, one of ['nearest', 'transportsolver']. Default is 'nearest'. nPrev (int): Number of previous frames to consider. Default is 1. predictor (bool | Callable): Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details. nJobs (int): Number of jobs to run in parallel. Default is 1. showProgress (bool): Whether or not to show progress bar. Default is True. Returns: pd.DataFrame: Dataframe with tracked events. \"\"\" linker = Linker ( eps = eps , epsPrev = epsPrev , minClSz = minClSz , minSamples = minSamples , clusteringMethod = clusteringMethod , linkingMethod = linkingMethod , nPrev = nPrev , predictor = predictor , nJobs = nJobs , ) tracker = DataFrameTracker ( linker = linker , coordinates_column = coordinates_column , frame_column = frame_column , id_column = id_column , bin_meas_column = bin_meas_column , collid_column = collid_column , ) df_out = pd . concat ( [ timepoint for timepoint in tqdm ( tracker . track ( X ), total = X [ frame_column ] . nunique (), disable = not showProgress )] ) . reset_index ( drop = True ) return df_out . query ( f \" { collid_column } != -1\" ) . reset_index ( drop = True ) track_events_image ( X , eps = 1 , epsPrev = None , minClSz = 1 , minSamples = None , clusteringMethod = 'dbscan' , nPrev = 1 , predictor = False , linkingMethod = 'nearest' , dims = 'TXY' , downsample = 1 , nJobs = 1 , showProgress = True ) \u00b6 Function to track events in an image using specified linking and clustering methods. Parameters: Name Type Description Default X np . ndarray The input array containing the images to track. required eps float Distance for clustering. Default is 1. 1 epsPrev float | None Maximum distance for linking previous clusters, if None, eps is used. Default is None. None minClSz int Minimum cluster size. Default is 1. 1 minSamples int | None The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. None clusteringMethod str The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\". 'dbscan' nPrev int Number of previous frames to consider. Default is 1. 1 predictor bool | Callable Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details. False linkingMethod str The method used for linking. Default is 'nearest'. 'nearest' dims str String of dimensions in order, such as. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", \"Z\". 'TXY' downsample int Factor by which to downsample the image. Default is 1. 1 nJobs int Number of jobs to run in parallel. Default is 1. 1 showProgress bool Whether or not to show progress bar. Default is True. True Returns: Type Description np . ndarray np.ndarray: Array of images with tracked events. Source code in arcos4py/tools/_detect_events.py 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 def track_events_image ( X : np . ndarray , eps : float = 1 , epsPrev : int | None = None , minClSz : int = 1 , minSamples : int | None = None , clusteringMethod : str = \"dbscan\" , nPrev : int = 1 , predictor : bool | Callable = False , linkingMethod : str = 'nearest' , dims : str = \"TXY\" , downsample : int = 1 , nJobs : int = 1 , showProgress : bool = True , ) -> np . ndarray : \"\"\"Function to track events in an image using specified linking and clustering methods. Arguments: X (np.ndarray): The input array containing the images to track. eps (float): Distance for clustering. Default is 1. epsPrev (float | None): Maximum distance for linking previous clusters, if None, eps is used. Default is None. minClSz (int): Minimum cluster size. Default is 1. minSamples (int | None): The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. clusteringMethod (str): The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\". nPrev (int): Number of previous frames to consider. Default is 1. predictor (bool | Callable): Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details. linkingMethod (str): The method used for linking. Default is 'nearest'. dims (str): String of dimensions in order, such as. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", \"Z\". downsample (int): Factor by which to downsample the image. Default is 1. nJobs (int): Number of jobs to run in parallel. Default is 1. showProgress (bool): Whether or not to show progress bar. Default is True. Returns: np.ndarray: Array of images with tracked events. \"\"\" linker = Linker ( eps = eps / downsample , epsPrev = epsPrev , minClSz = minClSz , minSamples = minSamples , clusteringMethod = clusteringMethod , linkingMethod = linkingMethod , nPrev = nPrev , predictor = predictor , nJobs = nJobs , ) tracker = ImageTracker ( linker , downsample = downsample ) # find indices of T in dims T_index = dims . upper () . index ( \"T\" ) return np . stack ( [ timepoint for timepoint in tqdm ( tracker . track ( X , dims ), total = X . shape [ T_index ], disable = not showProgress )], axis = T_index , ) validation \u00b6 Tools for validating detected collective events. bootstrap_arcos ( df , posCols , frame_column , id_column , meas_column , method = 'shuffle_tracks' , smoothK = 3 , biasK = 51 , peakThr = 0.2 , binThr = 0.1 , polyDeg = 1 , biasMet = 'runmed' , eps = 2 , epsPrev = None , minClsz = 1 , nPrev = 1 , min_duration = 1 , min_size = 1 , stats_metric = [ 'total_size' , 'duration' ], pval_alternative = 'greater' , finite_correction = True , n = 100 , seed = 42 , allow_duplicates = False , max_tries = 100 , show_progress = True , verbose = False , paralell_processing = True , plot = True ) \u00b6 Bootstrap data using the ARCOS algorithm. Parameters: Name Type Description Default df pd . DataFrame DataFrame containing the data to be bootstrapped. required posCols list List of column names containing the x and y coordinates. required frame_column str Name of the column containing the frame number. required id_column str Name of the column containing the track id. required meas_column str Name of the column containing the measurement. required method str | list [ str ] Method used for bootstrapping. Can be \"shuffle_tracks\", 'shuffle_timepoints', 'shift_timepoints', 'shuffle_binary_blocks', 'shuffle_coordinates_timepoint or a list of methods, which will be applied in order of index. 'shuffle_tracks' smoothK int Smoothing kernel size. 3 biasK int Bias kernel size. 51 peakThr float Threshold for peak detection. 0.2 binThr float Threshold for binarization. 0.1 polyDeg int Degree of the polynomial used for bias correction. 1 biasMet str Bias correction method. Can be 'none', 'runmed', 'lm' 'runmed' eps float Epsilon parameter for DBSCAN. 2 epsPrev int | None Parameter for linking tracks. If None, eps is used. None minClsz int Minimum cluster size. 1 nPrev int Number of previous frames to consider for linking. 1 min_duration int Minimum duration of a track. 1 min_size int Minimum size of a track. 1 stats_metric str | list [ str ] Metric to calculate. Can be \"duration\", \"total_size\", \"min_size\", \"max_size\" or a list of metrics. Default is [\"duration\", \"total_size\"]. ['total_size', 'duration'] pval_alternative str Alternative hypothesis for the p-value calculation. Can be \"less\" or \"greater\". 'greater' finite_correction bool Correct p-values for finite sampling. Default is True. True n int Number of bootstraps. 100 seed int Seed for the random number generator. 42 allow_duplicates bool If False, resampling will check if the resampled data contains duplicates. If True, duplicates will be allowed. False max_tries int Maximum number of tries to resample data without duplicates. 100 show_progress bool Show a progress bar. True verbose bool Print additional information. False Returns: Type Description tuple [ pd . DataFrame , pd . DataFrame ] DataFrame containing the bootstrapped data. Source code in arcos4py/validation/_bootstrapping.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def bootstrap_arcos ( df : pd . DataFrame , posCols : list , frame_column : str , id_column : str , meas_column : str , method : str | list [ str ] = 'shuffle_tracks' , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , binThr : float = 0.1 , polyDeg : int = 1 , biasMet : str = \"runmed\" , eps : float = 2 , epsPrev : int | None = None , minClsz : int = 1 , nPrev : int = 1 , min_duration : int = 1 , min_size : int = 1 , stats_metric : str | list [ str ] = [ \"total_size\" , \"duration\" ], pval_alternative : str = \"greater\" , finite_correction : bool = True , n : int = 100 , seed : int = 42 , allow_duplicates : bool = False , max_tries : int = 100 , show_progress : bool = True , verbose : bool = False , paralell_processing : bool = True , plot : bool = True , ) -> tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\"Bootstrap data using the ARCOS algorithm. Arguments: df: DataFrame containing the data to be bootstrapped. posCols: List of column names containing the x and y coordinates. frame_column: Name of the column containing the frame number. id_column: Name of the column containing the track id. meas_column: Name of the column containing the measurement. method: Method used for bootstrapping. Can be \"shuffle_tracks\", 'shuffle_timepoints', 'shift_timepoints', 'shuffle_binary_blocks', 'shuffle_coordinates_timepoint or a list of methods, which will be applied in order of index. smoothK: Smoothing kernel size. biasK: Bias kernel size. peakThr: Threshold for peak detection. binThr: Threshold for binarization. polyDeg: Degree of the polynomial used for bias correction. biasMet: Bias correction method. Can be 'none', 'runmed', 'lm' eps: Epsilon parameter for DBSCAN. epsPrev: Parameter for linking tracks. If None, eps is used. minClsz: Minimum cluster size. nPrev: Number of previous frames to consider for linking. min_duration: Minimum duration of a track. min_size: Minimum size of a track. stats_metric: Metric to calculate. Can be \"duration\", \"total_size\", \"min_size\", \"max_size\" or a list of metrics. Default is [\"duration\", \"total_size\"]. pval_alternative: Alternative hypothesis for the p-value calculation. Can be \"less\" or \"greater\". finite_correction: Correct p-values for finite sampling. Default is True. n: Number of bootstraps. seed: Seed for the random number generator. allow_duplicates: If False, resampling will check if the resampled data contains duplicates. If True, duplicates will be allowed. max_tries: Maximum number of tries to resample data without duplicates. show_progress: Show a progress bar. verbose: Print additional information. Returns: DataFrame containing the bootstrapped data. \"\"\" if not isinstance ( stats_metric , list ): stats_metric = [ stats_metric ] for stats_m in stats_metric : if stats_m not in [ \"duration\" , \"total_size\" , \"min_size\" , \"max_size\" , ]: raise ValueError ( f \"Invalid metric: { stats_metric } \" ) if pval_alternative not in [ \"less\" , \"greater\" ]: raise ValueError ( f \"Invalid alternative hypothesis: { pval_alternative } \" ) clid_name = 'clid' if isinstance ( method , str ): print ( f 'Resampling data using method \" { method } \"...' ) elif isinstance ( method , list ): print ( f 'Resampling data using methods \" { method } \"...' ) df_resampled = resample_data ( data = df , posCols = posCols , frame_column = frame_column , id_column = id_column , meas_column = meas_column , method = method , n = n , seed = seed , allow_duplicates = allow_duplicates , max_tries = max_tries , show_progress = show_progress , verbose = verbose , paralell_processing = paralell_processing , ) iterations = df_resampled [ 'iteration' ] . unique () print ( f 'Running ARCOS and calculating \" { stats_metric } \"...' ) stats_df , stats_df_mean = calculate_arcos_stats ( df_resampled = df_resampled , posCols = posCols , frame_column = frame_column , id_column = id_column , meas_column = meas_column , smoothK = smoothK , biasK = biasK , peakThr = peakThr , binThr = binThr , polyDeg = polyDeg , biasMet = biasMet , eps = eps , epsPrev = epsPrev , minClsz = minClsz , nPrev = nPrev , min_duration = min_duration , min_size = min_size , stats_metric = stats_metric , show_progress = show_progress , paralell_processing = paralell_processing , clid_name = clid_name , iterations = iterations , ) df_p = calculate_pvalue ( stats_df_mean , stats_metric , pval_alternative , finite_correction , plot ) return stats_df , df_p calculate_arcos_stats ( df_resampled , iterations , posCols , frame_column , id_column , meas_column , smoothK = 3 , biasK = 51 , peakThr = 0.2 , binThr = 0.1 , polyDeg = 1 , biasMet = 'runmed' , eps = 2 , epsPrev = None , minClsz = 1 , nPrev = 1 , min_duration = 1 , min_size = 1 , stats_metric = [ 'duration' , 'total_size' ], show_progress = True , paralell_processing = True , clid_name = 'clid' ) \u00b6 Calculate the bootstrapped statistics. Parameters: Name Type Description Default df_resampled DataFrame Dataframe with resampled data. required iterations list [ int ] List of iteration names, or range. required posCols list List of position columns.. required frame_column str Name of the frame column. required id_column str Name of the id column. required meas_column str Name of the measurement column. required smoothK int Smoothing kernel size for local detrending. Defaults to 3. 3 biasK int Bias kernel size for large scale detrending (used with biasMet='runmed'). Defaults to 51. 51 peakThr float Peak threshold used for rescaling (used with biasMet='runmed'). Defaults to 0.2. 0.2 binThr float Threshold for binarizing measurements after detrending. Defaults to 0.1. 0.1 polyDeg int Polynomial degree used for detrending (used with biasMet='lm'). Defaults to 1. 1 biasMet str Bias method, can be 'none', 'runmed', 'lm'. Defaults to \"runmed\". 'runmed' eps float Epsilon used for culstering active entities. Defaults to 2. 2 epsPrev int Epsilon used for linking together culsters across time. Defaults to None. None minClsz int Minimum cluster size. Defaults to 1. 1 nPrev int Number of previous frames to consider when tracking clusters. Defaults to 1. 1 min_duration int Minimum duration of detected event. Defaults to 1. 1 min_size int Minimum size, minimum size of detected event. Defaults to 1. 1 stats_metric list [ str ] List of metrics to calculate. Defaults to ['duration', 'total_size']. ['duration', 'total_size'] show_progress bool Show progress bar. Defaults to True. True paralell_processing bool Use paralell processing, uses the joblib package. Defaults to True. True clid_name str Name of the cluster id column. Defaults to 'clid'. 'clid' Returns: Name Type Description DataFrame pd . DataFrame Dataframe with the bootstrapped statistics. DataFrame pd . DataFrame Dataframe with mean statistics. Source code in arcos4py/validation/_bootstrapping.py 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 def calculate_arcos_stats ( df_resampled : pd . DataFrame , iterations : list [ int ], posCols : list , frame_column : str , id_column : str , meas_column : str , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , binThr : float = 0.1 , polyDeg : int = 1 , biasMet : str = \"runmed\" , eps : float = 2 , epsPrev : int | None = None , minClsz : int = 1 , nPrev : int = 1 , min_duration : int = 1 , min_size : int = 1 , stats_metric : list [ str ] = [ 'duration' , 'total_size' ], show_progress : bool = True , paralell_processing : bool = True , clid_name : str = 'clid' , ): \"\"\"Calculate the bootstrapped statistics. Arguments: df_resampled (DataFrame): Dataframe with resampled data. iterations (list[int]): List of iteration names, or range. posCols (list): List of position columns.. frame_column (str): Name of the frame column. id_column (str): Name of the id column. meas_column (str): Name of the measurement column. smoothK (int, optional): Smoothing kernel size for local detrending. Defaults to 3. biasK (int, optional): Bias kernel size for large scale detrending (used with biasMet='runmed'). Defaults to 51. peakThr (float, optional): Peak threshold used for rescaling (used with biasMet='runmed'). Defaults to 0.2. binThr (float, optional): Threshold for binarizing measurements after detrending. Defaults to 0.1. polyDeg (int, optional): Polynomial degree used for detrending (used with biasMet='lm'). Defaults to 1. biasMet (str, optional): Bias method, can be 'none', 'runmed', 'lm'. Defaults to \"runmed\". eps (float, optional): Epsilon used for culstering active entities. Defaults to 2. epsPrev (int, optional): Epsilon used for linking together culsters across time. Defaults to None. minClsz (int, optional): Minimum cluster size. Defaults to 1. nPrev (int, optional): Number of previous frames to consider when tracking clusters. Defaults to 1. min_duration (int, optional): Minimum duration of detected event. Defaults to 1. min_size (int, optional): Minimum size, minimum size of detected event. Defaults to 1. stats_metric (list[str], optional): List of metrics to calculate. Defaults to ['duration', 'total_size']. show_progress (bool, optional): Show progress bar. Defaults to True. paralell_processing (bool, optional): Use paralell processing, uses the joblib package. Defaults to True. clid_name (str, optional): Name of the cluster id column. Defaults to 'clid'. Returns: DataFrame (pd.DataFrame): Dataframe with the bootstrapped statistics. DataFrame (pd.DataFrame): Dataframe with mean statistics. \"\"\" if paralell_processing : from joblib import Parallel , delayed stats_df_list = Parallel ( n_jobs =- 1 )( delayed ( _apply_arcos )( i_iter = i_iter , df_resampled = df_resampled , posCols = posCols , frame_column = frame_column , id_column = id_column , meas_column = meas_column , smoothK = smoothK , biasK = biasK , peakThr = peakThr , binThr = binThr , polyDeg = polyDeg , biasMet = biasMet , eps = eps , epsPrev = epsPrev , minClsz = minClsz , nPrev = nPrev , min_dur = min_duration , min_size = min_size , clid_name = clid_name , ) for i_iter in tqdm ( iterations , disable = not show_progress ) ) else : stats_df_list = [] for i_iter in tqdm ( iterations , disable = not show_progress ): stats_df = _apply_arcos ( i_iter = i_iter , df_resampled = df_resampled , posCols = posCols , frame_column = frame_column , id_column = id_column , meas_column = meas_column , smoothK = smoothK , biasK = biasK , peakThr = peakThr , binThr = binThr , polyDeg = polyDeg , biasMet = biasMet , eps = eps , epsPrev = epsPrev , minClsz = minClsz , nPrev = nPrev , min_dur = min_duration , min_size = min_size , clid_name = clid_name , ) stats_df_list . append ( stats_df ) stats_df = pd . concat ( stats_df_list , ignore_index = True ) stats_df_indexer = [ 'bootstrap_iteration' ] + stats_metric stats_df_mean : pd . DataFrame = ( stats_df [ stats_df_indexer ] . groupby ([ 'bootstrap_iteration' ]) . agg ([ 'mean' ]) . reset_index () ) stats_df_mean = stats_df_mean . droplevel ( level = 1 , axis = 1 ) # for bootstrap iteratoins that did not detect any events, set the metric to 0 stats_df_mean [ stats_metric ] = stats_df_mean [ stats_metric ] . fillna ( 0 ) return stats_df , stats_df_mean calculate_pvalue ( stats_df_mean , stats_metric , pval_alternative , finite_correction , plot , ** plot_kwargs ) \u00b6 Calculates the p-value with the given alternative hypothesis. Parameters: Name Type Description Default stats_df_mean DataFrame DataFrame containing the bootstrapped data. required stats_metric str | list [ str ] Metric to calculate. Can be \"duration\", \"total_size\", \"min_size\", \"max_size\" or a list of metrics. Default is [\"duration\", \"total_size\"]. required pval_alternative str Alternative hypothesis for the p-value calculation. Can be \"less\", \"greater\" or both which will return p values for both alternatives. required finite_correction bool Correct p-values for finite sampling. Default is True. required plot bool Plot the distribution of the bootstrapped data. required Returns: Name Type Description DataFrame pd . DataFrame containing the p-values. Source code in arcos4py/validation/_bootstrapping.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def calculate_pvalue ( stats_df_mean : pd . DataFrame , stats_metric : str | list [ str ], pval_alternative : str , finite_correction : bool , plot : bool , ** plot_kwargs , ): \"\"\"Calculates the p-value with the given alternative hypothesis. Arguments: stats_df_mean (DataFrame): DataFrame containing the bootstrapped data. stats_metric (str | list[str]): Metric to calculate. Can be \"duration\", \"total_size\", \"min_size\", \"max_size\" or a list of metrics. Default is [\"duration\", \"total_size\"]. pval_alternative (str): Alternative hypothesis for the p-value calculation. Can be \"less\", \"greater\" or both which will return p values for both alternatives. finite_correction (bool): Correct p-values for finite sampling. Default is True. plot (bool): Plot the distribution of the bootstrapped data. Returns: DataFrame (pd.DataFrame): containing the p-values. \"\"\" if finite_correction : pval = stats_df_mean [ stats_metric ] . agg ( lambda x : _p_val_finite_sampling ( x , pval_alternative )) else : pval = stats_df_mean [[ 'total_size' , 'duration' ]] . agg ( lambda x : _p_val_infinite_sampling ( x , pval_alternative )) pval . name = 'p_value' if isinstance ( stats_metric , list ): _stats_metric = stats_metric else : _stats_metric = [ stats_metric ] if plot : fig , axis = plt . subplots ( 1 , len ( _stats_metric )) try : iter ( axis ) except TypeError : axis = [ axis ] for ax , stats_col in zip ( axis , _stats_metric ): # sns.kdeplot(stats_df_mean[stats_col], ax=ax, shade=True, sharey=True) sns . histplot ( stats_df_mean [ stats_col ], ax = ax , kde = True , stat = 'density' , common_norm = False , ** plot_kwargs ) # ax.hist(stats_df_mean[stats_col], alpha=0.5) ax . set_title ( stats_col ) ax . vlines ( stats_df_mean [ stats_col ] . iloc [ 0 ], ymin = 0 , ymax = ax . get_ylim ()[ 1 ], color = 'red' , ls = '--' ) ax . set_xlabel ( 'Value' ) if len ( axis ) > 1 and ax . is_first_col (): ax . set_ylabel ( 'Density' ) else : ax . set_ylabel ( '' ) x_pos = ax . get_xlim ()[ 0 ] + (( ax . get_xlim ()[ 1 ] - ax . get_xlim ()[ 0 ]) * 0.8 ) y_pos = ax . get_ylim ()[ 0 ] + (( ax . get_ylim ()[ 1 ] - ax . get_ylim ()[ 0 ]) * 0.8 ) ax . text ( x_pos , y_pos , f 'p-value: \\n { pval . loc [:, stats_col ] . round ( 3 ) . to_string () } ' , ha = 'center' , va = 'center' , color = 'red' , ) fig . suptitle ( 'Bootstrapped metrics' ) plt . show () return pval resample_data ( data , posCols , frame_column , id_column , meas_column = None , method = 'shuffle_tracks' , n = 100 , seed = 42 , allow_duplicates = False , max_tries = 100 , show_progress = True , verbose = False , paralell_processing = True ) \u00b6 Resamples data in order to perform bootstrapping analysis. Parameters: Name Type Description Default data pd . Dataframe The data to resample. required posCols list The columns to use for the position. required frame_column str The column to use for the frame. required id_column str The column to use for the object ID. required meas_column str The column to use for the measurement. Only needed for 'activity_blocks_shuffle'. Defaults to None. None method str The method to use for resampling. Defaults to 'shuffle_tracks'. Available methods are: \"shuffle_tracks\", 'shuffle_timepoints', 'shift_timepoints', 'shuffle_binary_blocks', 'shuffle_coordinates_timepoint' 'shuffle_tracks' n int The number of resample iterations. Defaults to 100. 100 seed int The random seed. Defaults to 42. 42 allow_duplicates bool Whether to allow resampling to randomly generate the same data twice. Defaults to False. False max_tries int The maximum number of tries to try ot generate unique data when allow_duplicates is set to True. Defaults to 100. 100 verbose bool Whether to print progress. Defaults to False. False Returns: Type Description pd . DataFrame pd.DataFrame: The resampled data. Source code in arcos4py/validation/_resampling.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 def resample_data ( # noqa: C901 data : pd . DataFrame , posCols : list , frame_column : str , id_column : str , meas_column : Union [ str , None ] = None , method : Union [ str , list [ str ]] = 'shuffle_tracks' , n = 100 , seed = 42 , allow_duplicates = False , max_tries = 100 , show_progress = True , verbose = False , paralell_processing = True , ) -> pd . DataFrame : \"\"\"Resamples data in order to perform bootstrapping analysis. Arguments: data (pd.Dataframe): The data to resample. posCols (list): The columns to use for the position. frame_column (str): The column to use for the frame. id_column (str): The column to use for the object ID. meas_column (str, optional): The column to use for the measurement. Only needed for 'activity_blocks_shuffle'. Defaults to None. method (str, optional): The method to use for resampling. Defaults to 'shuffle_tracks'. Available methods are: \"shuffle_tracks\", 'shuffle_timepoints', 'shift_timepoints', 'shuffle_binary_blocks', 'shuffle_coordinates_timepoint' n (int, optional): The number of resample iterations. Defaults to 100. seed (int, optional): The random seed. Defaults to 42. allow_duplicates (bool, optional): Whether to allow resampling to randomly generate the same data twice. Defaults to False. max_tries (int, optional): The maximum number of tries to try ot generate unique data when allow_duplicates is set to True. Defaults to 100. verbose (bool, optional): Whether to print progress. Defaults to False. Returns: pd.DataFrame: The resampled data. \"\"\" # validate the input if not isinstance ( data , pd . DataFrame ): raise TypeError ( 'data must be a pandas.DataFrame' ) if not isinstance ( posCols , list ): raise TypeError ( 'posCols must be a list' ) if not isinstance ( frame_column , str ): raise TypeError ( 'frame_column must be a string' ) if not isinstance ( id_column , str ): raise TypeError ( 'id_column must be a string' ) if not isinstance ( meas_column , str ) and meas_column is not None : raise TypeError ( 'meas_column must be a string or None' ) if not isinstance ( method , str ) and not isinstance ( method , list ): raise TypeError ( 'method must be a string or list' ) if not isinstance ( n , int ): raise TypeError ( 'n must be a positive integer' ) if not isinstance ( seed , int ): raise TypeError ( 'seed must be an integer' ) if not isinstance ( verbose , bool ): raise TypeError ( 'verbose must be a boolean' ) if not isinstance ( paralell_processing , bool ): raise TypeError ( 'paralell_processing must be a boolean' ) if len ( posCols ) < 1 : raise ValueError ( 'posCols must contain at least one column' ) if n < 1 : raise ValueError ( 'n must be a positive integer' ) if seed < 0 : raise ValueError ( 'seed must be a positive integer' ) method_dict : dict [ str , Callable ] = { 'shuffle_tracks' : shuffle_tracks , 'shuffle_timepoints' : shuffle_timepoints , 'shift_timepoints' : shift_timepoints_per_trajectory , 'shuffle_binary_blocks' : shuffle_activity_bocks_per_trajectory , 'shuffle_coordinates_timepoint' : shuffle_coordinates_per_timepoint , } function_args : dict [ str , tuple ] = { 'shuffle_tracks' : ( id_column , posCols , frame_column ), 'shuffle_timepoints' : ( id_column , frame_column ), 'shift_timepoints' : ( id_column , frame_column ), 'shuffle_binary_blocks' : ( id_column , frame_column , meas_column ), 'shuffle_coordinates_timepoint' : ( posCols , frame_column ), } resampling_func_list = [] # convert method to list if necessary if isinstance ( method , str ): methods = [ method ] else : methods = method # Check if the method is valid for method in methods : if method not in method_dict . keys (): raise ValueError ( f 'method must be one of { method_dict . keys () } ' ) if method == 'shuffle_binary_blocks' and meas_column is None : raise ValueError ( 'meas_column must be set for binary_blocks_shuffle' ) # Check if the columns are in the data if 'shuffle_binary_blocks' in methods : relevant_columns = posCols + [ frame_column , id_column , meas_column ] else : relevant_columns = posCols + [ frame_column , id_column ] for i in relevant_columns : if i not in data . columns : raise ValueError ( f ' { i } not in df.columns' ) # check if there are any Nan in the columns selected na_cols = [] for i in relevant_columns : if data [ posCols ] . isnull () . values . any (): na_cols . append ( i ) if na_cols : warnings . warn ( f 'NaN values in { na_cols } , default behaviour is to drop these rows' ) data . dropna ( subset = na_cols , inplace = True ) # Sort the data data . sort_values ([ id_column , frame_column ], inplace = True ) rng = np . random . default_rng ( seed ) # create a list of random numbers between 0 and 1000000 seed_list = rng . integers ( 1_000_000_000 , size = n ) df_out : list [ pd . DataFrame ] = [] # shuffle xy position for each object if verbose : print ( f 'Resampling for each object { n } times' ) # create a list of functions to call for method in methods : resampling_func_list . append ( method_dict [ method ]) iter_range = range ( 1 , n + 1 ) if paralell_processing : from joblib import Parallel , delayed # iterate over the number of resamples df_out = Parallel ( n_jobs =- 1 )( delayed ( _apply_resampling )( iter_number = i , data = data , methods = methods , resampling_func_list = resampling_func_list , seed_list = seed_list , function_args = function_args , ) for i in tqdm ( iter_range , disable = not show_progress ) ) else : # iterate over the number of resamples for i in tqdm ( iter_range , disable = not show_progress ): data_new = _apply_resampling ( iter_number = i , data = data , methods = methods , resampling_func_list = resampling_func_list , seed_list = seed_list , function_args = function_args , ) if not allow_duplicates : current_try = 0 # make sure that data_new is not already in df_out, # but they are both dataframes, else redo the resampling while any ( data_new . loc [:, data_new . columns != 'iteration' ] . equals ( i . loc [:, i . columns != 'iteration' ]) for i in df_out ): current_try += 1 data_new = _apply_resampling ( iter_number = i , data = data , methods = methods , resampling_func_list = resampling_func_list , seed_list = seed_list , function_args = function_args , ) if current_try > max_tries : raise ValueError ( 'Could not find a unique resampling after 100 tries, try increasing n or allow_duplicates' ) df_out . append ( data_new ) data_it0 = data . copy () data_it0 [ 'iteration' ] = np . repeat ( 0 , len ( data_it0 )) df_out . insert ( 0 , data_it0 ) return pd . concat ( df_out )[ data . columns . tolist () + [ 'iteration' ]]","title":"Modules"},{"location":"api/#arcos4py.ARCOS","text":"Detects and tracks collective events in a tracked time-series dataset. Requires binarized measurement column, that can be generated with the bin_measurements method. Tracking makes use of the dbscan algorithm, which is applied to every frame and subsequently connects collective events between frames located within eps distance of each other. Attributes: Name Type Description data DataFrame Data of tracked time-series in \"long format\". Can be used to acess modified dataframe at any point. posCols list List containing position column names strings inside data e.g. At least one dimension is required. frame_column str Indicating the frame column in input_data. id_column str Indicating the track id/id column in input_data. measurement_column str Indicating the measurement column in input_data. clid_column str Indicating the column name containing the collective event ids. bin_col str | None Name of the binary column. This is generated based on the name of the measurement_column after binarization. Optionally can be set in order to provide a already binarized column to skip ARCOS binarization. Source code in arcos4py/_arcos4py.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 class ARCOS : \"\"\"Detects and tracks collective events in a tracked time-series dataset. Requires binarized measurement column, that can be generated with the bin_measurements method. Tracking makes use of the dbscan algorithm, which is applied to every frame and subsequently connects collective events between frames located within eps distance of each other. Attributes: data (DataFrame): Data of tracked time-series in \"long format\". Can be used to acess modified dataframe at any point. posCols (list): List containing position column names strings inside data e.g. At least one dimension is required. frame_column (str): Indicating the frame column in input_data. id_column (str): Indicating the track id/id column in input_data. measurement_column (str): Indicating the measurement column in input_data. clid_column (str): Indicating the column name containing the collective event ids. bin_col (str | None): Name of the binary column. This is generated based on the name of the measurement_column after binarization. Optionally can be set in order to provide a already binarized column to skip ARCOS binarization. \"\"\" def __init__ ( self , data : pd . DataFrame , posCols : list = [ \"x\" ], frame_column : str = 'time' , id_column : str | None = 'id' , measurement_column : str = 'meas' , clid_column : str = 'clTrackID' , n_jobs : int = 1 , ) -> None : \"\"\"Constructs class with provided arguments. Arguments: data (DataFrame): Input Data of tracked time-series in \"long format\" containing position columns, a measurement and an object ID column. posCols (list): List ontaining position column names strings inside data e.g. At least one dimension is required. frame_column (str): Indicating the frame column in input_data. id_column (str): Indicating the track id/object id column in input_data. If None, the data is assumed to not have a tracking column. Binarization can only be performed without detrending. measurement_column (str): Indicating the measurement column in input_data. clid_column (str): Indicating the column name containing the collective event ids. n_jobs (str): Number of workers to spawn, -1 uses all available cpus. \"\"\" self . data = data self . posCols = posCols self . frame_column = frame_column self . id_column = id_column self . measurement_column = measurement_column self . clid_column = clid_column self . n_jobs = n_jobs self . bin_col : Union [ str , None ] = None # to check if no measurement was provided assign None if self . id_column is None : self . data = self . data . sort_values ( by = [ self . frame_column ]) else : self . data = self . data . sort_values ( by = [ self . frame_column , self . id_column ]) self . _check_col () if self . measurement_column is not None : self . resc_col = f \" { self . measurement_column } .resc\" self . bin_col = f \" { self . measurement_column } .bin\" def __repr__ ( self ) -> pd . DataFrame : \"\"\"Set __repr__ to return self.data.\"\"\" return repr ( self . data ) def _check_col ( self ): \"\"\"Checks that self.cols contains all required columns.\"\"\" columns = self . data . columns input_columns = [ self . frame_column , self . id_column , self . id_column , self . measurement_column ] input_columns = [ col for col in input_columns if col is not None ] if not all ( item in columns for item in input_columns ): raise ValueError ( f \"Columns { input_columns } do not match with column in dataframe.\" ) def interpolate_measurements ( self ) -> pd . DataFrame : \"\"\"Interpolates NaN's in place in measurement column. Returns: Dataframe with interpolated measurement column. \"\"\" meas_interp = interpolation ( self . data ) . interpolate () self . data = meas_interp return self . data def clip_meas ( self , clip_low : float = 0.001 , clip_high : float = 0.999 ) -> pd . DataFrame : \"\"\"Clip measurement column to upper and lower quantiles defined in clip_low and clip_high. Arguments: clip_low (float): Lower clipping boundary (quantile). clip_high (float): Upper clipping boundary (quantile). Returns: Dataframe with in place clipped measurement column. \"\"\" meas_column = self . data [ self . measurement_column ] . to_numpy () meas_clipped = clipMeas ( meas_column ) . clip ( clip_low , clip_high ) self . data [ self . measurement_column ] = meas_clipped return self . data def bin_measurements ( self , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , binThr : float = 0.1 , polyDeg : int = 1 , biasMet : str = \"runmed\" , ) -> pd . DataFrame : r \"\"\"Smooth, de-trend, and binarise the input data. First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold Arguments: smoothK (int): Size of the short-term median smoothing filter. biasK (int): Size of the long-term de-trending median filter peakThr (float): Threshold for rescaling of the de-trended signal. binThr (float): Threshold for binary classification. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): De-trending method, one of ['runmed', 'lm', 'none']. If no id_column is provided, only 'none' is allowed. Returns: DataFrame with detrended/smoothed and binarized measurement column. \"\"\" self . data = binData ( smoothK , biasK , peakThr , binThr , polyDeg , biasMet , n_jobs = self . n_jobs , ) . run ( self . data , colMeas = self . measurement_column , colGroup = self . id_column , colFrame = self . frame_column ) return self . data def trackCollev ( self , eps : float = 1 , epsPrev : Union [ int , None ] = None , minClsz : int = 1 , nPrev : int = 1 , clusteringMethod : str = \"dbscan\" , linkingMethod : str = \"nearest\" , minSamples : int | None = None , ) -> pd . DataFrame : \"\"\"Requires binarized measurement column. Makes use of the dbscan algorithm, applies this to every timeframe and subsequently connects collective events between frames located within eps distance of each other. Arguments: eps (float): The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. epsPrev (float | None): Frame to frame distance, value is used to connect collective events across multiple frames.If \"None\", same value as eps is used. minClsz (str): The minimum size for a cluster to be identified as a collective event nPrev (int): Number of previous frames the tracking algorithm looks back to connect collective events clusteringMethod (str): Clustering method, one of ['dbscan', 'hdbscan']. minSamples (int | None): The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. linkingMethod (str): Linking method, one of ['nearest', 'transportation']. Returns: DataFrame with detected collective events across time. \"\"\" data_events = track_events_dataframe ( X = self . data , coordinates_column = self . posCols , frame_column = self . frame_column , id_column = self . id_column , bin_meas_column = self . bin_col , eps = eps , epsPrev = epsPrev , minClSz = minClsz , nPrev = nPrev , collid_column = self . clid_column , linkingMethod = linkingMethod , clusteringMethod = clusteringMethod , minSamples = minSamples , nJobs = self . n_jobs , ) return data_events","title":"ARCOS"},{"location":"api/#arcos4py._arcos4py.ARCOS.__init__","text":"Constructs class with provided arguments. Parameters: Name Type Description Default data DataFrame Input Data of tracked time-series in \"long format\" containing position columns, a measurement and an object ID column. required posCols list List ontaining position column names strings inside data e.g. At least one dimension is required. ['x'] frame_column str Indicating the frame column in input_data. 'time' id_column str Indicating the track id/object id column in input_data. If None, the data is assumed to not have a tracking column. Binarization can only be performed without detrending. 'id' measurement_column str Indicating the measurement column in input_data. 'meas' clid_column str Indicating the column name containing the collective event ids. 'clTrackID' n_jobs str Number of workers to spawn, -1 uses all available cpus. 1 Source code in arcos4py/_arcos4py.py 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __init__ ( self , data : pd . DataFrame , posCols : list = [ \"x\" ], frame_column : str = 'time' , id_column : str | None = 'id' , measurement_column : str = 'meas' , clid_column : str = 'clTrackID' , n_jobs : int = 1 , ) -> None : \"\"\"Constructs class with provided arguments. Arguments: data (DataFrame): Input Data of tracked time-series in \"long format\" containing position columns, a measurement and an object ID column. posCols (list): List ontaining position column names strings inside data e.g. At least one dimension is required. frame_column (str): Indicating the frame column in input_data. id_column (str): Indicating the track id/object id column in input_data. If None, the data is assumed to not have a tracking column. Binarization can only be performed without detrending. measurement_column (str): Indicating the measurement column in input_data. clid_column (str): Indicating the column name containing the collective event ids. n_jobs (str): Number of workers to spawn, -1 uses all available cpus. \"\"\" self . data = data self . posCols = posCols self . frame_column = frame_column self . id_column = id_column self . measurement_column = measurement_column self . clid_column = clid_column self . n_jobs = n_jobs self . bin_col : Union [ str , None ] = None # to check if no measurement was provided assign None if self . id_column is None : self . data = self . data . sort_values ( by = [ self . frame_column ]) else : self . data = self . data . sort_values ( by = [ self . frame_column , self . id_column ]) self . _check_col () if self . measurement_column is not None : self . resc_col = f \" { self . measurement_column } .resc\" self . bin_col = f \" { self . measurement_column } .bin\"","title":"__init__()"},{"location":"api/#arcos4py._arcos4py.ARCOS.__repr__","text":"Set repr to return self.data. Source code in arcos4py/_arcos4py.py 97 98 99 def __repr__ ( self ) -> pd . DataFrame : \"\"\"Set __repr__ to return self.data.\"\"\" return repr ( self . data )","title":"__repr__()"},{"location":"api/#arcos4py._arcos4py.ARCOS.bin_measurements","text":"Smooth, de-trend, and binarise the input data. First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold Parameters: Name Type Description Default smoothK int Size of the short-term median smoothing filter. 3 biasK int Size of the long-term de-trending median filter 51 peakThr float Threshold for rescaling of the de-trended signal. 0.2 binThr float Threshold for binary classification. 0.1 polyDeg int Sets the degree of the polynomial for lm fitting. 1 biasMet str De-trending method, one of ['runmed', 'lm', 'none']. If no id_column is provided, only 'none' is allowed. 'runmed' Returns: Type Description pd . DataFrame DataFrame with detrended/smoothed and binarized measurement column. Source code in arcos4py/_arcos4py.py 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 def bin_measurements ( self , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , binThr : float = 0.1 , polyDeg : int = 1 , biasMet : str = \"runmed\" , ) -> pd . DataFrame : r \"\"\"Smooth, de-trend, and binarise the input data. First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold Arguments: smoothK (int): Size of the short-term median smoothing filter. biasK (int): Size of the long-term de-trending median filter peakThr (float): Threshold for rescaling of the de-trended signal. binThr (float): Threshold for binary classification. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): De-trending method, one of ['runmed', 'lm', 'none']. If no id_column is provided, only 'none' is allowed. Returns: DataFrame with detrended/smoothed and binarized measurement column. \"\"\" self . data = binData ( smoothK , biasK , peakThr , binThr , polyDeg , biasMet , n_jobs = self . n_jobs , ) . run ( self . data , colMeas = self . measurement_column , colGroup = self . id_column , colFrame = self . frame_column ) return self . data","title":"bin_measurements()"},{"location":"api/#arcos4py._arcos4py.ARCOS.clip_meas","text":"Clip measurement column to upper and lower quantiles defined in clip_low and clip_high. Parameters: Name Type Description Default clip_low float Lower clipping boundary (quantile). 0.001 clip_high float Upper clipping boundary (quantile). 0.999 Returns: Type Description pd . DataFrame Dataframe with in place clipped measurement column. Source code in arcos4py/_arcos4py.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 def clip_meas ( self , clip_low : float = 0.001 , clip_high : float = 0.999 ) -> pd . DataFrame : \"\"\"Clip measurement column to upper and lower quantiles defined in clip_low and clip_high. Arguments: clip_low (float): Lower clipping boundary (quantile). clip_high (float): Upper clipping boundary (quantile). Returns: Dataframe with in place clipped measurement column. \"\"\" meas_column = self . data [ self . measurement_column ] . to_numpy () meas_clipped = clipMeas ( meas_column ) . clip ( clip_low , clip_high ) self . data [ self . measurement_column ] = meas_clipped return self . data","title":"clip_meas()"},{"location":"api/#arcos4py._arcos4py.ARCOS.interpolate_measurements","text":"Interpolates NaN's in place in measurement column. Returns: Type Description pd . DataFrame Dataframe with interpolated measurement column. Source code in arcos4py/_arcos4py.py 109 110 111 112 113 114 115 116 117 def interpolate_measurements ( self ) -> pd . DataFrame : \"\"\"Interpolates NaN's in place in measurement column. Returns: Dataframe with interpolated measurement column. \"\"\" meas_interp = interpolation ( self . data ) . interpolate () self . data = meas_interp return self . data","title":"interpolate_measurements()"},{"location":"api/#arcos4py._arcos4py.ARCOS.trackCollev","text":"Requires binarized measurement column. Makes use of the dbscan algorithm, applies this to every timeframe and subsequently connects collective events between frames located within eps distance of each other. Parameters: Name Type Description Default eps float The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. 1 epsPrev float | None Frame to frame distance, value is used to connect collective events across multiple frames.If \"None\", same value as eps is used. None minClsz str The minimum size for a cluster to be identified as a collective event 1 nPrev int Number of previous frames the tracking algorithm looks back to connect collective events 1 clusteringMethod str Clustering method, one of ['dbscan', 'hdbscan']. 'dbscan' minSamples int | None The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. None linkingMethod str Linking method, one of ['nearest', 'transportation']. 'nearest' Returns: Type Description pd . DataFrame DataFrame with detected collective events across time. Source code in arcos4py/_arcos4py.py 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def trackCollev ( self , eps : float = 1 , epsPrev : Union [ int , None ] = None , minClsz : int = 1 , nPrev : int = 1 , clusteringMethod : str = \"dbscan\" , linkingMethod : str = \"nearest\" , minSamples : int | None = None , ) -> pd . DataFrame : \"\"\"Requires binarized measurement column. Makes use of the dbscan algorithm, applies this to every timeframe and subsequently connects collective events between frames located within eps distance of each other. Arguments: eps (float): The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. epsPrev (float | None): Frame to frame distance, value is used to connect collective events across multiple frames.If \"None\", same value as eps is used. minClsz (str): The minimum size for a cluster to be identified as a collective event nPrev (int): Number of previous frames the tracking algorithm looks back to connect collective events clusteringMethod (str): Clustering method, one of ['dbscan', 'hdbscan']. minSamples (int | None): The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. linkingMethod (str): Linking method, one of ['nearest', 'transportation']. Returns: DataFrame with detected collective events across time. \"\"\" data_events = track_events_dataframe ( X = self . data , coordinates_column = self . posCols , frame_column = self . frame_column , id_column = self . id_column , bin_meas_column = self . bin_col , eps = eps , epsPrev = epsPrev , minClSz = minClsz , nPrev = nPrev , collid_column = self . clid_column , linkingMethod = linkingMethod , clusteringMethod = clusteringMethod , minSamples = minSamples , nJobs = self . n_jobs , ) return data_events","title":"trackCollev()"},{"location":"api/#arcos4py.plotting","text":"Tools for plotting collective events.","title":"plotting"},{"location":"api/#arcos4py.plotting.NoodlePlot","text":"Create Noodle Plot of cell tracks, colored by collective event id. Attributes: Name Type Description df pd . DataFrame DataFrame containing collective events from arcos. colev str Name of the collective event column in df. trackid str Name of the track column in df. frame str Name of the frame column in df. posx str Name of the X coordinate column in df. posy str Name of the Y coordinate column in df. posz str Name of the Z coordinate column in df, or None if no z column. Source code in arcos4py/plotting/_plotting.py 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 class NoodlePlot : \"\"\"Create Noodle Plot of cell tracks, colored by collective event id. Attributes: df (pd.DataFrame): DataFrame containing collective events from arcos. colev (str): Name of the collective event column in df. trackid (str): Name of the track column in df. frame (str): Name of the frame column in df. posx (str): Name of the X coordinate column in df. posy (str): Name of the Y coordinate column in df. posz (str): Name of the Z coordinate column in df, or None if no z column. \"\"\" def __init__ ( self , df : pd . DataFrame , colev : str , trackid : str , frame : str , posx : str , posy : str , posz : Union [ str , None ] = None , ): \"\"\"Constructs class with given parameters. Arguments: df (pd.DataFrame): DataFrame containing collective events from arcos. colev (str): Name of the collective event column in df. trackid (str): Name of the track column in df. frame (str): Name of the frame column in df. posx (str): Name of the X coordinate column in df. posy (str): Name of the Y coordinate column in df. posz (str | None): Name of the Z coordinate column in df, or None if no z column. \"\"\" self . df = df self . colev = colev self . trackid = trackid self . frame = frame self . posx = posx self . posy = posy self . posz = posz def _prepare_data_noodleplot ( self , df : pd . DataFrame , color_cylce : list [ str ], colev : str , trackid : str , frame : str , posx : str , posy : str , posz : Union [ str , None ] = None , ) -> tuple [ list [ np . ndarray ], np . ndarray ]: \"\"\"From arcos collective event data,\\ generates a list of numpy arrays, one for each event. Arguments: df (pd.DataFrame): DataFrame containing collective events from arcos. color_cylce (list[str]): list of colors used to color trackid's for individual collective events. colev (str): Name of the collective event column in df. trackid (str): Name of the track column in df. frame: (str): Name of the frame column in df. posx (str): Name of the X coordinate column in df. posy (str): Name of the Y coordinate column in df. posz (str): Name of the Z coordinate column in df, or None if no z column. Returns: grouped_array (list[np.ndarray]): List of collective events data colors (np.ndarray): colors for each collective event. \"\"\" # values need to be sorted to group with numpy df = df . sort_values ([ colev , trackid ]) if posz : array = df [[ colev , trackid , frame , posx , posy , posz ]] . to_numpy () else : array = df [[ colev , trackid , frame , posx , posy ]] . to_numpy () # generate goroups for each unique value grouped_array = np . split ( array , np . unique ( array [:, 0 ], axis = 0 , return_index = True )[ 1 ][ 1 :]) # make collids sequential seq_colids = np . concatenate ( [ np . repeat ( i , value . shape [ 0 ]) for i , value in enumerate ( grouped_array )], axis = 0 , ) array_seq_colids = np . column_stack (( array , seq_colids )) # split sequential collids array by trackid and collid grouped_array = np . split ( array_seq_colids , np . unique ( array_seq_colids [:, : 2 ], axis = 0 , return_index = True )[ 1 ][ 1 :], ) # generate colors for each collective event, wrap arround the color cycle colors = np . take ( np . array ( color_cylce ), [ i + 1 for i in np . unique ( seq_colids )], mode = \"wrap\" ) return grouped_array , colors def _create_noodle_plot ( self , grouped_data : list [ np . ndarray ], colors : np . ndarray ): \"\"\"Plots the noodle plot.\"\"\" fig , ax = plt . subplots () ax . set_xlabel ( \"Time Point\" ) ax . set_ylabel ( \"Position\" ) for dat in grouped_data : ax . plot ( dat [:, 2 ], dat [:, self . projection_index ], c = colors [ int ( dat [ 0 , - 1 ])], ) return fig , ax def plot ( self , projection_axis : str , color_cylce : list [ str ] = TAB20 ): \"\"\"Create Noodle Plot of cell tracks, colored by collective event id. Arguments: projection_axis (str): Specify with witch coordinate the noodle plot should be drawn. Has to be one of the posx, posy or posz arguments passed in during the class instantiation process. color_cylce (list[str]): List of hex color values or string names (i.e. ['red', 'yellow']) used to color collecitve events. Cycles through list. Returns: fig (matplotlib.figure.Figure): Matplotlib figure object for the noodle plot. axes (matplotlib.axes.Axes): Matplotlib axes for the nooble plot. \"\"\" if projection_axis not in [ self . posx , self . posy , self . posz ]: raise ValueError ( f \"projection_axis has to be one of { [ self . posx , self . posy , self . posz ] } \" ) if projection_axis == self . posx : self . projection_index = 3 elif projection_axis == self . posy : self . projection_index = 4 elif projection_axis == self . posz : self . projection_index = 5 grpd_data , colors = self . _prepare_data_noodleplot ( self . df , color_cylce , self . colev , self . trackid , self . frame , self . posx , self . posy , self . posz ) fig , axes = self . _create_noodle_plot ( grpd_data , colors ) return fig , axes","title":"NoodlePlot"},{"location":"api/#arcos4py.plotting._plotting.NoodlePlot.__init__","text":"Constructs class with given parameters. Parameters: Name Type Description Default df pd . DataFrame DataFrame containing collective events from arcos. required colev str Name of the collective event column in df. required trackid str Name of the track column in df. required frame str Name of the frame column in df. required posx str Name of the X coordinate column in df. required posy str Name of the Y coordinate column in df. required posz str | None Name of the Z coordinate column in df, or None if no z column. None Source code in arcos4py/plotting/_plotting.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 def __init__ ( self , df : pd . DataFrame , colev : str , trackid : str , frame : str , posx : str , posy : str , posz : Union [ str , None ] = None , ): \"\"\"Constructs class with given parameters. Arguments: df (pd.DataFrame): DataFrame containing collective events from arcos. colev (str): Name of the collective event column in df. trackid (str): Name of the track column in df. frame (str): Name of the frame column in df. posx (str): Name of the X coordinate column in df. posy (str): Name of the Y coordinate column in df. posz (str | None): Name of the Z coordinate column in df, or None if no z column. \"\"\" self . df = df self . colev = colev self . trackid = trackid self . frame = frame self . posx = posx self . posy = posy self . posz = posz","title":"__init__()"},{"location":"api/#arcos4py.plotting._plotting.NoodlePlot.plot","text":"Create Noodle Plot of cell tracks, colored by collective event id. Parameters: Name Type Description Default projection_axis str Specify with witch coordinate the noodle plot should be drawn. Has to be one of the posx, posy or posz arguments passed in during the class instantiation process. required color_cylce list [ str ] List of hex color values or string names (i.e. ['red', 'yellow']) used to color collecitve events. Cycles through list. TAB20 Returns: Name Type Description fig matplotlib . figure . Figure Matplotlib figure object for the noodle plot. axes matplotlib . axes . Axes Matplotlib axes for the nooble plot. Source code in arcos4py/plotting/_plotting.py 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 def plot ( self , projection_axis : str , color_cylce : list [ str ] = TAB20 ): \"\"\"Create Noodle Plot of cell tracks, colored by collective event id. Arguments: projection_axis (str): Specify with witch coordinate the noodle plot should be drawn. Has to be one of the posx, posy or posz arguments passed in during the class instantiation process. color_cylce (list[str]): List of hex color values or string names (i.e. ['red', 'yellow']) used to color collecitve events. Cycles through list. Returns: fig (matplotlib.figure.Figure): Matplotlib figure object for the noodle plot. axes (matplotlib.axes.Axes): Matplotlib axes for the nooble plot. \"\"\" if projection_axis not in [ self . posx , self . posy , self . posz ]: raise ValueError ( f \"projection_axis has to be one of { [ self . posx , self . posy , self . posz ] } \" ) if projection_axis == self . posx : self . projection_index = 3 elif projection_axis == self . posy : self . projection_index = 4 elif projection_axis == self . posz : self . projection_index = 5 grpd_data , colors = self . _prepare_data_noodleplot ( self . df , color_cylce , self . colev , self . trackid , self . frame , self . posx , self . posy , self . posz ) fig , axes = self . _create_noodle_plot ( grpd_data , colors ) return fig , axes","title":"plot()"},{"location":"api/#arcos4py.plotting.dataPlots","text":"Plot different metrics of input data. Attributes: Name Type Description data Dataframe containing ARCOS data. frame str name of frame column in data. measurement str name of measurement column in data. id str name of track id column. Source code in arcos4py/plotting/_plotting.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 class dataPlots : \"\"\"Plot different metrics of input data. Attributes: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. id (str): name of track id column. \"\"\" def __init__ ( self , data : pd . DataFrame , frame : str , measurement : str , id : str ): \"\"\"Plot different metrics such as histogram, position-t and density. Arguments: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. id (str): name of track id column. \"\"\" self . data = data self . id = id self . frame = frame self . measurement = measurement def position_t_plot ( self , posCol : set [ str ] = { 'x' }, n : int = 20 ) -> Union [ plt . Axes , plt . Figure ]: \"\"\"Plots X and Y over T to visualize tracklength. Arguments: posCol (set): containing names of position columns in data. n (int): number of samples to plot. Returns: fig (matplotlib.figure.Figure): Matplotlib figure object of density plot. axes (matplotlib.axes.Axes): Matplotlib axes of density plot. \"\"\" sample = pd . Series ( self . data [ self . id ] . unique ()) . sample ( n ) pd_from_r_df = self . data . loc [ self . data [ self . id ] . isin ( sample )] fig , axes = plt . subplots ( 1 , len ( posCol ), figsize = ( 6 , 3 )) for label , df in pd_from_r_df . groupby ( self . id ): for index , value in enumerate ( posCol ): if len ( posCol ) > 1 : df . plot ( x = self . frame , y = value , ax = axes [ index ], legend = None ) else : df . plot ( x = self . frame , y = value , ax = axes , legend = None ) if len ( posCol ) > 1 : for index , value in enumerate ( posCol ): axes [ index ] . set_title ( value ) else : axes . set_title ( value ) return fig , axes def density_plot ( self , * args , ** kwargs ): \"\"\"Density plot of measurement. Uses Seaborn distplot to plot measurement density. Arguments: *args (Any): arguments passed on to seaborn histplot function. **kwargs (Any): keyword arguments passed on to seaborn histplot function. Returns: FacetGrid (seaborn.FacetGrid): Seaborn FacetGrid of density density plot. \"\"\" plot = sns . displot ( self . data [ self . measurement ], kind = \"kde\" , palette = \"pastel\" , label = self . measurement , * args , ** kwargs ) # Plot formatting plt . legend ( prop = { 'size' : 10 }) plt . title ( 'Density Plot of Measurement' ) plt . xlabel ( 'Measurement' ) plt . ylabel ( 'Density' ) return plot def histogram ( self , bins : str = 'auto' , * args , ** kwargs ) -> plt . Axes . subplot : \"\"\"Histogram of tracklenght. Uses seaborn histplot function to plot tracklenght histogram. Arguments: bins (str): number or width of bins in histogram *args (Any): arguments passed on to seaborn histplot function. **kwargs (Any): keyword arguments passed on to seaborn histplot function. Returns: AxesSubplot: Matplotlib AxesSubplot of histogram. \"\"\" # Draw histogram track_length = self . data . groupby ( self . id ) . size () axes = sns . histplot ( track_length , label = \"Track Length\" , bins = bins , * args , ** kwargs ) # Plot formatting plt . title ( 'Track length Histogram' ) axes . set_xlabel ( 'Track Length' ) axes . set_ylabel ( 'Count' ) return axes","title":"dataPlots"},{"location":"api/#arcos4py.plotting._plotting.dataPlots.__init__","text":"Plot different metrics such as histogram, position-t and density. Parameters: Name Type Description Default data Dataframe containing ARCOS data. required frame str name of frame column in data. required measurement str name of measurement column in data. required id str name of track id column. required Source code in arcos4py/plotting/_plotting.py 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , data : pd . DataFrame , frame : str , measurement : str , id : str ): \"\"\"Plot different metrics such as histogram, position-t and density. Arguments: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. id (str): name of track id column. \"\"\" self . data = data self . id = id self . frame = frame self . measurement = measurement","title":"__init__()"},{"location":"api/#arcos4py.plotting._plotting.dataPlots.density_plot","text":"Density plot of measurement. Uses Seaborn distplot to plot measurement density. Parameters: Name Type Description Default *args Any arguments passed on to seaborn histplot function. () **kwargs Any keyword arguments passed on to seaborn histplot function. {} Returns: Name Type Description FacetGrid seaborn . FacetGrid Seaborn FacetGrid of density density plot. Source code in arcos4py/plotting/_plotting.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def density_plot ( self , * args , ** kwargs ): \"\"\"Density plot of measurement. Uses Seaborn distplot to plot measurement density. Arguments: *args (Any): arguments passed on to seaborn histplot function. **kwargs (Any): keyword arguments passed on to seaborn histplot function. Returns: FacetGrid (seaborn.FacetGrid): Seaborn FacetGrid of density density plot. \"\"\" plot = sns . displot ( self . data [ self . measurement ], kind = \"kde\" , palette = \"pastel\" , label = self . measurement , * args , ** kwargs ) # Plot formatting plt . legend ( prop = { 'size' : 10 }) plt . title ( 'Density Plot of Measurement' ) plt . xlabel ( 'Measurement' ) plt . ylabel ( 'Density' ) return plot","title":"density_plot()"},{"location":"api/#arcos4py.plotting._plotting.dataPlots.histogram","text":"Histogram of tracklenght. Uses seaborn histplot function to plot tracklenght histogram. Parameters: Name Type Description Default bins str number or width of bins in histogram 'auto' *args Any arguments passed on to seaborn histplot function. () **kwargs Any keyword arguments passed on to seaborn histplot function. {} Returns: Name Type Description AxesSubplot plt . Axes . subplot Matplotlib AxesSubplot of histogram. Source code in arcos4py/plotting/_plotting.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 def histogram ( self , bins : str = 'auto' , * args , ** kwargs ) -> plt . Axes . subplot : \"\"\"Histogram of tracklenght. Uses seaborn histplot function to plot tracklenght histogram. Arguments: bins (str): number or width of bins in histogram *args (Any): arguments passed on to seaborn histplot function. **kwargs (Any): keyword arguments passed on to seaborn histplot function. Returns: AxesSubplot: Matplotlib AxesSubplot of histogram. \"\"\" # Draw histogram track_length = self . data . groupby ( self . id ) . size () axes = sns . histplot ( track_length , label = \"Track Length\" , bins = bins , * args , ** kwargs ) # Plot formatting plt . title ( 'Track length Histogram' ) axes . set_xlabel ( 'Track Length' ) axes . set_ylabel ( 'Count' ) return axes","title":"histogram()"},{"location":"api/#arcos4py.plotting._plotting.dataPlots.position_t_plot","text":"Plots X and Y over T to visualize tracklength. Parameters: Name Type Description Default posCol set containing names of position columns in data. {'x'} n int number of samples to plot. 20 Returns: Name Type Description fig matplotlib . figure . Figure Matplotlib figure object of density plot. axes matplotlib . axes . Axes Matplotlib axes of density plot. Source code in arcos4py/plotting/_plotting.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def position_t_plot ( self , posCol : set [ str ] = { 'x' }, n : int = 20 ) -> Union [ plt . Axes , plt . Figure ]: \"\"\"Plots X and Y over T to visualize tracklength. Arguments: posCol (set): containing names of position columns in data. n (int): number of samples to plot. Returns: fig (matplotlib.figure.Figure): Matplotlib figure object of density plot. axes (matplotlib.axes.Axes): Matplotlib axes of density plot. \"\"\" sample = pd . Series ( self . data [ self . id ] . unique ()) . sample ( n ) pd_from_r_df = self . data . loc [ self . data [ self . id ] . isin ( sample )] fig , axes = plt . subplots ( 1 , len ( posCol ), figsize = ( 6 , 3 )) for label , df in pd_from_r_df . groupby ( self . id ): for index , value in enumerate ( posCol ): if len ( posCol ) > 1 : df . plot ( x = self . frame , y = value , ax = axes [ index ], legend = None ) else : df . plot ( x = self . frame , y = value , ax = axes , legend = None ) if len ( posCol ) > 1 : for index , value in enumerate ( posCol ): axes [ index ] . set_title ( value ) else : axes . set_title ( value ) return fig , axes","title":"position_t_plot()"},{"location":"api/#arcos4py.plotting.plotOriginalDetrended","text":"Plot different detrended vs original data. Attributes: Name Type Description data Dataframe containing ARCOS data. frame str name of frame column in data. measurement str name of measurement column in data. detrended str name of detrended column with detrended data. id str name of track id column. Source code in arcos4py/plotting/_plotting.py 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 class plotOriginalDetrended : \"\"\"Plot different detrended vs original data. Attributes: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. detrended (str): name of detrended column with detrended data. id (str): name of track id column. \"\"\" def __init__ ( self , data : pd . DataFrame , frame : str , measurement : str , detrended : str , id : str ): \"\"\"Plot detrended vs original data. Arguments: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. detrended (str): name of detrended column with detrended data. id (str): name of track id column. \"\"\" self . data = data self . measurement = measurement self . detrended = detrended self . id = id self . frame = frame def plot_detrended ( self , n_samples : int = 25 , subplots : tuple = ( 5 , 5 ), plotsize : tuple = ( 20 , 10 )): \"\"\"Method to plot detrended vs original data. Arguments: n_samples (int): Number of tracks to plot. subplots (tuple): Number of subplots, should be approx. one per sample. plotsize (tuple): Size of generated plot. Returns: Fig (matplotlib.axes.Axes): Matplotlib axes2d of detrended vs original data. Axes (matplotlib.axes.Axes): Matplotlib figure of detrended vs original data. \"\"\" vals = np . random . choice ( self . data [ self . id ] . unique (), n_samples , replace = False ) self . data = self . data . set_index ( self . id ) . loc [ vals ] . reset_index () grouped = self . data . groupby ( self . id ) ncols = subplots [ 0 ] nrows = subplots [ 1 ] fig , axes2d = plt . subplots ( nrows = nrows , ncols = ncols , figsize = plotsize , sharey = True ) for ( key , ax ) in zip ( grouped . groups . keys (), axes2d . flatten ()): grouped . get_group ( key ) . plot ( x = self . frame , y = [ self . measurement , self . detrended ], ax = ax ) ax . get_legend () . remove () handles , labels = ax . get_legend_handles_labels () fig . legend ( handles , labels , loc = \"lower right\" ) return fig , axes2d","title":"plotOriginalDetrended"},{"location":"api/#arcos4py.plotting._plotting.plotOriginalDetrended.__init__","text":"Plot detrended vs original data. Parameters: Name Type Description Default data Dataframe containing ARCOS data. required frame str name of frame column in data. required measurement str name of measurement column in data. required detrended str name of detrended column with detrended data. required id str name of track id column. required Source code in arcos4py/plotting/_plotting.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def __init__ ( self , data : pd . DataFrame , frame : str , measurement : str , detrended : str , id : str ): \"\"\"Plot detrended vs original data. Arguments: data (Dataframe): containing ARCOS data. frame (str): name of frame column in data. measurement (str): name of measurement column in data. detrended (str): name of detrended column with detrended data. id (str): name of track id column. \"\"\" self . data = data self . measurement = measurement self . detrended = detrended self . id = id self . frame = frame","title":"__init__()"},{"location":"api/#arcos4py.plotting._plotting.plotOriginalDetrended.plot_detrended","text":"Method to plot detrended vs original data. Parameters: Name Type Description Default n_samples int Number of tracks to plot. 25 subplots tuple Number of subplots, should be approx. one per sample. (5, 5) plotsize tuple Size of generated plot. (20, 10) Returns: Name Type Description Fig matplotlib . axes . Axes Matplotlib axes2d of detrended vs original data. Axes matplotlib . axes . Axes Matplotlib figure of detrended vs original data. Source code in arcos4py/plotting/_plotting.py 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def plot_detrended ( self , n_samples : int = 25 , subplots : tuple = ( 5 , 5 ), plotsize : tuple = ( 20 , 10 )): \"\"\"Method to plot detrended vs original data. Arguments: n_samples (int): Number of tracks to plot. subplots (tuple): Number of subplots, should be approx. one per sample. plotsize (tuple): Size of generated plot. Returns: Fig (matplotlib.axes.Axes): Matplotlib axes2d of detrended vs original data. Axes (matplotlib.axes.Axes): Matplotlib figure of detrended vs original data. \"\"\" vals = np . random . choice ( self . data [ self . id ] . unique (), n_samples , replace = False ) self . data = self . data . set_index ( self . id ) . loc [ vals ] . reset_index () grouped = self . data . groupby ( self . id ) ncols = subplots [ 0 ] nrows = subplots [ 1 ] fig , axes2d = plt . subplots ( nrows = nrows , ncols = ncols , figsize = plotsize , sharey = True ) for ( key , ax ) in zip ( grouped . groups . keys (), axes2d . flatten ()): grouped . get_group ( key ) . plot ( x = self . frame , y = [ self . measurement , self . detrended ], ax = ax ) ax . get_legend () . remove () handles , labels = ax . get_legend_handles_labels () fig . legend ( handles , labels , loc = \"lower right\" ) return fig , axes2d","title":"plot_detrended()"},{"location":"api/#arcos4py.plotting.statsPlots","text":"Plot data generated by the stats module. Attributes: Name Type Description data DataFrame containing ARCOS stats data. Source code in arcos4py/plotting/_plotting.py 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 class statsPlots : \"\"\"Plot data generated by the stats module. Attributes: data (DataFrame): containing ARCOS stats data. \"\"\" def __init__ ( self , data : pd . DataFrame ): \"\"\"Plot detrended vs original data. Arguments: data (DataFrame): containing ARCOS stats data. \"\"\" self . data = data def plot_events_duration ( self , total_size : str , duration : str , point_size : int = 40 , * args , ** kwargs ) -> matplotlib . axes . Axes : \"\"\"Scatterplot of collective event duration. Arguments: total_size (str): name of total size column. duration (str):, name of column with collective event duration. point_size (int): scatterplot point size. *args (Any): Arguments passed on to seaborn scatterplot function. **kwargs (Any): Keyword arguments passed on to seaborn scatterplot function. Returns: Axes (matplotlib.axes.Axes): Matplotlib Axes object of scatterplot \"\"\" plot = sns . scatterplot ( x = self . data [ total_size ], y = self . data [ duration ], s = point_size , * args , ** kwargs ) return plot","title":"statsPlots"},{"location":"api/#arcos4py.plotting._plotting.statsPlots.__init__","text":"Plot detrended vs original data. Parameters: Name Type Description Default data DataFrame containing ARCOS stats data. required Source code in arcos4py/plotting/_plotting.py 221 222 223 224 225 226 227 def __init__ ( self , data : pd . DataFrame ): \"\"\"Plot detrended vs original data. Arguments: data (DataFrame): containing ARCOS stats data. \"\"\" self . data = data","title":"__init__()"},{"location":"api/#arcos4py.plotting._plotting.statsPlots.plot_events_duration","text":"Scatterplot of collective event duration. Parameters: Name Type Description Default total_size str name of total size column. required duration str , name of column with collective event duration. required point_size int scatterplot point size. 40 *args Any Arguments passed on to seaborn scatterplot function. () **kwargs Any Keyword arguments passed on to seaborn scatterplot function. {} Returns: Name Type Description Axes matplotlib . axes . Axes Matplotlib Axes object of scatterplot Source code in arcos4py/plotting/_plotting.py 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def plot_events_duration ( self , total_size : str , duration : str , point_size : int = 40 , * args , ** kwargs ) -> matplotlib . axes . Axes : \"\"\"Scatterplot of collective event duration. Arguments: total_size (str): name of total size column. duration (str):, name of column with collective event duration. point_size (int): scatterplot point size. *args (Any): Arguments passed on to seaborn scatterplot function. **kwargs (Any): Keyword arguments passed on to seaborn scatterplot function. Returns: Axes (matplotlib.axes.Axes): Matplotlib Axes object of scatterplot \"\"\" plot = sns . scatterplot ( x = self . data [ total_size ], y = self . data [ duration ], s = point_size , * args , ** kwargs ) return plot","title":"plot_events_duration()"},{"location":"api/#arcos4py.tools","text":"Tools for detecting collective events.","title":"tools"},{"location":"api/#arcos4py.tools.binData","text":"Bases: detrender Smooth, de-trend, and binarise the input data. First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold. Attributes: Name Type Description smoothK int Size of the short-term median smoothing filter. biasK int Size of the long-term de-trending median filter. peakThr float Threshold for rescaling of the de-trended signal. binThr float Threshold for binarizing the de-trended signal. polyDeg int Sets the degree of the polynomial for lm fitting. biasMet str De-trending method, one of ['runmed', 'lm', 'none']. Source code in arcos4py/tools/_binarize_detrend.py 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 class binData ( detrender ): \"\"\"Smooth, de-trend, and binarise the input data. First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}. After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold. Attributes: smoothK (int): Size of the short-term median smoothing filter. biasK (int): Size of the long-term de-trending median filter. peakThr (float): Threshold for rescaling of the de-trended signal. binThr (float): Threshold for binarizing the de-trended signal. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): De-trending method, one of ['runmed', 'lm', 'none']. \"\"\" def __init__ ( self , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , binThr : float = 0.1 , polyDeg : int = 1 , biasMet : str = \"runmed\" , n_jobs : int = 1 , ) -> None : \"\"\"Smooth, de-trend, and binarise the input data. Arguments: smoothK (int): Size of the short-term median smoothing filter. biasK (int): Size of the long-term de-trending median filter. peakThr (float): Threshold for rescaling of the de-trended signal. binThr (float): Threshold for binarizing the de-trended signal. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): De-trending method, one of ['runmed', 'lm', 'none']. n_jobs (int): Number of jobs to run in parallel. \"\"\" super () . __init__ ( smoothK , biasK , peakThr , polyDeg , biasMet , n_jobs ) self . binThr = binThr def _rescale_data ( self , x : np . ndarray , meas_index : int , feat_range : tuple = ( 0 , 1 )) -> np . ndarray : \"\"\"Rescale data to a given range.\"\"\" meas_array = x [:, meas_index ] . astype ( 'float64' ) rescaled = minmax_scale ( meas_array , feature_range = feat_range ) x [:, meas_index ] = rescaled return x def _bin_data ( self , x : np . ndarray ) -> np . ndarray : bin = ( x >= self . binThr ) . astype ( np . int_ ) return bin def run ( self , x : pd . DataFrame , colGroup : str | None , colMeas : str , colFrame : str ) -> pd . DataFrame : \"\"\"Runs binarization and detrending. If the bias Method is 'none', first it rescales the data to between [0,1], then local smoothing is applied to the measurement by groups, followed by binarization. If biasMeth is one of ['lm', 'runmed'], first the data is detrended locally with a median filter and then detrended globally, for 'lm' with a linear model and for 'runmed' with a median filter. Followed by binarization of the data. Arguments: x (DataFrame): The time-series data for smoothing, detrending and binarization. colGroup (str | None): Object id column in x. Detrending and rescaling is performed on a per-object basis. If None, no detrending is performed, only rescaling and bias method is ignored. colMeas (str): Measurement column in x on which detrending and rescaling is performed. colFrame (str): Frame column in Time-series data. Used for sorting. Returns: DataFrame: Dataframe containing binarized data, rescaled data and the original columns. \"\"\" if colGroup is None : return self . _run_without_groupcol ( x , colMeas , colFrame ) else : return self . _run_with_groupcol ( x , colGroup , colMeas , colFrame ) def _run_without_groupcol ( self , x , colMeas , colFrame ): col_resc = f \" { colMeas } .resc\" col_bin = f \" { colMeas } .bin\" cols = [ colMeas ] x = x . sort_values ( colFrame ) data_np = x [ cols ] . to_numpy () if self . biasMet == \"none\" : rescaled_data = self . _rescale_data ( data_np , meas_index = 0 ) binarized_data = self . _bin_data ( rescaled_data ) else : warn ( \"No detrending is performed, only rescaling. To run detrending, set colGroup.\" ) rescaled_data = self . _rescale_data ( data_np , meas_index = 0 ) binarized_data = self . _bin_data ( rescaled_data ) x [ col_resc ] = rescaled_data [:, 0 ] x [ col_bin ] = binarized_data [:, 0 ] return x def _run_with_groupcol ( self , x , colGroup , colMeas , colFrame ): col_resc = f \" { colMeas } .resc\" col_bin = f \" { colMeas } .bin\" col_fact = f ' { colGroup } _factorized' cols = [ col_fact , colMeas ] x = x . sort_values ([ colGroup , colFrame ]) # factorize column in order to prevent numpy grouping error in detrending value , label = x [ colGroup ] . factorize () x [ col_fact ] = value data_np = x [ cols ] . to_numpy () if self . biasMet == \"none\" : rescaled_data = self . _rescale_data ( data_np , meas_index = 1 ) detrended_data = self . detrend ( rescaled_data , 0 , 1 ) binarized_data = self . _bin_data ( detrended_data ) else : detrended_data = self . detrend ( data_np , group_index = 0 , meas_index = 1 ) binarized_data = self . _bin_data ( detrended_data ) x = x . drop ([ col_fact ], axis = 1 ) x [ col_resc ] = detrended_data x [ col_bin ] = binarized_data return x","title":"binData"},{"location":"api/#arcos4py.tools._binarize_detrend.binData.__init__","text":"Smooth, de-trend, and binarise the input data. Parameters: Name Type Description Default smoothK int Size of the short-term median smoothing filter. 3 biasK int Size of the long-term de-trending median filter. 51 peakThr float Threshold for rescaling of the de-trended signal. 0.2 binThr float Threshold for binarizing the de-trended signal. 0.1 polyDeg int Sets the degree of the polynomial for lm fitting. 1 biasMet str De-trending method, one of ['runmed', 'lm', 'none']. 'runmed' n_jobs int Number of jobs to run in parallel. 1 Source code in arcos4py/tools/_binarize_detrend.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def __init__ ( self , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , binThr : float = 0.1 , polyDeg : int = 1 , biasMet : str = \"runmed\" , n_jobs : int = 1 , ) -> None : \"\"\"Smooth, de-trend, and binarise the input data. Arguments: smoothK (int): Size of the short-term median smoothing filter. biasK (int): Size of the long-term de-trending median filter. peakThr (float): Threshold for rescaling of the de-trended signal. binThr (float): Threshold for binarizing the de-trended signal. polyDeg (int): Sets the degree of the polynomial for lm fitting. biasMet (str): De-trending method, one of ['runmed', 'lm', 'none']. n_jobs (int): Number of jobs to run in parallel. \"\"\" super () . __init__ ( smoothK , biasK , peakThr , polyDeg , biasMet , n_jobs ) self . binThr = binThr","title":"__init__()"},{"location":"api/#arcos4py.tools._binarize_detrend.binData.run","text":"Runs binarization and detrending. If the bias Method is 'none', first it rescales the data to between [0,1], then local smoothing is applied to the measurement by groups, followed by binarization. If biasMeth is one of ['lm', 'runmed'], first the data is detrended locally with a median filter and then detrended globally, for 'lm' with a linear model and for 'runmed' with a median filter. Followed by binarization of the data. Parameters: Name Type Description Default x DataFrame The time-series data for smoothing, detrending and binarization. required colGroup str | None Object id column in x. Detrending and rescaling is performed on a per-object basis. If None, no detrending is performed, only rescaling and bias method is ignored. required colMeas str Measurement column in x on which detrending and rescaling is performed. required colFrame str Frame column in Time-series data. Used for sorting. required Returns: Name Type Description DataFrame pd . DataFrame Dataframe containing binarized data, rescaled data and the original columns. Source code in arcos4py/tools/_binarize_detrend.py 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def run ( self , x : pd . DataFrame , colGroup : str | None , colMeas : str , colFrame : str ) -> pd . DataFrame : \"\"\"Runs binarization and detrending. If the bias Method is 'none', first it rescales the data to between [0,1], then local smoothing is applied to the measurement by groups, followed by binarization. If biasMeth is one of ['lm', 'runmed'], first the data is detrended locally with a median filter and then detrended globally, for 'lm' with a linear model and for 'runmed' with a median filter. Followed by binarization of the data. Arguments: x (DataFrame): The time-series data for smoothing, detrending and binarization. colGroup (str | None): Object id column in x. Detrending and rescaling is performed on a per-object basis. If None, no detrending is performed, only rescaling and bias method is ignored. colMeas (str): Measurement column in x on which detrending and rescaling is performed. colFrame (str): Frame column in Time-series data. Used for sorting. Returns: DataFrame: Dataframe containing binarized data, rescaled data and the original columns. \"\"\" if colGroup is None : return self . _run_without_groupcol ( x , colMeas , colFrame ) else : return self . _run_with_groupcol ( x , colGroup , colMeas , colFrame )","title":"run()"},{"location":"api/#arcos4py.tools.calcCollevStats","text":"Class to calculate statistics of collective events. Source code in arcos4py/tools/_stats.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 class calcCollevStats : \"\"\"Class to calculate statistics of collective events.\"\"\" def __init__ ( self ) -> None : \"\"\"Class to calculate statistics of collective events.\"\"\" pass def _calculate_stats_group ( self , data : np . ndarray ) -> np . ndarray : \"\"\"Calculates duration and size for the collective event in the dataframe. Arguments: data (np.ndarray): Containing a single collective event. Returns: np.ndarray: Array containing the collective event id, duration, tot_size, min_size, max_size, \"nd_frame, first_frame_centroid and last_frame_centroid of the current collective event. \"\"\" coll_dur = max ( data [:, 0 ]) - min ( data [:, 0 ]) + 1 coll_total_size = np . unique ( data [:, 1 ]) . size ( unique , counts ) = np . unique ( data [:, 0 ], return_counts = True ) frequencies = np . asarray (( unique , counts )) . T coll_min_size = np . min ( frequencies [:, 1 ]) coll_max_size = np . max ( frequencies [:, 1 ]) coll_start_frame = np . min ( data [:, 0 ]) coll_end_frame = np . max ( data [:, 0 ]) if data . shape [ 1 ] > 3 : coll_start_coord = np . mean ( data [( data [:, 0 ] == coll_start_frame )][:, 3 :], axis = 0 ) coll_end_coord = np . mean ( data [( data [:, 0 ] == coll_end_frame )][:, 3 :], axis = 0 ) else : coll_start_coord = np . nan coll_end_coord = np . nan d = np . array ( [ data [ 0 , 2 ], coll_dur , coll_total_size , coll_min_size , coll_max_size , coll_start_frame , coll_end_frame , coll_start_coord , coll_end_coord , ], dtype = object , ) return d def _get_collev_stats ( self , data : pd . DataFrame , frame_column : str , collev_id : str , obj_id_column : str , posCol : Union [ list , None ], ) -> pd . DataFrame : \"\"\"Applies self._calculate_duration_size_group() to every group\\ i.e. every collective event. Arguments: data (DataFrame): Containing unfiltered collective events. collev_id (str): Indicating the contained collective id column. frame_column (str): Indicating the contained frame column. obj_id_column (str): Indicating object id. posCol (list | None): Contains names of position columns. If None coordinates of start and end frame are not calcualted Returns: DataFrame: DataFrame containing \"collid\", \"duration\", \"total_size\", \"min_size\",\"max_size\", \"start_frame\", \"end_frame\", \"first_frame_centroid\" and \"last_frame_centroid\" of all collective events. \"\"\" cols = [ collev_id , \"duration\" , \"total_size\" , \"min_size\" , \"max_size\" , \"start_frame\" , \"end_frame\" , \"first_frame_centroid\" , \"last_frame_centroid\" , ] if data . empty : return pd . DataFrame ( data = { i : [ np . nan ] for i in cols }) subset = [ frame_column , obj_id_column , collev_id ] if posCol : subset . extend ( posCol ) # if object id cannot be converted to a float, generate unique labels with pd.factorize try : data_np = data [ subset ] . to_numpy ( dtype = np . float64 ) except ValueError : labels , levels = pd . factorize ( data [ obj_id_column ]) new_obj_id = f ' { obj_id_column } _labels' data [ new_obj_id ] = labels subset [ 1 ] = new_obj_id data_np = data [ subset ] . to_numpy ( dtype = np . float64 ) data_np = data_np [ ~ np . isnan ( data_np ) . any ( axis = 1 )] data_np_sorted = data_np [ data_np [:, 2 ] . argsort ()] grouped_array = np . split ( data_np_sorted , np . unique ( data_np_sorted [:, 2 ], axis = 0 , return_index = True )[ 1 ][ 1 :]) # map to grouped_array out = map ( self . _calculate_stats_group , grouped_array ) out_list = [ i for i in out ] df = pd . DataFrame ( out_list , columns = cols ) return df def calculate ( self , data : pd . DataFrame , frame_column : str , collid_column : str , obj_id_column : str , posCol : Union [ list , None ] = None , ) -> pd . DataFrame : \"\"\"Calculate statistics of collective events. Arguments: data (DataFrame): Containing collective events. frame_column (str): Indicating the frame column in data. collid_column (str): Indicating the collective event id column in data. obj_id_column (str): Indicating object id. posCol (list | None): Contains names of position columns. If None coordinates of start and end frame are not calcualted Returns: DataFrame: DataFrame containing \"collid\", \"duration\", \"total_size\", \"min_size\",\"max_size\", \"start_frame\", \"end_frame\", \"first_frame_centroid\" and \"last_frame_centroid\" of all collective events. \"\"\" colev_stats = self . _get_collev_stats ( data , frame_column , collid_column , obj_id_column , posCol ) return colev_stats","title":"calcCollevStats"},{"location":"api/#arcos4py.tools._stats.calcCollevStats.__init__","text":"Class to calculate statistics of collective events. Source code in arcos4py/tools/_stats.py 18 19 20 def __init__ ( self ) -> None : \"\"\"Class to calculate statistics of collective events.\"\"\" pass","title":"__init__()"},{"location":"api/#arcos4py.tools._stats.calcCollevStats.calculate","text":"Calculate statistics of collective events. Parameters: Name Type Description Default data DataFrame Containing collective events. required frame_column str Indicating the frame column in data. required collid_column str Indicating the collective event id column in data. required obj_id_column str Indicating object id. required posCol list | None Contains names of position columns. If None coordinates of start and end frame are not calcualted None Returns: Name Type Description DataFrame pd . DataFrame DataFrame containing \"collid\", \"duration\", \"total_size\", \"min_size\",\"max_size\", \"start_frame\", \"end_frame\", \"first_frame_centroid\" and \"last_frame_centroid\" of all collective events. Source code in arcos4py/tools/_stats.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def calculate ( self , data : pd . DataFrame , frame_column : str , collid_column : str , obj_id_column : str , posCol : Union [ list , None ] = None , ) -> pd . DataFrame : \"\"\"Calculate statistics of collective events. Arguments: data (DataFrame): Containing collective events. frame_column (str): Indicating the frame column in data. collid_column (str): Indicating the collective event id column in data. obj_id_column (str): Indicating object id. posCol (list | None): Contains names of position columns. If None coordinates of start and end frame are not calcualted Returns: DataFrame: DataFrame containing \"collid\", \"duration\", \"total_size\", \"min_size\",\"max_size\", \"start_frame\", \"end_frame\", \"first_frame_centroid\" and \"last_frame_centroid\" of all collective events. \"\"\" colev_stats = self . _get_collev_stats ( data , frame_column , collid_column , obj_id_column , posCol ) return colev_stats","title":"calculate()"},{"location":"api/#arcos4py.tools.clipMeas","text":"Clip input array. Source code in arcos4py/tools/_cleandata.py 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 class clipMeas : \"\"\"Clip input array.\"\"\" def __init__ ( self , data : np . ndarray ) -> None : \"\"\"Clips array to quantilles. Arguments: data (ndarray): To be clipped. \"\"\" self . data = data def _calculate_percentile ( self , data : np . ndarray , clip_low : float , clip_high : float ) -> tuple [ np . ndarray , np . ndarray ]: \"\"\"Calculate upper and lower quantille. Arguments: data (ndarray): To calculate upper and lower quantile on. clip_low (float): Lower clipping boundary (quantile). clip_high (float): Upper clipping boundry (quantille). Returns: np.ndarray: Array with lower quantile and array with upper quantile. \"\"\" quantille_low = np . quantile ( data , clip_low , keepdims = True ) quantille_high = np . quantile ( data , clip_high , keepdims = True ) return quantille_low , quantille_high def clip ( self , clip_low : float = 0.001 , clip_high : float = 0.999 ) -> np . ndarray : \"\"\"Clip input array to upper and lower quantiles defined in clip_low and clip_high. Arguments: clip_low (float): Lower clipping boundary (quantile). clip_high (float): Upper clipping boundry (quantille). Returns: np.ndarray (np.ndarray): A clipped array of the input data. \"\"\" low , high = self . _calculate_percentile ( self . data , clip_low , clip_high ) out = self . data . clip ( low , high ) return out","title":"clipMeas"},{"location":"api/#arcos4py.tools._cleandata.clipMeas.__init__","text":"Clips array to quantilles. Parameters: Name Type Description Default data ndarray To be clipped. required Source code in arcos4py/tools/_cleandata.py 54 55 56 57 58 59 60 def __init__ ( self , data : np . ndarray ) -> None : \"\"\"Clips array to quantilles. Arguments: data (ndarray): To be clipped. \"\"\" self . data = data","title":"__init__()"},{"location":"api/#arcos4py.tools._cleandata.clipMeas.clip","text":"Clip input array to upper and lower quantiles defined in clip_low and clip_high. Parameters: Name Type Description Default clip_low float Lower clipping boundary (quantile). 0.001 clip_high float Upper clipping boundry (quantille). 0.999 Returns: Type Description np . ndarray np.ndarray (np.ndarray): A clipped array of the input data. Source code in arcos4py/tools/_cleandata.py 80 81 82 83 84 85 86 87 88 89 90 91 92 def clip ( self , clip_low : float = 0.001 , clip_high : float = 0.999 ) -> np . ndarray : \"\"\"Clip input array to upper and lower quantiles defined in clip_low and clip_high. Arguments: clip_low (float): Lower clipping boundary (quantile). clip_high (float): Upper clipping boundry (quantille). Returns: np.ndarray (np.ndarray): A clipped array of the input data. \"\"\" low , high = self . _calculate_percentile ( self . data , clip_low , clip_high ) out = self . data . clip ( low , high ) return out","title":"clip()"},{"location":"api/#arcos4py.tools.detectCollev","text":"Class to detect collective events. Attributes: Name Type Description input_data Union [ pd . DataFrame , np . ndarray ] The input data to track. eps float Maximum distance for clustering, default is 1. epsPrev Union [ float , None] Maximum distance for linking previous clusters, if None, eps is used. Default is None. minClSz int Minimum cluster size. Default is 3. nPrev int Number of previous frames to consider. Default is 1. posCols list List of column names for the position columns. Default is [\"x\"]. frame_column str Name of the column containing the frame number. Default is 'time'. id_column Union [ str , None] Name of the column containing the id. Default is None. bin_meas_column Union [ str , None] Name of the column containing the binary measurement. Default is 'meas'. clid_column str Name of the column containing the cluster id. Default is 'clTrackID'. dims str String of dimensions in order, such as. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", \"Z\". method str The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\". min_samples int | None The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. linkingMethod str The method used for linking. Default is 'nearest'. n_jobs int Number of jobs to run in parallel. Default is 1. predictor bool | Callable Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details. show_progress bool Whether or not to show progress bar. Default is True. Source code in arcos4py/tools/_detect_events.py 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 class detectCollev : \"\"\"Class to detect collective events. Attributes: input_data (Union[pd.DataFrame, np.ndarray]): The input data to track. eps (float): Maximum distance for clustering, default is 1. epsPrev (Union[float, None]): Maximum distance for linking previous clusters, if None, eps is used. Default is None. minClSz (int): Minimum cluster size. Default is 3. nPrev (int): Number of previous frames to consider. Default is 1. posCols (list): List of column names for the position columns. Default is [\"x\"]. frame_column (str): Name of the column containing the frame number. Default is 'time'. id_column (Union[str, None]): Name of the column containing the id. Default is None. bin_meas_column (Union[str, None]): Name of the column containing the binary measurement. Default is 'meas'. clid_column (str): Name of the column containing the cluster id. Default is 'clTrackID'. dims (str): String of dimensions in order, such as. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", \"Z\". method (str): The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\". min_samples (int | None): The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. linkingMethod (str): The method used for linking. Default is 'nearest'. n_jobs (int): Number of jobs to run in parallel. Default is 1. predictor (bool | Callable): Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details. show_progress (bool): Whether or not to show progress bar. Default is True. \"\"\" def __init__ ( self , input_data : Union [ pd . DataFrame , np . ndarray ], eps : float = 1 , epsPrev : Union [ int , None ] = None , minClSz : int = 1 , nPrev : int = 1 , posCols : list = [ \"x\" ], frame_column : str = 'time' , id_column : Union [ str , None ] = None , bin_meas_column : Union [ str , None ] = 'meas' , clid_column : str = 'clTrackID' , dims : str = \"TXY\" , method : str = \"dbscan\" , min_samples : int | None = None , linkingMethod = 'nearest' , n_jobs : int = 1 , predictor : bool | Callable = False , show_progress : bool = True , ) -> None : \"\"\"Constructs class with input parameters. Arguments: input_data (DataFrame): Input data to be processed. Must contain a binarized measurement column. eps (float): The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. epsPrev (int | None): Frame to frame distance, value is used to connect collective events across multiple frames.If \"None\", same value as eps is used. minClSz (int): Minimum size for a cluster to be identified as a collective event. nPrev (int): Number of previous frames the tracking algorithm looks back to connect collective events. posCols (list): List of position columns contained in the data. Must at least contain one. frame_column (str): Indicating the frame column in input_data. id_column (str | None): Indicating the track id/id column in input_data, optional. bin_meas_column (str): Indicating the bin_meas_column in input_data or None. clid_column (str): Indicating the column name containing the ids of collective events. dims (str): String of dimensions in order, used if input_data is a numpy array. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", \"Z\". method (str): The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\". min_samples (int | None): The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. linkingMethod (str): The method used for linking. Default is 'nearest'. n_jobs (int): Number of paralell workers to spawn, -1 uses all available cpus. predictor (bool | Callable): Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details. show_progress (bool): Whether or not to show progress bar. Default is True. \"\"\" self . input_data = input_data self . eps = eps self . epsPrev = epsPrev self . minClSz = minClSz self . nPrev = nPrev self . posCols = posCols self . frame_column = frame_column self . id_column = id_column self . bin_meas_column = bin_meas_column self . clid_column = clid_column self . dims = dims self . method = method self . linkingMethod = linkingMethod self . min_samples = min_samples self . predictor = predictor self . n_jobs = n_jobs self . show_progress = show_progress warnings . warn ( \"This class is deprecated and will be removed a future release, use the track_events_dataframe or track_events_image functions directly.\" , # noqa: E501 DeprecationWarning , ) def run ( self , copy : bool = True ) -> pd . DataFrame : \"\"\"Runs the collective event detection algorithm. Arguments: copy (bool): Whether or not to copy the input data. Default is True. Returns: DataFrame: Input data with added collective event ids. \"\"\" if isinstance ( self . input_data , pd . DataFrame ): if copy : self . input_data = self . input_data . copy () return track_events_dataframe ( X = self . input_data , coordinates_column = self . posCols , frame_column = self . frame_column , id_column = self . id_column , bin_meas_column = self . bin_meas_column , collid_column = self . clid_column , eps = self . eps , epsPrev = self . epsPrev , minClSz = self . minClSz , minSamples = self . min_samples , clusteringMethod = self . method , linkingMethod = self . linkingMethod , nPrev = self . nPrev , predictor = self . predictor , nJobs = self . n_jobs , showProgress = self . show_progress , ) elif isinstance ( self . input_data , np . ndarray ): if copy : self . input_data = np . copy ( self . input_data ) return track_events_image ( X = self . input_data , eps = self . eps , epsPrev = self . epsPrev , minClSz = self . minClSz , minSamples = self . min_samples , clusteringMethod = self . method , nPrev = self . nPrev , predictor = self . predictor , linkingMethod = self . linkingMethod , dims = self . dims , nJobs = self . n_jobs , showProgress = self . show_progress , )","title":"detectCollev"},{"location":"api/#arcos4py.tools._detect_events.detectCollev.__init__","text":"Constructs class with input parameters. Parameters: Name Type Description Default input_data DataFrame Input data to be processed. Must contain a binarized measurement column. required eps float The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. 1 epsPrev int | None Frame to frame distance, value is used to connect collective events across multiple frames.If \"None\", same value as eps is used. None minClSz int Minimum size for a cluster to be identified as a collective event. 1 nPrev int Number of previous frames the tracking algorithm looks back to connect collective events. 1 posCols list List of position columns contained in the data. Must at least contain one. ['x'] frame_column str Indicating the frame column in input_data. 'time' id_column str | None Indicating the track id/id column in input_data, optional. None bin_meas_column str Indicating the bin_meas_column in input_data or None. 'meas' clid_column str Indicating the column name containing the ids of collective events. 'clTrackID' dims str String of dimensions in order, used if input_data is a numpy array. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", \"Z\". 'TXY' method str The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\". 'dbscan' min_samples int | None The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. None linkingMethod str The method used for linking. Default is 'nearest'. 'nearest' n_jobs int Number of paralell workers to spawn, -1 uses all available cpus. 1 predictor bool | Callable Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details. False show_progress bool Whether or not to show progress bar. Default is True. True Source code in arcos4py/tools/_detect_events.py 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 def __init__ ( self , input_data : Union [ pd . DataFrame , np . ndarray ], eps : float = 1 , epsPrev : Union [ int , None ] = None , minClSz : int = 1 , nPrev : int = 1 , posCols : list = [ \"x\" ], frame_column : str = 'time' , id_column : Union [ str , None ] = None , bin_meas_column : Union [ str , None ] = 'meas' , clid_column : str = 'clTrackID' , dims : str = \"TXY\" , method : str = \"dbscan\" , min_samples : int | None = None , linkingMethod = 'nearest' , n_jobs : int = 1 , predictor : bool | Callable = False , show_progress : bool = True , ) -> None : \"\"\"Constructs class with input parameters. Arguments: input_data (DataFrame): Input data to be processed. Must contain a binarized measurement column. eps (float): The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster. epsPrev (int | None): Frame to frame distance, value is used to connect collective events across multiple frames.If \"None\", same value as eps is used. minClSz (int): Minimum size for a cluster to be identified as a collective event. nPrev (int): Number of previous frames the tracking algorithm looks back to connect collective events. posCols (list): List of position columns contained in the data. Must at least contain one. frame_column (str): Indicating the frame column in input_data. id_column (str | None): Indicating the track id/id column in input_data, optional. bin_meas_column (str): Indicating the bin_meas_column in input_data or None. clid_column (str): Indicating the column name containing the ids of collective events. dims (str): String of dimensions in order, used if input_data is a numpy array. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", \"Z\". method (str): The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\". min_samples (int | None): The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. linkingMethod (str): The method used for linking. Default is 'nearest'. n_jobs (int): Number of paralell workers to spawn, -1 uses all available cpus. predictor (bool | Callable): Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details. show_progress (bool): Whether or not to show progress bar. Default is True. \"\"\" self . input_data = input_data self . eps = eps self . epsPrev = epsPrev self . minClSz = minClSz self . nPrev = nPrev self . posCols = posCols self . frame_column = frame_column self . id_column = id_column self . bin_meas_column = bin_meas_column self . clid_column = clid_column self . dims = dims self . method = method self . linkingMethod = linkingMethod self . min_samples = min_samples self . predictor = predictor self . n_jobs = n_jobs self . show_progress = show_progress warnings . warn ( \"This class is deprecated and will be removed a future release, use the track_events_dataframe or track_events_image functions directly.\" , # noqa: E501 DeprecationWarning , )","title":"__init__()"},{"location":"api/#arcos4py.tools._detect_events.detectCollev.run","text":"Runs the collective event detection algorithm. Parameters: Name Type Description Default copy bool Whether or not to copy the input data. Default is True. True Returns: Name Type Description DataFrame pd . DataFrame Input data with added collective event ids. Source code in arcos4py/tools/_detect_events.py 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 def run ( self , copy : bool = True ) -> pd . DataFrame : \"\"\"Runs the collective event detection algorithm. Arguments: copy (bool): Whether or not to copy the input data. Default is True. Returns: DataFrame: Input data with added collective event ids. \"\"\" if isinstance ( self . input_data , pd . DataFrame ): if copy : self . input_data = self . input_data . copy () return track_events_dataframe ( X = self . input_data , coordinates_column = self . posCols , frame_column = self . frame_column , id_column = self . id_column , bin_meas_column = self . bin_meas_column , collid_column = self . clid_column , eps = self . eps , epsPrev = self . epsPrev , minClSz = self . minClSz , minSamples = self . min_samples , clusteringMethod = self . method , linkingMethod = self . linkingMethod , nPrev = self . nPrev , predictor = self . predictor , nJobs = self . n_jobs , showProgress = self . show_progress , ) elif isinstance ( self . input_data , np . ndarray ): if copy : self . input_data = np . copy ( self . input_data ) return track_events_image ( X = self . input_data , eps = self . eps , epsPrev = self . epsPrev , minClSz = self . minClSz , minSamples = self . min_samples , clusteringMethod = self . method , nPrev = self . nPrev , predictor = self . predictor , linkingMethod = self . linkingMethod , dims = self . dims , nJobs = self . n_jobs , showProgress = self . show_progress , )","title":"run()"},{"location":"api/#arcos4py.tools.filterCollev","text":"Select Collective events that last longer than coll_duration and have a larger total size than coll_total_size. Attributes: Name Type Description data Dataframe With detected collective events. frame_column str Indicating the frame column in data. collid_column str Indicating the collective event id column in data. obj_id_column str Inidicating the object identifier column such as cell track id. Source code in arcos4py/tools/_filter_events.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 class filterCollev : \"\"\"Select Collective events that last longer than coll_duration\\ and have a larger total size than coll_total_size. Attributes: data (Dataframe): With detected collective events. frame_column (str): Indicating the frame column in data. collid_column (str): Indicating the collective event id column in data. obj_id_column (str): Inidicating the object identifier column such as cell track id. \"\"\" def __init__ ( self , data : pd . DataFrame , frame_column : str = \"time\" , collid_column : str = \"collid\" , obj_id_column : str = \"trackID\" , ): \"\"\"Constructs filterCollev class with Parameters. Arguments: data (Dataframe): With detected collective events. frame_column (str): Indicating the frame column in data. collid_column (str): Indicating the collective event id column in data. obj_id_column (str): Inidicating the object identifier column such as cell track id. \"\"\" self . data = data self . frame_column = frame_column self . collid_column = collid_column self . obj_id_column = obj_id_column def _filter_collev ( self , data : pd . DataFrame , collev_stats : pd . DataFrame , collev_id : str , min_duration : int , min_size : int , ): \"\"\"Uses the dataframe generated by self._get_collev_duration()\\ to filter collective events that last longer than\\ min_duration and are larger than min_size. Arguments: data (DataFrame): Containing unfiltered collective events. collev_stats (DataFrame): Containing stats of collective events. collev_id (str): Indicating the contained collective id column. min_duration (str): minimal duration of a collective event for it to be returned. min_size (int): minimal size for a collective event to be returned. Returns: DataFrame (DataFrame): Dataframe containing filtered collective events. \"\"\" collev_stats = collev_stats [ ( collev_stats [ \"duration\" ] >= min_duration ) & ( collev_stats [ \"total_size\" ] >= min_size ) ] data = data [ data [ collev_id ] . isin ( collev_stats [ collev_id ])] return data def filter ( self , coll_duration : int = 9 , coll_total_size : int = 10 ) -> pd . DataFrame : \"\"\"Filter collective events. Method to filter collective events according to the parameters specified in the object instance. Arguments: coll_duration (int): Minimal duration of collective events to be selected. coll_total_size (int): Minimal total size of collective events to be selected. Returns: Returns pandas dataframe containing filtered collective events \"\"\" if self . data . empty : return self . data stats = calcCollevStats () colev_duration = stats . calculate ( self . data , self . frame_column , self . collid_column , self . obj_id_column ) filtered_df = self . _filter_collev ( self . data , colev_duration , self . collid_column , coll_duration , coll_total_size , ) return filtered_df","title":"filterCollev"},{"location":"api/#arcos4py.tools._filter_events.filterCollev.__init__","text":"Constructs filterCollev class with Parameters. Parameters: Name Type Description Default data Dataframe With detected collective events. required frame_column str Indicating the frame column in data. 'time' collid_column str Indicating the collective event id column in data. 'collid' obj_id_column str Inidicating the object identifier column such as cell track id. 'trackID' Source code in arcos4py/tools/_filter_events.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 def __init__ ( self , data : pd . DataFrame , frame_column : str = \"time\" , collid_column : str = \"collid\" , obj_id_column : str = \"trackID\" , ): \"\"\"Constructs filterCollev class with Parameters. Arguments: data (Dataframe): With detected collective events. frame_column (str): Indicating the frame column in data. collid_column (str): Indicating the collective event id column in data. obj_id_column (str): Inidicating the object identifier column such as cell track id. \"\"\" self . data = data self . frame_column = frame_column self . collid_column = collid_column self . obj_id_column = obj_id_column","title":"__init__()"},{"location":"api/#arcos4py.tools._filter_events.filterCollev.filter","text":"Filter collective events. Method to filter collective events according to the parameters specified in the object instance. Parameters: Name Type Description Default coll_duration int Minimal duration of collective events to be selected. 9 coll_total_size int Minimal total size of collective events to be selected. 10 Returns: Type Description pd . DataFrame Returns pandas dataframe containing filtered collective events Source code in arcos4py/tools/_filter_events.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def filter ( self , coll_duration : int = 9 , coll_total_size : int = 10 ) -> pd . DataFrame : \"\"\"Filter collective events. Method to filter collective events according to the parameters specified in the object instance. Arguments: coll_duration (int): Minimal duration of collective events to be selected. coll_total_size (int): Minimal total size of collective events to be selected. Returns: Returns pandas dataframe containing filtered collective events \"\"\" if self . data . empty : return self . data stats = calcCollevStats () colev_duration = stats . calculate ( self . data , self . frame_column , self . collid_column , self . obj_id_column ) filtered_df = self . _filter_collev ( self . data , colev_duration , self . collid_column , coll_duration , coll_total_size , ) return filtered_df","title":"filter()"},{"location":"api/#arcos4py.tools.interpolation","text":"Interpolate nan values in a numpy array. Attributes: Name Type Description data DataFrame Where NaN should be replaced with interpolated values. Source code in arcos4py/tools/_cleandata.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 class interpolation : \"\"\"Interpolate nan values in a numpy array. Attributes: data (DataFrame): Where NaN should be replaced with interpolated values. \"\"\" def __init__ ( self , data : pd . DataFrame ): \"\"\"Interpolate nan values in a pandas dataframe. Uses pandas.interpolate with liner interpolation. Arguments: data (DataFrame): Where NaN should be replaced with interpolated values. \"\"\" self . data = data def interpolate ( self ) -> pd . DataFrame : \"\"\"Interpolate nan and missing values. Returns: DataFrame: Interpolated input data. \"\"\" self . data = self . data . interpolate ( axis = 0 ) return self . data","title":"interpolation"},{"location":"api/#arcos4py.tools._cleandata.interpolation.__init__","text":"Interpolate nan values in a pandas dataframe. Uses pandas.interpolate with liner interpolation. Parameters: Name Type Description Default data DataFrame Where NaN should be replaced with interpolated values. required Source code in arcos4py/tools/_cleandata.py 30 31 32 33 34 35 36 37 38 def __init__ ( self , data : pd . DataFrame ): \"\"\"Interpolate nan values in a pandas dataframe. Uses pandas.interpolate with liner interpolation. Arguments: data (DataFrame): Where NaN should be replaced with interpolated values. \"\"\" self . data = data","title":"__init__()"},{"location":"api/#arcos4py.tools._cleandata.interpolation.interpolate","text":"Interpolate nan and missing values. Returns: Name Type Description DataFrame pd . DataFrame Interpolated input data. Source code in arcos4py/tools/_cleandata.py 40 41 42 43 44 45 46 47 48 def interpolate ( self ) -> pd . DataFrame : \"\"\"Interpolate nan and missing values. Returns: DataFrame: Interpolated input data. \"\"\" self . data = self . data . interpolate ( axis = 0 ) return self . data","title":"interpolate()"},{"location":"api/#arcos4py.tools.estimate_eps","text":"Estimates eps parameter in DBSCAN. Estimates the eps parameter for the DBSCAN clustering method, as used by ARCOS, by calculating the nearest neighbour distances for each point in the data. N_neighbours should be chosen to match the minimum point size in DBSCAN or the minimum clustersize in detect_events respectively. The method argument determines how the eps parameter is estimated. 'kneepoint' estimates the knee of the nearest neighbour distribution. 'mean' and 'median' return (by default) 1.5 times the mean or median of the nearest neighbour distances respectively. Parameters: Name Type Description Default data pd . DataFrame DataFrame containing the data. required method str Method to use for estimating eps. Defaults to 'kneepoint'. Can be one of ['kneepoint', 'mean', 'median'].'kneepoint' estimates the knee of the nearest neighbour distribution to to estimate eps. 'mean' and 'median' use the 1.5 times the mean or median of the nearest neighbour distances respectively. 'kneepoint' pos_cols list [ str ] List of column names containing the position data. ['x,y'] frame_col str Name of the column containing the frame number. Defaults to 't'. 't' n_neighbors int Number of nearest neighbours to consider. Defaults to 5. 5 plot bool Whether to plot the results. Defaults to True. True plt_size tuple [ int , int ] Size of the plot. Defaults to (5, 5). (5, 5) kwargs dict Keyword arguments for the method. Modify behaviour of respecitve method. For kneepoint: [S online, curve, direction, interp_method,polynomial_degree; For mean: [mean_multiplier] For median [median_multiplier] {} Returns: Name Type Description Eps float eps parameter for DBSCAN. Source code in arcos4py/tools/_detect_events.py 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 def estimate_eps ( data : pd . DataFrame , method : str = 'kneepoint' , pos_cols : list [ str ] = [ 'x,y' ], frame_col : str = 't' , n_neighbors : int = 5 , plot : bool = True , plt_size : tuple [ int , int ] = ( 5 , 5 ), max_samples = 50_000 , ** kwargs : dict , ): \"\"\"Estimates eps parameter in DBSCAN. Estimates the eps parameter for the DBSCAN clustering method, as used by ARCOS, by calculating the nearest neighbour distances for each point in the data. N_neighbours should be chosen to match the minimum point size in DBSCAN or the minimum clustersize in detect_events respectively. The method argument determines how the eps parameter is estimated. 'kneepoint' estimates the knee of the nearest neighbour distribution. 'mean' and 'median' return (by default) 1.5 times the mean or median of the nearest neighbour distances respectively. Arguments: data (pd.DataFrame): DataFrame containing the data. method (str, optional): Method to use for estimating eps. Defaults to 'kneepoint'. Can be one of ['kneepoint', 'mean', 'median'].'kneepoint' estimates the knee of the nearest neighbour distribution to to estimate eps. 'mean' and 'median' use the 1.5 times the mean or median of the nearest neighbour distances respectively. pos_cols (list[str]): List of column names containing the position data. frame_col (str, optional): Name of the column containing the frame number. Defaults to 't'. n_neighbors (int, optional): Number of nearest neighbours to consider. Defaults to 5. plot (bool, optional): Whether to plot the results. Defaults to True. plt_size (tuple[int, int], optional): Size of the plot. Defaults to (5, 5). kwargs: Keyword arguments for the method. Modify behaviour of respecitve method. For kneepoint: [S online, curve, direction, interp_method,polynomial_degree; For mean: [mean_multiplier] For median [median_multiplier] Returns: Eps (float): eps parameter for DBSCAN. \"\"\" subset = [ frame_col ] + pos_cols for i in subset : if i not in data . columns : raise ValueError ( f \"Column { i } not in data\" ) method_option = [ 'kneepoint' , 'mean' , 'median' ] if method not in method_option : raise ValueError ( f \"Method must be one of { method_option } \" ) allowedtypes : dict [ str , str ] = { 'kneepoint' : 'kneepoint_values' , 'mean' : 'mean_values' , 'median' : 'median_values' , } kwdefaults : dict [ str , Any ] = { 'S' : 1 , 'online' : True , 'curve' : 'convex' , 'direction' : 'increasing' , 'interp_method' : 'polynomial' , 'mean_multiplier' : 1.5 , 'median_multiplier' : 1.5 , 'polynomial_degree' : 7 , } kwtypes : dict [ str , Any ] = { 'S' : int , 'online' : bool , 'curve' : str , 'direction' : str , 'interp_method' : str , 'polynomial_degree' : int , 'mean_multiplier' : ( float , int ), 'median_multiplier' : ( float , int ), } allowedkwargs : dict [ str , list [ str ]] = { 'kneepoint_values' : [ 'S' , 'online' , 'curve' , 'interp_method' , 'direction' , 'polynomial_degree' ], 'mean_values' : [ 'mean_multiplier' ], 'median_values' : [ 'median_multiplier' ], } for key in kwargs : if key not in allowedkwargs [ allowedtypes [ method ]]: raise ValueError ( f ' { key } keyword not in allowed keywords { allowedkwargs [ allowedtypes [ method ]] } ' ) if not isinstance ( kwargs [ key ], kwtypes [ key ]): raise ValueError ( f ' { key } must be of type { kwtypes [ key ] } ' ) # Set kwarg defaults for kw in allowedkwargs [ allowedtypes [ method ]]: kwargs . setdefault ( kw , kwdefaults [ kw ]) subset = [ frame_col ] + pos_cols data_np = data [ subset ] . to_numpy ( dtype = np . float64 ) # sort by frame data_np = data_np [ data_np [:, 0 ] . argsort ()] grouped_array = np . split ( data_np [:, 1 :], np . unique ( data_np [:, 0 ], axis = 0 , return_index = True )[ 1 ][ 1 :]) # map nearest_neighbours to grouped_array distances = np . concatenate ([ _nearest_neighbour_eps ( i , n_neighbors ) for i in grouped_array if i . shape [ 0 ] > 1 ]) # flatten array distances_flat = distances . flatten () distances_flat = distances_flat [ np . isfinite ( distances_flat )] distances_flat_selection = np . random . choice ( distances_flat , min ( max_samples , distances_flat . shape [ 0 ]), replace = False ) distances_sorted = np . sort ( distances_flat_selection ) if method == 'kneepoint' : k1 = KneeLocator ( np . arange ( 0 , distances_sorted . shape [ 0 ]), distances_sorted , S = kwargs [ 'S' ], online = kwargs [ 'online' ], curve = kwargs [ 'curve' ], interp_method = kwargs [ 'interp_method' ], direction = kwargs [ 'direction' ], polynomial_degree = kwargs [ 'polynomial_degree' ], ) eps = distances_sorted [ k1 . knee ] elif method == 'mean' : eps = np . mean ( distances_sorted ) * kwargs [ 'mean_multiplier' ] elif method == 'median' : eps = np . median ( distances_sorted ) * kwargs [ 'median_multiplier' ] if plot : fig , ax = plt . subplots ( figsize = plt_size ) ax . plot ( distances_sorted ) ax . axhline ( eps , color = 'r' , linestyle = '--' ) ax . set_xlabel ( 'Sorted Distance Index' ) ax . set_ylabel ( 'Nearest Neighbour Distance' ) ax . set_title ( f 'Estimated eps: { eps : .4f } ' ) plt . show () return eps","title":"estimate_eps()"},{"location":"api/#arcos4py.tools.track_events_dataframe","text":"Function to track collective events in a dataframe. Parameters: Name Type Description Default X pd . DataFrame The input dataframe containing the data to track. required coordinates_column List [ str ] The names of the columns representing coordinates. required frame_column str The name of the column containing frame ids. required id_column str | None The name of the column representing IDs. None if no such column. required bin_meas_column str | None The name of the column representing binarized measurements, if None all measurements are used. None collid_column str The name of the output column representing collective events, will be generated. 'collid' eps float Maximum distance for clustering, default is 1. 1.0 epsPrev float | None Maximum distance for linking previous clusters, if None, eps is used. Default is None. None minClSz int Minimum cluster size. Default is 3. 3 minSamples int The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. None clusteringMethod str The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\". 'dbscan' linkingMethod str The method used for linking, one of ['nearest', 'transportsolver']. Default is 'nearest'. 'nearest' nPrev int Number of previous frames to consider. Default is 1. 1 predictor bool | Callable Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details. False nJobs int Number of jobs to run in parallel. Default is 1. 1 showProgress bool Whether or not to show progress bar. Default is True. True Returns: Type Description pd . DataFrame pd.DataFrame: Dataframe with tracked events. Source code in arcos4py/tools/_detect_events.py 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 def track_events_dataframe ( X : pd . DataFrame , coordinates_column : List [ str ], frame_column : str , id_column : str | None , bin_meas_column : str | None = None , collid_column : str = \"collid\" , eps : float = 1.0 , epsPrev : int | None = None , minClSz : int = 3 , minSamples : int | None = None , clusteringMethod : str = \"dbscan\" , linkingMethod : str = 'nearest' , nPrev : int = 1 , predictor : bool | Callable = False , nJobs : int = 1 , showProgress : bool = True , ) -> pd . DataFrame : \"\"\"Function to track collective events in a dataframe. Arguments: X (pd.DataFrame): The input dataframe containing the data to track. coordinates_column (List[str]): The names of the columns representing coordinates. frame_column (str): The name of the column containing frame ids. id_column (str | None): The name of the column representing IDs. None if no such column. bin_meas_column (str | None): The name of the column representing binarized measurements, if None all measurements are used. collid_column (str): The name of the output column representing collective events, will be generated. eps (float): Maximum distance for clustering, default is 1. epsPrev (float | None): Maximum distance for linking previous clusters, if None, eps is used. Default is None. minClSz (int): Minimum cluster size. Default is 3. minSamples (int): The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. clusteringMethod (str): The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\". linkingMethod (str): The method used for linking, one of ['nearest', 'transportsolver']. Default is 'nearest'. nPrev (int): Number of previous frames to consider. Default is 1. predictor (bool | Callable): Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details. nJobs (int): Number of jobs to run in parallel. Default is 1. showProgress (bool): Whether or not to show progress bar. Default is True. Returns: pd.DataFrame: Dataframe with tracked events. \"\"\" linker = Linker ( eps = eps , epsPrev = epsPrev , minClSz = minClSz , minSamples = minSamples , clusteringMethod = clusteringMethod , linkingMethod = linkingMethod , nPrev = nPrev , predictor = predictor , nJobs = nJobs , ) tracker = DataFrameTracker ( linker = linker , coordinates_column = coordinates_column , frame_column = frame_column , id_column = id_column , bin_meas_column = bin_meas_column , collid_column = collid_column , ) df_out = pd . concat ( [ timepoint for timepoint in tqdm ( tracker . track ( X ), total = X [ frame_column ] . nunique (), disable = not showProgress )] ) . reset_index ( drop = True ) return df_out . query ( f \" { collid_column } != -1\" ) . reset_index ( drop = True )","title":"track_events_dataframe()"},{"location":"api/#arcos4py.tools.track_events_image","text":"Function to track events in an image using specified linking and clustering methods. Parameters: Name Type Description Default X np . ndarray The input array containing the images to track. required eps float Distance for clustering. Default is 1. 1 epsPrev float | None Maximum distance for linking previous clusters, if None, eps is used. Default is None. None minClSz int Minimum cluster size. Default is 1. 1 minSamples int | None The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. None clusteringMethod str The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\". 'dbscan' nPrev int Number of previous frames to consider. Default is 1. 1 predictor bool | Callable Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details. False linkingMethod str The method used for linking. Default is 'nearest'. 'nearest' dims str String of dimensions in order, such as. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", \"Z\". 'TXY' downsample int Factor by which to downsample the image. Default is 1. 1 nJobs int Number of jobs to run in parallel. Default is 1. 1 showProgress bool Whether or not to show progress bar. Default is True. True Returns: Type Description np . ndarray np.ndarray: Array of images with tracked events. Source code in arcos4py/tools/_detect_events.py 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 def track_events_image ( X : np . ndarray , eps : float = 1 , epsPrev : int | None = None , minClSz : int = 1 , minSamples : int | None = None , clusteringMethod : str = \"dbscan\" , nPrev : int = 1 , predictor : bool | Callable = False , linkingMethod : str = 'nearest' , dims : str = \"TXY\" , downsample : int = 1 , nJobs : int = 1 , showProgress : bool = True , ) -> np . ndarray : \"\"\"Function to track events in an image using specified linking and clustering methods. Arguments: X (np.ndarray): The input array containing the images to track. eps (float): Distance for clustering. Default is 1. epsPrev (float | None): Maximum distance for linking previous clusters, if None, eps is used. Default is None. minClSz (int): Minimum cluster size. Default is 1. minSamples (int | None): The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples = minClsz. clusteringMethod (str): The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\". nPrev (int): Number of previous frames to consider. Default is 1. predictor (bool | Callable): Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details. linkingMethod (str): The method used for linking. Default is 'nearest'. dims (str): String of dimensions in order, such as. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", \"Z\". downsample (int): Factor by which to downsample the image. Default is 1. nJobs (int): Number of jobs to run in parallel. Default is 1. showProgress (bool): Whether or not to show progress bar. Default is True. Returns: np.ndarray: Array of images with tracked events. \"\"\" linker = Linker ( eps = eps / downsample , epsPrev = epsPrev , minClSz = minClSz , minSamples = minSamples , clusteringMethod = clusteringMethod , linkingMethod = linkingMethod , nPrev = nPrev , predictor = predictor , nJobs = nJobs , ) tracker = ImageTracker ( linker , downsample = downsample ) # find indices of T in dims T_index = dims . upper () . index ( \"T\" ) return np . stack ( [ timepoint for timepoint in tqdm ( tracker . track ( X , dims ), total = X . shape [ T_index ], disable = not showProgress )], axis = T_index , )","title":"track_events_image()"},{"location":"api/#arcos4py.validation","text":"Tools for validating detected collective events.","title":"validation"},{"location":"api/#arcos4py.validation.bootstrap_arcos","text":"Bootstrap data using the ARCOS algorithm. Parameters: Name Type Description Default df pd . DataFrame DataFrame containing the data to be bootstrapped. required posCols list List of column names containing the x and y coordinates. required frame_column str Name of the column containing the frame number. required id_column str Name of the column containing the track id. required meas_column str Name of the column containing the measurement. required method str | list [ str ] Method used for bootstrapping. Can be \"shuffle_tracks\", 'shuffle_timepoints', 'shift_timepoints', 'shuffle_binary_blocks', 'shuffle_coordinates_timepoint or a list of methods, which will be applied in order of index. 'shuffle_tracks' smoothK int Smoothing kernel size. 3 biasK int Bias kernel size. 51 peakThr float Threshold for peak detection. 0.2 binThr float Threshold for binarization. 0.1 polyDeg int Degree of the polynomial used for bias correction. 1 biasMet str Bias correction method. Can be 'none', 'runmed', 'lm' 'runmed' eps float Epsilon parameter for DBSCAN. 2 epsPrev int | None Parameter for linking tracks. If None, eps is used. None minClsz int Minimum cluster size. 1 nPrev int Number of previous frames to consider for linking. 1 min_duration int Minimum duration of a track. 1 min_size int Minimum size of a track. 1 stats_metric str | list [ str ] Metric to calculate. Can be \"duration\", \"total_size\", \"min_size\", \"max_size\" or a list of metrics. Default is [\"duration\", \"total_size\"]. ['total_size', 'duration'] pval_alternative str Alternative hypothesis for the p-value calculation. Can be \"less\" or \"greater\". 'greater' finite_correction bool Correct p-values for finite sampling. Default is True. True n int Number of bootstraps. 100 seed int Seed for the random number generator. 42 allow_duplicates bool If False, resampling will check if the resampled data contains duplicates. If True, duplicates will be allowed. False max_tries int Maximum number of tries to resample data without duplicates. 100 show_progress bool Show a progress bar. True verbose bool Print additional information. False Returns: Type Description tuple [ pd . DataFrame , pd . DataFrame ] DataFrame containing the bootstrapped data. Source code in arcos4py/validation/_bootstrapping.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 def bootstrap_arcos ( df : pd . DataFrame , posCols : list , frame_column : str , id_column : str , meas_column : str , method : str | list [ str ] = 'shuffle_tracks' , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , binThr : float = 0.1 , polyDeg : int = 1 , biasMet : str = \"runmed\" , eps : float = 2 , epsPrev : int | None = None , minClsz : int = 1 , nPrev : int = 1 , min_duration : int = 1 , min_size : int = 1 , stats_metric : str | list [ str ] = [ \"total_size\" , \"duration\" ], pval_alternative : str = \"greater\" , finite_correction : bool = True , n : int = 100 , seed : int = 42 , allow_duplicates : bool = False , max_tries : int = 100 , show_progress : bool = True , verbose : bool = False , paralell_processing : bool = True , plot : bool = True , ) -> tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\"Bootstrap data using the ARCOS algorithm. Arguments: df: DataFrame containing the data to be bootstrapped. posCols: List of column names containing the x and y coordinates. frame_column: Name of the column containing the frame number. id_column: Name of the column containing the track id. meas_column: Name of the column containing the measurement. method: Method used for bootstrapping. Can be \"shuffle_tracks\", 'shuffle_timepoints', 'shift_timepoints', 'shuffle_binary_blocks', 'shuffle_coordinates_timepoint or a list of methods, which will be applied in order of index. smoothK: Smoothing kernel size. biasK: Bias kernel size. peakThr: Threshold for peak detection. binThr: Threshold for binarization. polyDeg: Degree of the polynomial used for bias correction. biasMet: Bias correction method. Can be 'none', 'runmed', 'lm' eps: Epsilon parameter for DBSCAN. epsPrev: Parameter for linking tracks. If None, eps is used. minClsz: Minimum cluster size. nPrev: Number of previous frames to consider for linking. min_duration: Minimum duration of a track. min_size: Minimum size of a track. stats_metric: Metric to calculate. Can be \"duration\", \"total_size\", \"min_size\", \"max_size\" or a list of metrics. Default is [\"duration\", \"total_size\"]. pval_alternative: Alternative hypothesis for the p-value calculation. Can be \"less\" or \"greater\". finite_correction: Correct p-values for finite sampling. Default is True. n: Number of bootstraps. seed: Seed for the random number generator. allow_duplicates: If False, resampling will check if the resampled data contains duplicates. If True, duplicates will be allowed. max_tries: Maximum number of tries to resample data without duplicates. show_progress: Show a progress bar. verbose: Print additional information. Returns: DataFrame containing the bootstrapped data. \"\"\" if not isinstance ( stats_metric , list ): stats_metric = [ stats_metric ] for stats_m in stats_metric : if stats_m not in [ \"duration\" , \"total_size\" , \"min_size\" , \"max_size\" , ]: raise ValueError ( f \"Invalid metric: { stats_metric } \" ) if pval_alternative not in [ \"less\" , \"greater\" ]: raise ValueError ( f \"Invalid alternative hypothesis: { pval_alternative } \" ) clid_name = 'clid' if isinstance ( method , str ): print ( f 'Resampling data using method \" { method } \"...' ) elif isinstance ( method , list ): print ( f 'Resampling data using methods \" { method } \"...' ) df_resampled = resample_data ( data = df , posCols = posCols , frame_column = frame_column , id_column = id_column , meas_column = meas_column , method = method , n = n , seed = seed , allow_duplicates = allow_duplicates , max_tries = max_tries , show_progress = show_progress , verbose = verbose , paralell_processing = paralell_processing , ) iterations = df_resampled [ 'iteration' ] . unique () print ( f 'Running ARCOS and calculating \" { stats_metric } \"...' ) stats_df , stats_df_mean = calculate_arcos_stats ( df_resampled = df_resampled , posCols = posCols , frame_column = frame_column , id_column = id_column , meas_column = meas_column , smoothK = smoothK , biasK = biasK , peakThr = peakThr , binThr = binThr , polyDeg = polyDeg , biasMet = biasMet , eps = eps , epsPrev = epsPrev , minClsz = minClsz , nPrev = nPrev , min_duration = min_duration , min_size = min_size , stats_metric = stats_metric , show_progress = show_progress , paralell_processing = paralell_processing , clid_name = clid_name , iterations = iterations , ) df_p = calculate_pvalue ( stats_df_mean , stats_metric , pval_alternative , finite_correction , plot ) return stats_df , df_p","title":"bootstrap_arcos()"},{"location":"api/#arcos4py.validation.calculate_arcos_stats","text":"Calculate the bootstrapped statistics. Parameters: Name Type Description Default df_resampled DataFrame Dataframe with resampled data. required iterations list [ int ] List of iteration names, or range. required posCols list List of position columns.. required frame_column str Name of the frame column. required id_column str Name of the id column. required meas_column str Name of the measurement column. required smoothK int Smoothing kernel size for local detrending. Defaults to 3. 3 biasK int Bias kernel size for large scale detrending (used with biasMet='runmed'). Defaults to 51. 51 peakThr float Peak threshold used for rescaling (used with biasMet='runmed'). Defaults to 0.2. 0.2 binThr float Threshold for binarizing measurements after detrending. Defaults to 0.1. 0.1 polyDeg int Polynomial degree used for detrending (used with biasMet='lm'). Defaults to 1. 1 biasMet str Bias method, can be 'none', 'runmed', 'lm'. Defaults to \"runmed\". 'runmed' eps float Epsilon used for culstering active entities. Defaults to 2. 2 epsPrev int Epsilon used for linking together culsters across time. Defaults to None. None minClsz int Minimum cluster size. Defaults to 1. 1 nPrev int Number of previous frames to consider when tracking clusters. Defaults to 1. 1 min_duration int Minimum duration of detected event. Defaults to 1. 1 min_size int Minimum size, minimum size of detected event. Defaults to 1. 1 stats_metric list [ str ] List of metrics to calculate. Defaults to ['duration', 'total_size']. ['duration', 'total_size'] show_progress bool Show progress bar. Defaults to True. True paralell_processing bool Use paralell processing, uses the joblib package. Defaults to True. True clid_name str Name of the cluster id column. Defaults to 'clid'. 'clid' Returns: Name Type Description DataFrame pd . DataFrame Dataframe with the bootstrapped statistics. DataFrame pd . DataFrame Dataframe with mean statistics. Source code in arcos4py/validation/_bootstrapping.py 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 def calculate_arcos_stats ( df_resampled : pd . DataFrame , iterations : list [ int ], posCols : list , frame_column : str , id_column : str , meas_column : str , smoothK : int = 3 , biasK : int = 51 , peakThr : float = 0.2 , binThr : float = 0.1 , polyDeg : int = 1 , biasMet : str = \"runmed\" , eps : float = 2 , epsPrev : int | None = None , minClsz : int = 1 , nPrev : int = 1 , min_duration : int = 1 , min_size : int = 1 , stats_metric : list [ str ] = [ 'duration' , 'total_size' ], show_progress : bool = True , paralell_processing : bool = True , clid_name : str = 'clid' , ): \"\"\"Calculate the bootstrapped statistics. Arguments: df_resampled (DataFrame): Dataframe with resampled data. iterations (list[int]): List of iteration names, or range. posCols (list): List of position columns.. frame_column (str): Name of the frame column. id_column (str): Name of the id column. meas_column (str): Name of the measurement column. smoothK (int, optional): Smoothing kernel size for local detrending. Defaults to 3. biasK (int, optional): Bias kernel size for large scale detrending (used with biasMet='runmed'). Defaults to 51. peakThr (float, optional): Peak threshold used for rescaling (used with biasMet='runmed'). Defaults to 0.2. binThr (float, optional): Threshold for binarizing measurements after detrending. Defaults to 0.1. polyDeg (int, optional): Polynomial degree used for detrending (used with biasMet='lm'). Defaults to 1. biasMet (str, optional): Bias method, can be 'none', 'runmed', 'lm'. Defaults to \"runmed\". eps (float, optional): Epsilon used for culstering active entities. Defaults to 2. epsPrev (int, optional): Epsilon used for linking together culsters across time. Defaults to None. minClsz (int, optional): Minimum cluster size. Defaults to 1. nPrev (int, optional): Number of previous frames to consider when tracking clusters. Defaults to 1. min_duration (int, optional): Minimum duration of detected event. Defaults to 1. min_size (int, optional): Minimum size, minimum size of detected event. Defaults to 1. stats_metric (list[str], optional): List of metrics to calculate. Defaults to ['duration', 'total_size']. show_progress (bool, optional): Show progress bar. Defaults to True. paralell_processing (bool, optional): Use paralell processing, uses the joblib package. Defaults to True. clid_name (str, optional): Name of the cluster id column. Defaults to 'clid'. Returns: DataFrame (pd.DataFrame): Dataframe with the bootstrapped statistics. DataFrame (pd.DataFrame): Dataframe with mean statistics. \"\"\" if paralell_processing : from joblib import Parallel , delayed stats_df_list = Parallel ( n_jobs =- 1 )( delayed ( _apply_arcos )( i_iter = i_iter , df_resampled = df_resampled , posCols = posCols , frame_column = frame_column , id_column = id_column , meas_column = meas_column , smoothK = smoothK , biasK = biasK , peakThr = peakThr , binThr = binThr , polyDeg = polyDeg , biasMet = biasMet , eps = eps , epsPrev = epsPrev , minClsz = minClsz , nPrev = nPrev , min_dur = min_duration , min_size = min_size , clid_name = clid_name , ) for i_iter in tqdm ( iterations , disable = not show_progress ) ) else : stats_df_list = [] for i_iter in tqdm ( iterations , disable = not show_progress ): stats_df = _apply_arcos ( i_iter = i_iter , df_resampled = df_resampled , posCols = posCols , frame_column = frame_column , id_column = id_column , meas_column = meas_column , smoothK = smoothK , biasK = biasK , peakThr = peakThr , binThr = binThr , polyDeg = polyDeg , biasMet = biasMet , eps = eps , epsPrev = epsPrev , minClsz = minClsz , nPrev = nPrev , min_dur = min_duration , min_size = min_size , clid_name = clid_name , ) stats_df_list . append ( stats_df ) stats_df = pd . concat ( stats_df_list , ignore_index = True ) stats_df_indexer = [ 'bootstrap_iteration' ] + stats_metric stats_df_mean : pd . DataFrame = ( stats_df [ stats_df_indexer ] . groupby ([ 'bootstrap_iteration' ]) . agg ([ 'mean' ]) . reset_index () ) stats_df_mean = stats_df_mean . droplevel ( level = 1 , axis = 1 ) # for bootstrap iteratoins that did not detect any events, set the metric to 0 stats_df_mean [ stats_metric ] = stats_df_mean [ stats_metric ] . fillna ( 0 ) return stats_df , stats_df_mean","title":"calculate_arcos_stats()"},{"location":"api/#arcos4py.validation.calculate_pvalue","text":"Calculates the p-value with the given alternative hypothesis. Parameters: Name Type Description Default stats_df_mean DataFrame DataFrame containing the bootstrapped data. required stats_metric str | list [ str ] Metric to calculate. Can be \"duration\", \"total_size\", \"min_size\", \"max_size\" or a list of metrics. Default is [\"duration\", \"total_size\"]. required pval_alternative str Alternative hypothesis for the p-value calculation. Can be \"less\", \"greater\" or both which will return p values for both alternatives. required finite_correction bool Correct p-values for finite sampling. Default is True. required plot bool Plot the distribution of the bootstrapped data. required Returns: Name Type Description DataFrame pd . DataFrame containing the p-values. Source code in arcos4py/validation/_bootstrapping.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 def calculate_pvalue ( stats_df_mean : pd . DataFrame , stats_metric : str | list [ str ], pval_alternative : str , finite_correction : bool , plot : bool , ** plot_kwargs , ): \"\"\"Calculates the p-value with the given alternative hypothesis. Arguments: stats_df_mean (DataFrame): DataFrame containing the bootstrapped data. stats_metric (str | list[str]): Metric to calculate. Can be \"duration\", \"total_size\", \"min_size\", \"max_size\" or a list of metrics. Default is [\"duration\", \"total_size\"]. pval_alternative (str): Alternative hypothesis for the p-value calculation. Can be \"less\", \"greater\" or both which will return p values for both alternatives. finite_correction (bool): Correct p-values for finite sampling. Default is True. plot (bool): Plot the distribution of the bootstrapped data. Returns: DataFrame (pd.DataFrame): containing the p-values. \"\"\" if finite_correction : pval = stats_df_mean [ stats_metric ] . agg ( lambda x : _p_val_finite_sampling ( x , pval_alternative )) else : pval = stats_df_mean [[ 'total_size' , 'duration' ]] . agg ( lambda x : _p_val_infinite_sampling ( x , pval_alternative )) pval . name = 'p_value' if isinstance ( stats_metric , list ): _stats_metric = stats_metric else : _stats_metric = [ stats_metric ] if plot : fig , axis = plt . subplots ( 1 , len ( _stats_metric )) try : iter ( axis ) except TypeError : axis = [ axis ] for ax , stats_col in zip ( axis , _stats_metric ): # sns.kdeplot(stats_df_mean[stats_col], ax=ax, shade=True, sharey=True) sns . histplot ( stats_df_mean [ stats_col ], ax = ax , kde = True , stat = 'density' , common_norm = False , ** plot_kwargs ) # ax.hist(stats_df_mean[stats_col], alpha=0.5) ax . set_title ( stats_col ) ax . vlines ( stats_df_mean [ stats_col ] . iloc [ 0 ], ymin = 0 , ymax = ax . get_ylim ()[ 1 ], color = 'red' , ls = '--' ) ax . set_xlabel ( 'Value' ) if len ( axis ) > 1 and ax . is_first_col (): ax . set_ylabel ( 'Density' ) else : ax . set_ylabel ( '' ) x_pos = ax . get_xlim ()[ 0 ] + (( ax . get_xlim ()[ 1 ] - ax . get_xlim ()[ 0 ]) * 0.8 ) y_pos = ax . get_ylim ()[ 0 ] + (( ax . get_ylim ()[ 1 ] - ax . get_ylim ()[ 0 ]) * 0.8 ) ax . text ( x_pos , y_pos , f 'p-value: \\n { pval . loc [:, stats_col ] . round ( 3 ) . to_string () } ' , ha = 'center' , va = 'center' , color = 'red' , ) fig . suptitle ( 'Bootstrapped metrics' ) plt . show () return pval","title":"calculate_pvalue()"},{"location":"api/#arcos4py.validation.resample_data","text":"Resamples data in order to perform bootstrapping analysis. Parameters: Name Type Description Default data pd . Dataframe The data to resample. required posCols list The columns to use for the position. required frame_column str The column to use for the frame. required id_column str The column to use for the object ID. required meas_column str The column to use for the measurement. Only needed for 'activity_blocks_shuffle'. Defaults to None. None method str The method to use for resampling. Defaults to 'shuffle_tracks'. Available methods are: \"shuffle_tracks\", 'shuffle_timepoints', 'shift_timepoints', 'shuffle_binary_blocks', 'shuffle_coordinates_timepoint' 'shuffle_tracks' n int The number of resample iterations. Defaults to 100. 100 seed int The random seed. Defaults to 42. 42 allow_duplicates bool Whether to allow resampling to randomly generate the same data twice. Defaults to False. False max_tries int The maximum number of tries to try ot generate unique data when allow_duplicates is set to True. Defaults to 100. 100 verbose bool Whether to print progress. Defaults to False. False Returns: Type Description pd . DataFrame pd.DataFrame: The resampled data. Source code in arcos4py/validation/_resampling.py 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 def resample_data ( # noqa: C901 data : pd . DataFrame , posCols : list , frame_column : str , id_column : str , meas_column : Union [ str , None ] = None , method : Union [ str , list [ str ]] = 'shuffle_tracks' , n = 100 , seed = 42 , allow_duplicates = False , max_tries = 100 , show_progress = True , verbose = False , paralell_processing = True , ) -> pd . DataFrame : \"\"\"Resamples data in order to perform bootstrapping analysis. Arguments: data (pd.Dataframe): The data to resample. posCols (list): The columns to use for the position. frame_column (str): The column to use for the frame. id_column (str): The column to use for the object ID. meas_column (str, optional): The column to use for the measurement. Only needed for 'activity_blocks_shuffle'. Defaults to None. method (str, optional): The method to use for resampling. Defaults to 'shuffle_tracks'. Available methods are: \"shuffle_tracks\", 'shuffle_timepoints', 'shift_timepoints', 'shuffle_binary_blocks', 'shuffle_coordinates_timepoint' n (int, optional): The number of resample iterations. Defaults to 100. seed (int, optional): The random seed. Defaults to 42. allow_duplicates (bool, optional): Whether to allow resampling to randomly generate the same data twice. Defaults to False. max_tries (int, optional): The maximum number of tries to try ot generate unique data when allow_duplicates is set to True. Defaults to 100. verbose (bool, optional): Whether to print progress. Defaults to False. Returns: pd.DataFrame: The resampled data. \"\"\" # validate the input if not isinstance ( data , pd . DataFrame ): raise TypeError ( 'data must be a pandas.DataFrame' ) if not isinstance ( posCols , list ): raise TypeError ( 'posCols must be a list' ) if not isinstance ( frame_column , str ): raise TypeError ( 'frame_column must be a string' ) if not isinstance ( id_column , str ): raise TypeError ( 'id_column must be a string' ) if not isinstance ( meas_column , str ) and meas_column is not None : raise TypeError ( 'meas_column must be a string or None' ) if not isinstance ( method , str ) and not isinstance ( method , list ): raise TypeError ( 'method must be a string or list' ) if not isinstance ( n , int ): raise TypeError ( 'n must be a positive integer' ) if not isinstance ( seed , int ): raise TypeError ( 'seed must be an integer' ) if not isinstance ( verbose , bool ): raise TypeError ( 'verbose must be a boolean' ) if not isinstance ( paralell_processing , bool ): raise TypeError ( 'paralell_processing must be a boolean' ) if len ( posCols ) < 1 : raise ValueError ( 'posCols must contain at least one column' ) if n < 1 : raise ValueError ( 'n must be a positive integer' ) if seed < 0 : raise ValueError ( 'seed must be a positive integer' ) method_dict : dict [ str , Callable ] = { 'shuffle_tracks' : shuffle_tracks , 'shuffle_timepoints' : shuffle_timepoints , 'shift_timepoints' : shift_timepoints_per_trajectory , 'shuffle_binary_blocks' : shuffle_activity_bocks_per_trajectory , 'shuffle_coordinates_timepoint' : shuffle_coordinates_per_timepoint , } function_args : dict [ str , tuple ] = { 'shuffle_tracks' : ( id_column , posCols , frame_column ), 'shuffle_timepoints' : ( id_column , frame_column ), 'shift_timepoints' : ( id_column , frame_column ), 'shuffle_binary_blocks' : ( id_column , frame_column , meas_column ), 'shuffle_coordinates_timepoint' : ( posCols , frame_column ), } resampling_func_list = [] # convert method to list if necessary if isinstance ( method , str ): methods = [ method ] else : methods = method # Check if the method is valid for method in methods : if method not in method_dict . keys (): raise ValueError ( f 'method must be one of { method_dict . keys () } ' ) if method == 'shuffle_binary_blocks' and meas_column is None : raise ValueError ( 'meas_column must be set for binary_blocks_shuffle' ) # Check if the columns are in the data if 'shuffle_binary_blocks' in methods : relevant_columns = posCols + [ frame_column , id_column , meas_column ] else : relevant_columns = posCols + [ frame_column , id_column ] for i in relevant_columns : if i not in data . columns : raise ValueError ( f ' { i } not in df.columns' ) # check if there are any Nan in the columns selected na_cols = [] for i in relevant_columns : if data [ posCols ] . isnull () . values . any (): na_cols . append ( i ) if na_cols : warnings . warn ( f 'NaN values in { na_cols } , default behaviour is to drop these rows' ) data . dropna ( subset = na_cols , inplace = True ) # Sort the data data . sort_values ([ id_column , frame_column ], inplace = True ) rng = np . random . default_rng ( seed ) # create a list of random numbers between 0 and 1000000 seed_list = rng . integers ( 1_000_000_000 , size = n ) df_out : list [ pd . DataFrame ] = [] # shuffle xy position for each object if verbose : print ( f 'Resampling for each object { n } times' ) # create a list of functions to call for method in methods : resampling_func_list . append ( method_dict [ method ]) iter_range = range ( 1 , n + 1 ) if paralell_processing : from joblib import Parallel , delayed # iterate over the number of resamples df_out = Parallel ( n_jobs =- 1 )( delayed ( _apply_resampling )( iter_number = i , data = data , methods = methods , resampling_func_list = resampling_func_list , seed_list = seed_list , function_args = function_args , ) for i in tqdm ( iter_range , disable = not show_progress ) ) else : # iterate over the number of resamples for i in tqdm ( iter_range , disable = not show_progress ): data_new = _apply_resampling ( iter_number = i , data = data , methods = methods , resampling_func_list = resampling_func_list , seed_list = seed_list , function_args = function_args , ) if not allow_duplicates : current_try = 0 # make sure that data_new is not already in df_out, # but they are both dataframes, else redo the resampling while any ( data_new . loc [:, data_new . columns != 'iteration' ] . equals ( i . loc [:, i . columns != 'iteration' ]) for i in df_out ): current_try += 1 data_new = _apply_resampling ( iter_number = i , data = data , methods = methods , resampling_func_list = resampling_func_list , seed_list = seed_list , function_args = function_args , ) if current_try > max_tries : raise ValueError ( 'Could not find a unique resampling after 100 tries, try increasing n or allow_duplicates' ) df_out . append ( data_new ) data_it0 = data . copy () data_it0 [ 'iteration' ] = np . repeat ( 0 , len ( data_it0 )) df_out . insert ( 0 , data_it0 ) return pd . concat ( df_out )[ data . columns . tolist () + [ 'iteration' ]]","title":"resample_data()"},{"location":"changelog/","text":"Changelog \u00b6 [0.2.0] - 2023-08-09 \u00b6 Added \u00b6 Funcionallity to directly apply ARCOS to images Simple movement predictor to improve tracking HDBSCAN as an alternative clustering method Transportation linking as an alternative linking method Preprocessing function for detrending of images Unit tests for image tracking Added optional Progressbar Changed \u00b6 Refactorization of event detection to improve memory usage and simplify algorithm ARCOS main class now also supports event detection without specifying a tracking column Package is now tested on python 3.8 to 3.11 (dropped 3.7 and added 3.11) Event detection can now ingest data lazily Old detectCollev class is now deprecated in favor of track_events_image and track_events_dataframe [0.1.6] - 2022-05-02 \u00b6 Fixed \u00b6 Bug where trackCollev would overwrite the inputdata in the ARCOS object, preventing repeat tracking of collective events. Spelling mistake in __init__.py Changed \u00b6 None detrending now rescales measurements to 0,1 range on a global scale and not on a track-by-track basis. Added the parameter epsPrev by separating it from eps. epsPrev is the maximum distance cells within collective events can be separated from each other when linking them from one frame to the next. If set to 'None', as default, the same value as for eps is used. Added \u00b6 New function estimate_eps (import from tools) to estimate the eps paramter used for DBSCAN clustering based on the nearest neighbour distribution. Three methods are supported, either mean of NN, median of NN or kneepoint of the sorted NN distribution. Functions to perform resampling and bootstrapping to perform validation of arcos output. Unittests for added functionallity. [0.1.5] - 2022-08-23 \u00b6 Changed \u00b6 Changed running median for global smoothing of trajectories from scipy running_median to pandas running.median, since this allows a different endrule mode Changed running median endrule for local smoothing of trajectories from constant to nearest [0.1.4] - 2022-06-24 \u00b6 Fixed \u00b6 Fix for lm detrending Fix for none detrending Fix grouping failure when object id was a string in rescale data method in binarization class Changed \u00b6 Binarization Thresholding value now sets everything to be active for >=, as opposed to > like it was before [0.1.3] - 2022-05-23 \u00b6 Fixed \u00b6 Bug where if object id was a string, splitting arrays into groups would fail. Hardcoded collective id name in stats module Wrong example in main module Added \u00b6 More examples for plots in plotting module mkdocstrings-python-legacy extension (required for building docs) [0.1.2] - 2022-05-03 \u00b6 Added \u00b6 NoodlePlot for collective events Changed \u00b6 binarize_detrend: converted pandas operations to numpy for performance improvements detect_events: converted pandas operations to numpy for performance imporovements stats: converted pandas operations to numpy for performance improvements various small changes updated docstrings to match changes Fixed \u00b6 numpy warning caused by stats module [0.1.1] - 2022-04-04 \u00b6 Added \u00b6 More plotting functionallity to the plotting module. Measurment density plot Tracklength histogram Position/T plot Collective event statistcs plot Changed \u00b6 Interpolation class in tools now uses pandas.interpolate to interpolate missing values. Interpolation now interpolates all values in all columns of the dataframe. Improved usage section in the documentation. Fixed \u00b6 Bug in trackCollev class that would lead to an error message in some cases. Spelling in docstrings. [0.1.0] - 2022-03-26 \u00b6 Added \u00b6 First release on PyPI.","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#020---2023-08-09","text":"","title":"[0.2.0] - 2023-08-09"},{"location":"changelog/#added","text":"Funcionallity to directly apply ARCOS to images Simple movement predictor to improve tracking HDBSCAN as an alternative clustering method Transportation linking as an alternative linking method Preprocessing function for detrending of images Unit tests for image tracking Added optional Progressbar","title":"Added"},{"location":"changelog/#changed","text":"Refactorization of event detection to improve memory usage and simplify algorithm ARCOS main class now also supports event detection without specifying a tracking column Package is now tested on python 3.8 to 3.11 (dropped 3.7 and added 3.11) Event detection can now ingest data lazily Old detectCollev class is now deprecated in favor of track_events_image and track_events_dataframe","title":"Changed"},{"location":"changelog/#016---2022-05-02","text":"","title":"[0.1.6] - 2022-05-02"},{"location":"changelog/#fixed","text":"Bug where trackCollev would overwrite the inputdata in the ARCOS object, preventing repeat tracking of collective events. Spelling mistake in __init__.py","title":"Fixed"},{"location":"changelog/#changed_1","text":"None detrending now rescales measurements to 0,1 range on a global scale and not on a track-by-track basis. Added the parameter epsPrev by separating it from eps. epsPrev is the maximum distance cells within collective events can be separated from each other when linking them from one frame to the next. If set to 'None', as default, the same value as for eps is used.","title":"Changed"},{"location":"changelog/#added_1","text":"New function estimate_eps (import from tools) to estimate the eps paramter used for DBSCAN clustering based on the nearest neighbour distribution. Three methods are supported, either mean of NN, median of NN or kneepoint of the sorted NN distribution. Functions to perform resampling and bootstrapping to perform validation of arcos output. Unittests for added functionallity.","title":"Added"},{"location":"changelog/#015---2022-08-23","text":"","title":"[0.1.5] - 2022-08-23"},{"location":"changelog/#changed_2","text":"Changed running median for global smoothing of trajectories from scipy running_median to pandas running.median, since this allows a different endrule mode Changed running median endrule for local smoothing of trajectories from constant to nearest","title":"Changed"},{"location":"changelog/#014---2022-06-24","text":"","title":"[0.1.4] - 2022-06-24"},{"location":"changelog/#fixed_1","text":"Fix for lm detrending Fix for none detrending Fix grouping failure when object id was a string in rescale data method in binarization class","title":"Fixed"},{"location":"changelog/#changed_3","text":"Binarization Thresholding value now sets everything to be active for >=, as opposed to > like it was before","title":"Changed"},{"location":"changelog/#013---2022-05-23","text":"","title":"[0.1.3] - 2022-05-23"},{"location":"changelog/#fixed_2","text":"Bug where if object id was a string, splitting arrays into groups would fail. Hardcoded collective id name in stats module Wrong example in main module","title":"Fixed"},{"location":"changelog/#added_2","text":"More examples for plots in plotting module mkdocstrings-python-legacy extension (required for building docs)","title":"Added"},{"location":"changelog/#012---2022-05-03","text":"","title":"[0.1.2] - 2022-05-03"},{"location":"changelog/#added_3","text":"NoodlePlot for collective events","title":"Added"},{"location":"changelog/#changed_4","text":"binarize_detrend: converted pandas operations to numpy for performance improvements detect_events: converted pandas operations to numpy for performance imporovements stats: converted pandas operations to numpy for performance improvements various small changes updated docstrings to match changes","title":"Changed"},{"location":"changelog/#fixed_3","text":"numpy warning caused by stats module","title":"Fixed"},{"location":"changelog/#011---2022-04-04","text":"","title":"[0.1.1] - 2022-04-04"},{"location":"changelog/#added_4","text":"More plotting functionallity to the plotting module. Measurment density plot Tracklength histogram Position/T plot Collective event statistcs plot","title":"Added"},{"location":"changelog/#changed_5","text":"Interpolation class in tools now uses pandas.interpolate to interpolate missing values. Interpolation now interpolates all values in all columns of the dataframe. Improved usage section in the documentation.","title":"Changed"},{"location":"changelog/#fixed_4","text":"Bug in trackCollev class that would lead to an error message in some cases. Spelling in docstrings.","title":"Fixed"},{"location":"changelog/#010---2022-03-26","text":"","title":"[0.1.0] - 2022-03-26"},{"location":"changelog/#added_5","text":"First release on PyPI.","title":"Added"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/bgraedel/arcos4py/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation \u00b6 arcos4py could always use more documentation, whether as part of the official arcos4py docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/bgraedel/arcos4py/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible to make it easier to implement. Remember that this is a volunteer-driven project and that contributions are welcome :) Get Started! \u00b6 Ready to contribute? Here's how to set up arcos4py for local development. Fork the arcos4py repo on GitHub. Clone your fork locally git clone git@github.com:your_name_here/arcos4py.git Ensure poetry is installed. Install dependencies and start your virtualenv: poetry install -E test -E doc -E dev Create a branch for local development: git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: poetry run tox Commit your changes and push your branch to GitHub: git add . git commit -m \"Your detailed description of your changes.\" git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.8, 3.9, 3.10 and 3.11. Check https://github.com/bgraedel/arcos4py/actions and make sure that the tests pass for all supported Python versions. Tips \u00b6 $ poetry run pytest tests/test_arcos4py.py To run a subset of tests. Deploying \u00b6 A reminder for the maintainers on how to deploy: Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: poetry run bump2version patch # possible: major / minor / patch git push git push --tags GitHub Actions will then deploy to PyPI if tests pass.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/bgraedel/arcos4py/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"arcos4py could always use more documentation, whether as part of the official arcos4py docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/bgraedel/arcos4py/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible to make it easier to implement. Remember that this is a volunteer-driven project and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up arcos4py for local development. Fork the arcos4py repo on GitHub. Clone your fork locally git clone git@github.com:your_name_here/arcos4py.git Ensure poetry is installed. Install dependencies and start your virtualenv: poetry install -E test -E doc -E dev Create a branch for local development: git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: poetry run tox Commit your changes and push your branch to GitHub: git add . git commit -m \"Your detailed description of your changes.\" git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for Python 3.8, 3.9, 3.10 and 3.11. Check https://github.com/bgraedel/arcos4py/actions and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"contributing/#tips","text":"$ poetry run pytest tests/test_arcos4py.py To run a subset of tests.","title":"Tips"},{"location":"contributing/#deploying","text":"A reminder for the maintainers on how to deploy: Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run: poetry run bump2version patch # possible: major / minor / patch git push git push --tags GitHub Actions will then deploy to PyPI if tests pass.","title":"Deploying"},{"location":"installation/","text":"Installation \u00b6 Stable release \u00b6 To install arcos4py, run this command in your terminal: pip install arcos4py This is the preferred method to install arcos4py, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From source \u00b6 The source for arcos4py can be downloaded from the Github repo . You can either clone the public repository: git clone https://github.com/bgraedel/arcos4py Or download the tarball : curl -OJL https://github.com/bgraedel/arcos4py/tarball/master Once you have a copy of the source, you can install it with: pip install *tar.gz","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install arcos4py, run this command in your terminal: pip install arcos4py This is the preferred method to install arcos4py, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-source","text":"The source for arcos4py can be downloaded from the Github repo . You can either clone the public repository: git clone https://github.com/bgraedel/arcos4py Or download the tarball : curl -OJL https://github.com/bgraedel/arcos4py/tarball/master Once you have a copy of the source, you can install it with: pip install *tar.gz","title":"From source"},{"location":"usage/","text":"Usage \u00b6 To use arcos4py in a project import arcos4py Basic usage example of the main module \u00b6 To use the main class, generate a new class instance of ARCOS from arcos4py import ARCOS ts = ARCOS(data,[\"x, y\"], 't', 'id', 'm', 'clTrackID') Data has to be a time-series provided as a pandas DataFrame in the long format, containing at least a measurement column, a frame/index column, and an id column. t x y m id Position 0 1 0.228724716134052 -0.158939933368972 0 1 0 1 1 0.880322831777765 -0.117711550077457 0 2 0 2 1 1.93057074895645 0.0786037381335957 0 3 0 3 1 2.95877070488632 0.189801493820322 0 4 0 4 1 3.90293266588805 -0.0413798066471996 0 5 0 .. . ............... ................. . .. . Prepare the input data. \u00b6 interpolate Measurments \u00b6 If the measurement column contains missing values, run interpolate_measurements() first. ts.interpolate_measurements() Clip measurement to provided quantile range \u00b6 Clipping can be performed to remove extreme outliers from the dataset, but it is not necessary. ts.clip_meas(clip_low: = 0.001, clip_high=0.999) Rescale and Binarize the measurement \u00b6 Rescaling and detrending are optional for the algorithm to work but recommended. There are three options available: ['none', 'lm', 'runmed']. Rumned is the default. However, ARCOS requires binarized data to detect and track collective event clusters. Binarization is done by setting a threshold (binThr) and defining measurements below this threshold as 0 and above as 1. ts.bin_measurements(smoothK: int = 1, biasK = 1, peakThr = 1,binThr = 1, polyDeg = 1, biasMet = \"runmed\",) Detect collective events \u00b6 events_df = ts.trackCollev(eps = 1, minClsz = 1, nPrev = 1) print(events_df) t id x y clTrackID m Position 0 2 41 4.15698907764003 3.91461390425413 1 1 0 1 3 32 3.89042167730585 2.98886585399189 1 1 0 2 3 40 3.08624924975602 4.193936843095 1 1 0 3 3 41 3.99750905085216 3.9553900675078 1 1 0 4 3 42 5.06006349489829 4.0631364410516 1 1 0 .. . .. ... .. . . . TrackCollev returns a pandas DataFrame object containing a column with the collecive event id. Perform calculations without main class \u00b6 All functions from the ARCOS class are also accessible individually through the tools module, such as: from arcos4py.tools import trackCollev Additional modules \u00b6 In addition to the ARCOS algorithm and its helper classes, plots are generated with the plotting module, collective event statistics using the stats module. Please see the Modules Page for further details.","title":"Usage"},{"location":"usage/#usage","text":"To use arcos4py in a project import arcos4py","title":"Usage"},{"location":"usage/#basic-usage-example-of-the-main-module","text":"To use the main class, generate a new class instance of ARCOS from arcos4py import ARCOS ts = ARCOS(data,[\"x, y\"], 't', 'id', 'm', 'clTrackID') Data has to be a time-series provided as a pandas DataFrame in the long format, containing at least a measurement column, a frame/index column, and an id column. t x y m id Position 0 1 0.228724716134052 -0.158939933368972 0 1 0 1 1 0.880322831777765 -0.117711550077457 0 2 0 2 1 1.93057074895645 0.0786037381335957 0 3 0 3 1 2.95877070488632 0.189801493820322 0 4 0 4 1 3.90293266588805 -0.0413798066471996 0 5 0 .. . ............... ................. . .. .","title":"Basic usage example of the main module"},{"location":"usage/#prepare-the-input-data","text":"","title":"Prepare the input data."},{"location":"usage/#interpolate-measurments","text":"If the measurement column contains missing values, run interpolate_measurements() first. ts.interpolate_measurements()","title":"interpolate Measurments"},{"location":"usage/#clip-measurement-to-provided-quantile-range","text":"Clipping can be performed to remove extreme outliers from the dataset, but it is not necessary. ts.clip_meas(clip_low: = 0.001, clip_high=0.999)","title":"Clip measurement to provided quantile range"},{"location":"usage/#rescale-and-binarize-the-measurement","text":"Rescaling and detrending are optional for the algorithm to work but recommended. There are three options available: ['none', 'lm', 'runmed']. Rumned is the default. However, ARCOS requires binarized data to detect and track collective event clusters. Binarization is done by setting a threshold (binThr) and defining measurements below this threshold as 0 and above as 1. ts.bin_measurements(smoothK: int = 1, biasK = 1, peakThr = 1,binThr = 1, polyDeg = 1, biasMet = \"runmed\",)","title":"Rescale and Binarize the measurement"},{"location":"usage/#detect-collective-events","text":"events_df = ts.trackCollev(eps = 1, minClsz = 1, nPrev = 1) print(events_df) t id x y clTrackID m Position 0 2 41 4.15698907764003 3.91461390425413 1 1 0 1 3 32 3.89042167730585 2.98886585399189 1 1 0 2 3 40 3.08624924975602 4.193936843095 1 1 0 3 3 41 3.99750905085216 3.9553900675078 1 1 0 4 3 42 5.06006349489829 4.0631364410516 1 1 0 .. . .. ... .. . . . TrackCollev returns a pandas DataFrame object containing a column with the collecive event id.","title":"Detect collective events"},{"location":"usage/#perform-calculations-without-main-class","text":"All functions from the ARCOS class are also accessible individually through the tools module, such as: from arcos4py.tools import trackCollev","title":"Perform calculations without main class"},{"location":"usage/#additional-modules","text":"In addition to the ARCOS algorithm and its helper classes, plots are generated with the plotting module, collective event statistics using the stats module. Please see the Modules Page for further details.","title":"Additional modules"}]}